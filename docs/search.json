[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning Morpho-phonological Alternations like French Liaison",
    "section": "",
    "text": "About",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Learning Morpho-phonological Alternations like French Liaison",
    "section": "",
    "text": "This website was generated with Quarto books. If you prefer the dark mode, you can toggle it from the sidebar. On the sidebar, another offline version in .epub document is also available to download.\nAll of the statistical analyses and plots were created with R packages. The experiments were coded using jsPsych framework.\nThe experiments material including the stimuli list, javascript codes for jsPsych, and R codes for the statistical analysis are stored at an osf repository for open source access.\nüìî The official thesis document is available open-access in the UBC Theses and Dissertation collection. (Disclaimer: This version has better OT Tableux!)",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#lay-summary",
    "href": "index.html#lay-summary",
    "title": "Learning Morpho-phonological Alternations like French Liaison",
    "section": "Lay Summary",
    "text": "Lay Summary\nThe goal of this thesis is to improve our understanding of how certain sound patterns are learned when they are exceptions to the general sound patterns in a language. My study focuses on one system of sound patterns in French as its unique characteristics presents various challenges for language acquisition. In French, certain arbitrary words, like petite meaning ‚Äòsmall‚Äô, have two forms in pronunciation, and the choice between forms depends on whether the following noun starts with a vowel or a consonant. In this thesis, I create an artificial mini-language using nonsense words that mimic the sound patterns of French and present it to adult participants in an online experimental study. The participant errors are similar to children and show that they struggle learning the nouns starting with a vowel. However, they can judge which pattern is correct when they get the full phrase.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "chapters/0_abstract.html",
    "href": "chapters/0_abstract.html",
    "title": "Abstract",
    "section": "",
    "text": "Morpho-phonological phenomena present challenges to learners because they interface with morpho-syntactic, semantic, and phonological grammar. A common difficulty encountered in these interface phenomena lies in navigating idiosyncrasies and exceptional cases. This thesis explores the nature of learning implications of morpho-phonological alternations, in particular focusing on the case study of patterns akin to French liaison. Liaison is a prevalent and complex phenomenon in French that occurs at word boundaries. I propose a theoretical account of the learnability of French liaison within the Optimality Theory framework which gives rise to predictions regarding various components and dimensions of learning. I evaluate these predictions in an experimental study with an artificial language learning paradigm that consists of a training phase and a testing phase with two-alternative-forced-choice tasks. The training phase has a between-subject design where learners experience different hypothesized learning trajectories. The aim of the experiment is to investigate the extent to which learners would face challenges in learning given that the artificial language includes idiosyncratic patterns. The results demonstrate that in a controlled experimental setting, adult learners do show partial differential learning outcomes depending on their learning trajectory in training. However, participants also show task-dependent performance: above-chance for noun phrase choices but near-chance for word segmentation with both novel and familiar nouns. These results, taken together, imply that one of the most challenging components of learning in patterns similar to liaison predominantly lies in mechanisms related to computing word boundaries and deriving representations.",
    "crumbs": [
      "Abstract"
    ]
  },
  {
    "objectID": "chapters/1_introduction.html",
    "href": "chapters/1_introduction.html",
    "title": "1¬† Introduction",
    "section": "",
    "text": "1.1 General overview\nPhonologically conditioned allomorphy is a cross-linguistically common interface phenomenon that invokes the knowledge of different modules of the grammar (morphology, syntax, and phonology). Despite receiving great attention in the theoretical morpho-phonological literature (for overviews see Nevins, 2011; Wolf, 2008; Paster, 2006) and a series of experimental studies using wug tests and similar methodology within the child acquisition literature (Finn and Hudson Kam, (2015), Zamuner et al., 2012. Kerkhoff, 2007, Berko, 1958), experimental studies that focus on the learning of phonologically conditioned allomorphy patterns and exceptions are not numerous (cf.¬†Finley, 2023, 2021, Baer-Henney et al., 2015; Coetzee, 2009). The main goal of this thesis was to provide some insights regarding phonological alternation patterns which are idiosyncratic and not language-wide from a learnability perspective.\nThe specific case study that was the focus of this thesis is a morpho-phonological pattern in French called liaison (C√¥t√©, 2011; Tranel, 1995a; Morrison, 1968). Liaison is a prevalent yet idiosyncratic external sandhi phenomenon that occurs at the boundary of two words in various configurations. The specific morpho-syntactic configuration of liaison which this study concentrates on is the nominal domain, for instance the adjective + noun configurations (e.g., [p…ôtit]/[p…ôti]: petit ours [p…ô.ti.tu Ås] ‚Äòlittle bear‚Äô vs.¬†petit prince [p…ô.ti.p Å…õÃÉs] ‚Äòlittle prince‚Äô). The alternations of the first word are phonologically conditioned by the following word‚Äôs V- or C-initial segment, similar to the conditions of the indefinite determiner allomorphy in English a/an. However, the liaison pattern in French involves some idiosyncrasies that challenge the learner. In contrast to English, the set of morphemes with the same allomorphy pattern not only includes the indefinite determiner but also several other functional and non-functional morphemes. Nevertheless, the pattern does not become fully regular or language-wide as many counterexamples are found (e.g.¬†in adjective + noun configuration, [ í≈ìn]: jeune ours [ í≈ì.nu Ås] ‚Äòyoung bear‚Äô vs.¬†jeune prince [ í≈ìn.p Å…õÃÉs] ‚Äòyoung prince‚Äô).\nBy choosing this case study, the objective was to explore the implications of theories of learning for idiosyncratic patterns. For this purpose, I adopted the constraint indexation model in Optimality Theory (Pater, 2010) as the basis for the learning-based theoretical framework and designed an artificial language learning experiment to evaluate the predictions generated by that account. It is important to note that several alternative theoretical approaches exist in the literature for capturing similar morpho-phonological alternations. These include co-phonologies (Inkelas and Zoll, 2007; Ito and Mester, 1995) for addressing morpheme-specificity, gradient symbolic representations (Smolensky and Goldrick, 2016) concerning the unique status of certain representations, and Distributed Morphology (Embick, 2010) to explain allomorph selection. However, the nature of the constraint indexation model more readily allows for comparisons of predictions across learners within an experimental framework.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/1_introduction.html#scope-and-significance-of-the-study",
    "href": "chapters/1_introduction.html#scope-and-significance-of-the-study",
    "title": "1¬† Introduction",
    "section": "1.2 Scope and significance of the study",
    "text": "1.2 Scope and significance of the study\nThis thesis aims to empirically investigate the challenging aspects of learning emerging from the patterns in the learning data with regard to a pre-nominal idiosyncratic external sandhi phenomenon like French liaison. In liaison, there are two main learning challenges. The classification of morphemes in the lexicon that do or do not participate in liaison is not based on phonological surface information and is considered to be phonologically arbitrary1. In addition, in the context of noun phrases, the prenominal location of liaison is significant for learning the lexicon, as well as learning the phonological grammar which presents a unique challenge from a learnability perspective. By examining an edge case like liaison in French, the findings of this study may have broader implications for learning theories as a whole. Even though liaison-like patterns are rare, similar challenging patterns are also found in cases such as the initial consonant mutation in Celtic languages (Hannahs, 2011), and voicing alternation in Japanese Rendaku (Rosen, 2003).\nTo tackle a learnability question, a thorough investigation of the interaction of errors occurring during acquisition and trajectories of learners necessitates longitudinal studies that often face substantial logistical and design difficulties. Artificial Language Learning (ALL) paradigms are an alternative method among the experimental studies in the literature that have been successful in examining the impact of biases and challenges in learning (Culbertson, 2024; Finley, 2023; Glewwe, 2022; Chong, 2021; Linzen and Gallagher, 2017; Hayes and White, 2013; Moreton and Pater, 2012a, 2012b; Tessier 2012 among others). An experimental study that artificially simulates a learning scenario could have implications for theories of learnability. If certain dimensions of learning are not as challenging as the rest, relevant mechanisms and components entailed in theories of learnability could be modified to better capture how learning is achieved by learners. Therefore, this thesis benefits from an Artificial Language learning paradigm. These experiments can take a variety of forms and designs, but often they are designed in two phases: a. training phase where participants are trained by being exposed to learning data using audio, text, or visual stimuli; b. testing phase where participants are tested on data exactly matching the training data and/or additional novel data (depending on the design) by performing tasks.\nIn the current experiment, incorporating a typical artificial language learning paradigm, the learning scenario was as similar as possible to language acquisition by children. The experiment‚Äôs training phase provided an opportunity for implicit learning where there is no negative learning data or feedback. However, as opposed to real life acquisition, it is possible to control for semantic information. I designed three training conditions for three groups of learners: either to begin with the class of morphemes that participate in liaison or the class of morphemes that do not, or to receive both classes at the same time. Importantly, the training condition trajectories differ in the order in which the learning data unfolds, not any other aspect such as the frequency or amount of the data. Focusing on the distribution and order of learning data throughout the experimental training, the objective of testing is to investigate whether there is a differential learning outcome depending on the difficulty level emerging from the training conditions.\nThe results do not fully align with predictions regarding the challenging aspects of learning; however, they indicate that learners‚Äô accuracy could to some extent be predicted by the training conditions. Overall, the difficulty level of the tasks and inter-participant variation, in addition to stronger universal biases against language-specific segmentation preclude observing a statistically significant interaction between the types of errors and different training conditions, and further research is needed to be able to clarify the impact of the distribution and order of learning data for patterns such as liaison. However, one of the implications that stem from the results of the current study may be that the most challenging component of learning predominantly lies in mechanisms related to computing boundaries and selecting representations.\nThe structure of this thesis is as follows: In Chapter 2, I present an analysis grounded in an Optimality theoretic framework that addresses the learnability issues discussed followed by predictions and research questions building on the learnability theory. Chapter 3 discusses the methodology and details of the experiment design. The results of the experiment and the accompanying statistical analyses are detailed in Chapter 4. The thesis ends with a discussion of the implications concerning the research questions and conclusion in Chapter 5.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/1_introduction.html#footnotes",
    "href": "chapters/1_introduction.html#footnotes",
    "title": "1¬† Introduction",
    "section": "",
    "text": "In some theoretical accounts (such as in non-linear phonology), differences in phonological representations are used to explain the distinction between liaison vs non-liaison word1s (e.g., Tranel, 1995b). However, from the perspective of a theory of learnability in this thesis, this is a phonologically arbitrary classification that is achieved with non-phonological indexation.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/2_backgroundandtheory.html",
    "href": "chapters/2_backgroundandtheory.html",
    "title": "2¬† Background and Theoretical Framework",
    "section": "",
    "text": "2.1 Introduction to phonologically conditioned allomorphy\nAllomorphy refers to the variations in a morpheme‚Äôs phonological form, often controlled by contextual linguistic factors arising from morpho-syntax or morpho-phonology. The allomorph variants of a morpheme can exhibit various degrees of phonological dissimilarity and idiosyncrasy. For example, the verb to be in English presents three dissimilar forms in the present tense (am/is/are) that are morphologically conditioned and somewhat idiosyncratic, as the choices are not consistent across person and number. In contrast, the two indefinite article forms in English (a/an) are more phonologically similar and demonstrate regular conditioning, as the choice depends on whether the noun begins with a vowel or consonant. The latter example is commonly referred to as phonologically conditioned allomorphy (PCA), where the phonological context determines the selection of the allomorph. In such cases, the allomorphs selected can often be considered as optimizing and motivated by avoiding marked phonological structure (cf.¬†‚Äúnon-optimizing phonologically conditioned suppletive allomorphy‚Äù in Paster, 2006: 76-97). Nevins (2011) outlines six types of phonological conditions that lead to optimized and natural outputs. The most prevalent type involves conditions that produce optimized (unmarked) syllable structures, ranging from preference for onsets, dispreference for codas, or sonority drops in codas (for a comprehensive review, see Nevins, 2011: 6-8; Paster, 2006: 63-71). Besides at the level of syllable structure, other types of phonological conditions can appear at the level of segments or prosody.\nThe next section will introduce the case study focusing on a phonologically conditioned allomorphy phenomenon in French. French liaison presents challenges regarding the phonological conditions governing allomorph selection and representations which will have consequences for learning.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Background and Theoretical Framework</span>"
    ]
  },
  {
    "objectID": "chapters/2_backgroundandtheory.html#french-liaison",
    "href": "chapters/2_backgroundandtheory.html#french-liaison",
    "title": "2¬† Background and Theoretical Framework",
    "section": "2.2 French liaison",
    "text": "2.2 French liaison\nLiaison is a frequent and widespread morpho-phonological phenomenon in French which has received great attention over the years in the study of the phonological grammar of French (Storme, 2024; Smolensky and Goldrick, 2016; Tranel, 2000; Selkirk, 1974). In terms of its domain, it is categorized as external allomorphy where the conditioning factors are determined by phonological information in adjacent words in the higher prosodic domain. In this respect, liaison is also known as an external sandhi phenomenon , since the alternation occurs at the boundary of two words. The locus of liaison is limited to certain syntactic configurations; at the same time, it is also affected by variation and sociolinguistic variables (Meinschaefer, Sven and Frisch, 2015; Durand and Lyche, 2008). The following data points illustrate the basic pattern in prenominal configurations. The examples (1‚àí4) are of noun phrases with two syntactic configurations in¬†which liaison applies, including [determiner + noun] and [adjective + noun] such as (1) the plural definite article les with the allomorphs [le]/[lez], (2) the masculine singular indefinite article un with the allomorphs [≈ìÃÉ]/[≈ìÃÉn], (3) the numeral modifier deux ‚Äòtwo‚Äô with [d√∏]/[d√∏z] and (4) the masculine singular adjective petit ‚Äòlittle‚Äô with [p…ôti]/[p…ôtit]:\n\n\n\n\n\n\n\n\n\n\n\n\n\n(1)\n\n\n\n(2)\n\n\n\n\n\n\n\na.\nles b√©b√©s\n[le.be.be]\n‚Äòthe babies‚Äô\na.\nun b√©b√©\n[≈ìÃÉ.be.be]\n‚Äòa baby‚Äô\n\n\nb.\nles amis\n[le.za.mi]\n‚Äòthe friends‚Äô\nb.\nun ami\n[≈ìÃÉ.na.mi]\n‚Äòa friend‚Äô\n\n\nc.\nles ours\n[le.zu. Ås]\n‚Äòthe bears‚Äô\nc.\nun ours\n[≈ìÃÉ.nu Ås]\n‚Äòa bear‚Äô\n\n\n(3)\n\n\n\n(4)\n\n\n\n\n\na.\ndeux b√©b√©s\n[d√∏.be.be]\n‚Äòtwo babies‚Äô\na.\npetit b√©b√©\n[p…ô.ti.be.be]\n‚Äòlittle baby‚Äô\n\n\nb.\ndeux amis\n[d√∏.za.mi]\n‚Äòtwo friends‚Äô\nb.\npetit ami\n[p…ô.ti.ta.mi]\n‚Äòlittle friend‚Äô\n\n\nc.\ndeux ours\n[d√∏.zu Ås]\n‚Äòtwo bears‚Äô\nc.\npetit ours\n[p…ô.ti.tu Ås]\n‚Äòlittle bear‚Äô\n\n\n\nThe final consonant, here [z, n, t] in bold, in the VC# allomorphs is referred to as the ‚Äúliaison consonant‚Äù (LC) in the literature. The basic generalization in these examples is that some structural property of the following words determine the presence of the allomorph containing the liaison consonant or lack thereof. As examples (a-c) for each allomorph set show, the onset of the left word (nouns) seems to be conditioning the choice of the allomorph in the right morpheme. Examples (a) are C-initial nouns, resulting in the selection of the V-final allomorph. In contrast, examples (b) and (c) demonstrate V-initial nouns, which results in the allomorph containing the liaison consonant.\nTo fully understand the role of this pattern in the morpho-phonological system of French, it is essential to recognize that morphemes with allomorph pairs, that is those involved in liaison, are lexically determined and do not constitute a coherent class. According to C√¥t√© (2011: 5) the morphemes grouped under liaison are argued to be either among closed lexical classes or specific morphological categories. The determiners (1‚àí2) are an example of the closed class of morphemes, other functional morphemes in this category are clitics, conjunctions, and prepositions. The specific morphological categories include adjectives, nouns, adverbs, within compounds and fixed phrases. However, it is crucial to note that this is not a language-wide pattern; for instance, not all [adjective + noun] configurations in French follow the same pattern across-the-board. The data in (5) shows two masculine adjectives which do not undergo liaison. Examples (5a) and (5b) show C-initial nouns and examples (5c) and (5d) V-initial nouns which all co-occur with non-alternating adjectives joli [ í…îli] and jeune [ í≈ìn].\n\n\n\n\n\n\n\n\n\n\n\n\n\n(5)\n\n\n\n\n\n\n\n\n\n\n\na.\njoli b√©b√©\n[ í…îli.be.be]\n‚Äòpretty baby‚Äô\nb.\njeune b√©b√©\n[ í≈ìn.be.be]\n‚Äòyoung baby‚Äô\n\n\nc.\njoli ami\n[ í…îli.a.mi]\n‚Äòpretty friend‚Äô\nd.\njeune ami\n[ í≈ì.na.mi]\n‚Äòyoung friend‚Äô\n\n\n\nThe contrast in examples (4 vs.¬†5) illustrate an important empirical pattern to be captured, whereby a situation is created for phonological processes favouring naturally well-formed structures in an allomorph to be restricted to only certain morphemes. The nature of these restrictions appears quite arbitrary which even seems to be insensitive to the morpho-syntactic categories of morphemes. This is evident from the fact that certain groups of prenominal adjectives possess allomorphic pairs (4), while others do not (5).\nFrom the point of view of acquisition, it is worth mentioning that the right words also contribute to creating added irregularities in the pattern. Therefore, considering the phenomenon as a whole, idiosyncratic and morpheme-specific characteristics extend to the right words which were introduced as the conditioning factor. Examples in (6) illustrate that certain nouns traditionally categorized as h-aspir√© are anticipated to condition the C-final allomorph but in fact resist it (for a review of historical facts see Southworth, 1970). A small group of words such as homard, h√©ros, and hibou which phonologically begin with a vowel in modern French, function as if they are C-initial, thereby not selecting the allomorph containing the liaison consonant (cf.¬†examples b and c in 1‚àí4). Therefore, in the event that a h-aspir√© noun constitutes the context in prenominal liaison, the basic generalization does not hold and the regular phonological sensitivity of the plural definite morpheme is neutralized.\n\n\n\n\n\n\n\n\n\n\n(6)\n\n\n\n\n\n\n\n\na.\nles homards\n[le.…îm.a Å]\n*[le.z…îm.a Å]\n‚Äòthe lobsters‚Äô\n\n\nb.\nles h√©ros\n[le.e Åo]\n*[le.ze Åo]\n‚Äòthe heroes‚Äô\n\n\nc.\nles hiboux\n[le.ibu]\n*[le.zibu]\n‚Äòthe owls‚Äô\n\n\n\nAfter having examined the basic synchronic patterns and characteristics of morphemes within the higher prosodic domain where liaison occurs, it must be noted that the system of liaison has evolved over hundreds of years (Morin, 2005). The idiosyncratic nature and irregularities observed are largely attributed to diachronic factors. Overall, the irregular interactions between phonological elements and morphology give rise to various analyses and competing theories concerning the organization of grammar. Several accounts have also been proposed about the affiliation of the liaison consonant (for a summary see C√¥t√©, 2011: 8; Tessier et al., 2023: 3; Storme, 2024: 31) where it could be associated with the word1 (e.g., les), word2 (e.g., ami), both or neither. This system of liaison, shaped by both synchronic and diachronic influences, has sparked debates regarding the mechanisms of allomorphy selection. Some accounts emphasize morphological listing (Paster, 2006; Embick, 2010), while others such as Smith (2015) advocate for constraint-based models where the ranking of phonological and morphological constraints select the underlying representations. As Nevins (2011: 23) notes, using artificial language acquisition experiments where ‚Äúdiachrony is fully controlled by the experimenter and nonetheless the learner demonstrates the emergence of a preference for phonological optimization based on sparse or insufficient data‚Äù can be a good test bed for this debate.\nIn the next section, to shed some light on the extent to which the role of phonology is central in speakers‚Äô minds, the literature on children‚Äôs acquisition of French liaison is reviewed to discuss different approaches explaining empirical results in experimental studies with young children. Additionally, a brief overview of developmental stages in morphophonemic learning is provided.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Background and Theoretical Framework</span>"
    ]
  },
  {
    "objectID": "chapters/2_backgroundandtheory.html#developmental-path-of-morpho-phonological-acquisition-and-the-literature-on-child-acquisition-of-liaison",
    "href": "chapters/2_backgroundandtheory.html#developmental-path-of-morpho-phonological-acquisition-and-the-literature-on-child-acquisition-of-liaison",
    "title": "2¬† Background and Theoretical Framework",
    "section": "2.3 Developmental path of morpho-phonological acquisition and the literature on child acquisition of liaison",
    "text": "2.3 Developmental path of morpho-phonological acquisition and the literature on child acquisition of liaison\nIt is generally accepted that early in the developmental path of phonological acquisition infants exhibit signs of developing sensitivity to phonotactic patterns, allowing them to build their foundational phonological knowledge by 8 to 12 months of age (for a recent review, see Sundara et al., 2022; Jusczyk, 2000). Learning about alternations typically occurs later, starting around 12 months (White and Sundara, 2014). Perception studies support this notion, revealing that infants (approximately 9 months old) can distinguish phonotactic probabilities among words (Friederici and Wessels, 1993) and employ this distributional information for word segmentation (Mattys and Jusczyk, 2001), as well as for learning (voicing) alternations (White et al., 2008). A longitudinal study by Luckaszewicz (2006) reports that a child aged 3;5-8 was more successful at mastering alternations that reflect the phonotactic rules of Polish than those found only in specific morphemes. These studies suggest that phonotactic knowledge may play a crucial role in guiding the subsequent learning of alternations. Further along the developmental path, evidence regarding infants‚Äô receptive morphological knowledge indicates that they display sensitivity to morpho-phonological patterns in their second year of life, well before they achieve mastery of these patterns in later stages (Soderstrom et al., 2007). Understanding morpho-phonological alternations involves mapping variations in surface forms to underlying representations and recognizing morphemes‚Äô distinct meanings. In this context, the literature on morphological acquisition illustrates that the discovery of meaning influences infants‚Äô acquisition of suffixes (Mintz, 2013). However, more recent experiments reveal that children‚Äôs knowledge of morphology develops concurrently with their understanding of phonology and semantics in English (Kim and Sundara, 2021) as well as in French (Marquis and Shi, 2012). In fact, Kim and Sundara (2021) suggest that their findings propose a potentially bidirectional relationship between the acquisition of morphology and phonology, challenging the earlier assumption that phonological acquisition solely drives morphological acquisition.\nAs Tessier (2016:1) points out, acquiring morpho-phonological patterns presents challenges both to learners and researchers as ‚Äúthe learner must determine a set of lexical representations attached to meanings, and also control the phonological regularities involved in the combinations of those stored representations‚Äù. In other words, morpho-phonological learning is inherently more complex than phonological learning. While it is possible to account for across-the-board phonological alternations by the phonological grammar alone, morpho-phonological patterns necessitate the integration of morphology, semantics, and phonology. For instance, consider sequences involving two morphemes verb + X, such as talk+[-t] and log+[-d]; the complexity in mastering the alternation of forms X arises from several interrelated components. It is essential to acknowledge that developmental processes are more entangled and iterative than linear in reality. These components can be learned synchronously or asynchronously, as Kim and Sundara (2021) show learning can be concurrent across domains. Regardless of the relative order of acquisition in the developmental path of learning allomorphy, the key components that an ideal learner must acquire along their path to master the variations of the English past tense suffix1 is outlined in (7).\n\n\n\n\n\n\n\n\n(7)\n\n\n\n\n\n\n\na.\nThese sequences are comprised of 2 morphemes: verb + morpheme X.\n\n\n\nb.\nMorphemes talk and log are verbs with their respective meaning.\n\n\n\nc.\nMorphs [-t] and [-d] are similar in form (broadly speaking phonological similarity is not a requirement for allomorphs, but can be used as a cue in some cases).\n\n\n\nd.\nMorphs [-t] and [-d] share the same meaning (or syntactic function).\n\n\n\ne.\nDistributions of [-t] and [-d] follow a phonological pattern ([-t] after a voiceless non-sibilant C; [-d] after a V or voiced non-sibilant C).\n\n\n\nf.\nMorphs [-t] and [-d] should be represented and stored in the lexicon as variants of one morpheme &lt;+past tense&gt; and not separately.\n\n\n\nIn certain instances, alternations related to a morpheme may adhere to the phonotactic rules of the language. For example, the acquisition of past tense allomorphs in English aligns with learning static restrictions found in mono-morphemic nouns about similarity in voicing in sequences of adjacent obstruents. Therefore, discovering the phonological basis for these alternations can, at least theoretically, be informed and guided by phonotactic learning. Empirical support for this issue is illustrated in a series of experiments by Chong (2021) where participants learned the alternations across morpheme boundaries best when the patterns matched the stem-internal phonotactics. However, it is common for phonological alternations to be limited to specific morphemes, presenting an additional challenging component for learners to resolve along with the components in (7a‚àíf). As discussed in the previous section, the case of French liaison exemplifies this phenomenon.\nThe latter half of this subsection focuses on the research related to the acquisition of French liaison. Various scholars have explored the early development of this morpho-phonological pattern utilizing both corpus data and experimental methodologies (Buerkin-Pontrelli et al., 2017; Demuth and Tremblay, 2008; Dugua, 2006; Chevrot and Fayol, 2001; Morel, 1994). This review will succinctly examine recent studies that aim to explain and characterize the errors associated with representations of prenominal liaison and nouns that begin with vowels.\nEvidence from perception experiments using a preferential-looking paradigm, as designed by Babineau and Shi (2014), indicates that children as young as 2;0 year of age familiarized with novel pseudo-words in various liaison contexts (which could be ambiguous between being vowel-initial or consonant-initial) accepted both segmentations of pseudo-words. For instance, in case of phrases like un onche [≈ìÃÉ.n√µ É] or des onches [de.zo É], the pseudo-word onche could be ambiguous between combinations of [≈ìÃÉ]+[n√µ É] where the syllable boundary and word boundary are aligned or [≈ìÃÉn]+[√µ É] where they are not. In contrast, 1;8 year old children tend to segment these novel words as consonant-initial (e.g., [n√µ É], [zo É].). More recently, Babineau et al.¬†(2023) conducted follow-up experiments using different familiarization conditions with 2;0 year olds to shed light on the nature of their knowledge regarding liaison. Specifically, the goal was to determine whether their knowledge is shaped by language-specific statistical information from sub-syllabic distributions in French or whether it is not based on language-specific information per se and is only using broad statistical sub-syllabic distributions. If the children use their language-specific knowledge they could leverage it to overcome the universal bias influencing their segmentation which aligns syllable onsets with word edges. Their findings support the notion that, at this age, infants can recognize the co-occurrence patterns particular to certain liaison consonants in French in a larger context (including the first words) from which the sub-syllabic distributions emerge while disregarding the universal segmentation bias. However, this advanced top-down processing that resembles adult understanding is not as evident in the production of the toddlers of the same age as the authors also point out. Now before turning to the two different models discussed in the literature to explain and predict the production of French-speaking children, I will summarize the main error categories and empirical facts for young children as documented in different corpora.\nThe rate of production errors during the first two years of life indicates that mastering liaison takes considerable time and often extends beyond two years. Buerkin-Pontrelli et al.¬†(2017) analyze and report data from the Lyon corpus (Demuth and Tremblay, 2008) in CHILDES database (MacWhinney, 2000), categorizing the errors into two main types: a. those that involve phonological processes such as truncation or reduplication (e.g., les ami [le.mi] or [le.ma.mi]); and b. other errors related to the liaison consonant, such as insertion, replacement and omission (e.g., joli ami [ í…î.li.na.mi], les ami [le.na.mi], les enfants [le.√£.f√£]). Their analysis reveals that during the early stages until 2;0 the errors are predominantly of a phonological nature to fill the onset positions of V-initial words or preserve minimal-foot structure. Between the ages of 2;0 to 3;0, errors involving liaison consonants appear in the children‚Äôs speech within the corpus. Further than 3;0, however, available data from one child in the corpus demonstrates that such errors are comparatively rare between the ages of 3 ‚Äì 3;5. They highlight that those phonological errors (truncation and reduplication) were more common than liaison consonants errors, however the overall trajectory of the errors could suggest that as children‚Äôs phonological system stabilizes, they might produce errors which are directly due to their early awareness of liaison. One important point to keep in mind with regards to these studies is that the estimated age of mastery varies depending on whether the data is elicited or produced spontaneously. The spontaneous liaison errors documented might decrease in earlier age ranges compared to elicited errors in experimental settings that could be even found with children tested up to 5;2. Buerkin-Pontrelli et al.¬†(2017) also present results from an original experiment to investigate whether 3;0 children could produce two novel vowel-initial words in both liaison and non-liaison contexts productively. Their findings reveal that 3;0 children are reaching adults‚Äô accuracy rate in full phrase contexts, which suggests they can segment and use liaison after a brief exposure. On the other hand, two-thirds of the children‚Äôs errors involved replacements or insertions, contrasting with adults, who tended to make more errors by omitting the liaison consonant. This indicates that children struggle more than adults to provide vowel-initial forms in non-liaison-triggering contexts.\nThere are two models put forward in the literature to explain children‚Äôs acquisition of liaison. One model has examined the stages of learning liaison through a phonological lens (Buerkin-Pontrelli et al., 2017; Wauquier-Gravelines and Braud, 2005). With this model, the stages of acquisition are based on the evolution of syllabic templates. It is argued that following the maximum onset principle, children rely on distinct mechanisms in their errors to resyllabify word-final consonants in the word resulting in V.CV syllables rather than VC.V syllables. According to this model, once the phonological development is more complete, by ages 3;0 to 3;5, children can utilize morphological bootstrapping, enabling them to identify morphemes and the contexts in which particular liaison consonants (LCs) appear. The second model follows a constructionist approach (Chevrot et al., 2009) couched within usage-based theories (Tomasello, 2003) and argues that the acquisition of liaison occurs in three stages. In this framework, it is predicted that children store schemas (such as les + X, un + X) without information about the liaison consonant in determiners and alternating noun variants from 3;0 to 5;0. By the ages of 4;0 to 5;0, they begin to develop schemas that reflect the relationship between a determiner and a class of noun variants (e.g., les + /zX/, un + /nX/). As Buerkin-Pontrelli et al.¬†(2017) indicate, the expected timelines for these stages in the constructionist model are not entirely precise, as children tend to exhibit fewer liaison errors after the age of 3. However, constructionist models heavily depend on input frequency, leading to the prediction that certain nouns are more closely associated with errors in singular contexts than with plural. As supported by the results of the experiment reported by Buerkin-Pontrelli et al.¬†(2017), input frequency plays a role in early stages of learning; however, its role and its interaction with other aspects of learning more complicated patterns such as liaison needs further investigation.\nIn the next section, before I discuss a proposal regarding how the phonological grammar can enable the learning of liaison-like patterns‚Äîand to what extent it can do so, along with the predictions it generates for the experiment‚ÄîI will first provide an overview of the nature of the learning problem.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Background and Theoretical Framework</span>"
    ]
  },
  {
    "objectID": "chapters/2_backgroundandtheory.html#learnability-of-liaison-as-a-morpheme-specific-phonological-phenomenon-within-optimality-theory",
    "href": "chapters/2_backgroundandtheory.html#learnability-of-liaison-as-a-morpheme-specific-phonological-phenomenon-within-optimality-theory",
    "title": "2¬† Background and Theoretical Framework",
    "section": "2.4 Learnability of liaison as a morpheme-specific phonological phenomenon within Optimality Theory",
    "text": "2.4 Learnability of liaison as a morpheme-specific phonological phenomenon within Optimality Theory\nThe logical problem of language acquisition as summarized by Kager (2004: 323) from the original work on learnability in Optimality theory (Tesar and Smolensky, 2000) concerns learning three factors as shown in Figure 2-1: a. output representations; b. underlying representations; c.¬†constraint hierarchy. These factors are assumed to be interdependent which means that learners in principle use iterative strategies to be able to go back and forth between these elements.\n\n\n\nFigure 2‚Äë1. Interdependence of the three factors of output, underlying representation and constraint hierarchy in language acquisition taken from Kager (2004:323)\n\n\nFundamentally, as Tesar and Smolensky (2000) introduced this problem, learning could be decomposed along some dimensions with processes that must interpret the hidden structure of the output (robust parser component) and compute the optimal structure for the underlying form (lexicon learner component) through the constraint hierarchy (grammar learner component). Morphophonemic learning in contrast to phonotactic learning involves an additional layer of morphological information, specifically when deriving underlying representation. Consequently, learning idiosyncratic patterns of allomorphy involves interaction between different parts of linguistic knowledge, namely, the lexicon and phonology. Two aspects of learning, in particular, are of interest in this thesis for an account that deals with the learnability problem of morpheme-specific alternations like liaison.\nFirstly, as demonstrated by the data in section 2.2, the idiosyncratic nature of external allomorphy patterns like liaison can be attributed to their extended domain. Mascar√≥ (2004) highlights the paradoxes in this type of allomorphy with some examples from English which he refers to as external allomorphy. In the case of French liaison, to begin with, classifying word1s under liaison (allomorphy) or non-liaison (non-allomorphy) classes is not predictable by surface phonological forms. A lexical solution enables the learner to solve the problem for similar cases of unpredictability, for instance, regular vs.¬†irregular past tense verbs in English: walk/walk-ed, talk/talk-ed vs.¬†buy/bought, bring/brought. In cases when some verbs do not take the regular allomorph (-ed suffix), the past tense suppletive allomorph is listed in the lexicon. Whereas in liaison, a similar lexical solution does not suffice; learners must be aware of a bigger domain that extends beyond the word1 and includes the adjacent right word. For any given word1, the learner needs to know when to apply the allomorphy by considering if the word2 is C/V-initial. In other words, the learners would not be able to simply list every word1 + word2 phrase in the lexicon as they might do with the English irregular past tense forms.\nEven though the classification of each word1 under allomorphy or non-allomorphy could seem rather arbitrary to the leaner and without any phonological motivations, phonological optimization is important for the participating word1s in liaison that do have an allomorphic pair. Predicting the choice of the allomorph in liaison patterns is possible through the lens of phonological optimization as it is natural to create forms that are following the most unmarked outcome in terms of syllable structure. However, if phonology is emerging to create an optimal structure in the larger prosodic domain of the allomorphy, the puzzle remaining for the learner is now the following question: why is it only restricted to just a limited set of lexical items? Simply stated, in the current case study, the lexical and arbitrary nature of allomorphy itself creates an arbitrary partition in the phonological grammar as shown in Figure 2-2. On the one side of the grammar, in the closed lexical set of liaison morphemes, prioritizing optimal structures in the grammar with constraints on markedness is required. While on the other side, in morphemes outside that closed lexical set, it is not.\n\n\n\nFigure 2.‚Äë2. Partition in the phonological grammar for word1s in liaison or non-liaison classes\n\n\nIn the learning theory outlined below in the following subsections, I aim to address how this issue could be managed within the learner‚Äôs grammar. The current proposal involves a mechanism based on constraint indexation (Pater, 2010) applied to the learning of liaison.\nThe second rather unique property of liaison pertains to the locus of the alternation in the allomorphs along with the order of the of specific morphemes relative to each other. As seen in examples (1‚Äí4) in Section 2.2. liaison is an external sandhi phenomenon which occurs in a domain of two morphemes where the first morpheme exhibits alternation on its right edge (e.g., [peti]~[petit]), bordering the left edge of the second morpheme. When the second morpheme begins with a vowel, the liaison consonant is resyllabified as an onset of the second morpheme. Therefore, especially in noun phrases where the second morpheme is a vowel-initial noun, the noun can be resyllabified with different onsets every time it is combined with a variety of liaison consonants in different contexts. Similar patterns are found in Celtic languages, which exhibit initial consonant mutation on their nouns in different contexts such as with pronouns or prepositions (Hannahs, 2011). However, such patterns are relatively uncommon and less widespread typologically compared to other cases of external allomorphy, where suffixation is more prevalent. As observed in the developmental trajectory of learning liaison, this presents challenges in the initial stages of learning morpho-phonological phenomena for learners, making it particularly challenging to correctly represent the input forms of morphemes involved in liaison.\nIn the following subsection, a detailed discussion of the issue regarding the input and its crucial role in a learnability account with an OT-based learning algorithm is presented. This is followed by an illustration of the formal properties that such a grammar must possess to enable learning.\n\n2.4.1 Learning the underlying representation\nIn the course of acquisition, a learner must simultaneously learn the ranking and the lexicon. The latter has been incorporated into the system as identity mapping based on the principle of lexicon optimization (Prince and Tesar, 2004). When learners are faced with no evidence of overt alternations, the chosen underlying form among possible input forms is the one that makes the most faithful input-output mapping. Learning the underlying representations of morphemes is crucial to the process of learning the allomorphy, as ‚Äúthe underlying representations influence the grammar and the grammar influences the underlying representations‚Äù (McCarthy, 2005: 25). However, it is important to note that in a real-life scenario of language acquisition, morphological analysis and segmentation must be integrated with other aspects of learning. Leveraging the paradigmatic information in the alternation for computing the underlying representation seems accessible to the analyst. However, it is deemed to be tractable for learners only at a stage where they have made progress in understanding the semantics and segmentation of the phrases they encounter as overt forms.\nIn cases of allomorphy, the underlying representations of morphemes and the alternation are highly interdependent and ‚Äúhave to be inferred from combined analytic assumptions about the output and the constraint hierarchy‚Äù (Kager, 2004: 322). To expand on this, it is useful to first consider a less complex example of allomorphy as a point of comparison where the alternation is motivated by a phonotactic constraint in the language2. In the previously mentioned example of the English past tense allomorphs (e.g., walk[t], jog[d], etc.), the conditions for the voicing alternations are true such that the suffix‚Äôs alveolar consonant is voiced after a voiced consonant and voiceless after a voiceless consonant. This distributional restriction also occurs within clusters in mono-morphemic words which are always similar in voicing. Having successfully learned the phonotactic constraints rankings governing the distributions of phonemes, in early stages of morphophonemic learning the learner needs to recognize the past tense morpheme as a suffix which means an underlying representation must be specified. Failing to select the appropriate underlying representation could result in the learner‚Äôs inability to learn the alternation given its existing constraint ranking. Theoretically, in the process of learning, when the learner makes errors on an alternation, two possible scenarios exist: either the constraint ranking within the grammar or the underlying representation would need to be revised. For the learners of the regular past tense allomorphy in English, modifying the grammar implies revising the constraint hierarchy which had been successful for mono-morphemic words. Any other constraint hierarchy would not be consistent with the rest of the learning data in English as a modified ranking could not apply to both the mono-morphemic words and words including the past tense suffix. Therefore, out of the two choices, the underlying representation is more likely to be revised. For learners of French, however, part of the challenge lies in the fact that liaison is not motivated by a phonotactic generalization that is language-wide. Furthermore, if the learner is to revise its underlying representations, as mentioned before, the unique location of the alternation in French liaison presents an additional layer of complexity for segmentation, which complicates the learning process of deriving underlying representation.\nThe information provided in the examples below can be categorized into two types: morphemic contrast and morphemic alternation (Tesar, 2014: 247). The following paradigm in (8) provides an opportunity for the learners to utilize the morphemic alternation information and compare phrases demonstrating diverse contexts for the same morpheme (in this case, plural determiner /le(z)/).\n\n\n\n\n\n\n\n\n\n(8)\n\n\n\n\n\n\n\na.\nles b√©b√©s\n[le.be.be]\n‚Äòthe babies‚Äô\n\n\nb.\nles amis\n[le.za.mi]\n‚Äòthe friends‚Äô\n\n\nc.\nles ours\n[le.zu. Ås]\n‚Äòthe bears‚Äô\n\n\n\nTo recognize the variation in the forms and to determine that both [le] and [lez] are two possible outputs for the same morpheme indicating the plural determiner, learners need to rely on their understanding of the underlying forms of the morphemes /bebe/ and /u Ås/ or /ami/. However, if the learner has not fully computed the underlying forms in the morphemic contrast paradigm for both the vowel-initial nouns (e.g., /u Ås/) and the consonant-initial nouns (e.g., /bebe/), their ability to learn the multiple representations for the allomorphy will be impacted. The issue specifically arises with vowel-initial nouns and their position. In the following contrastive paradigm in (9), the learners observe the noun /u Ås/ preceded by different morphemes.\n\n\n\n\n\n\n\n\n\n(9)\n\n\n\n\n\n\n\na.\nles ours\n[le.zu Ås]\n‚Äòthe bears‚Äô\n\n\nb.\nun ours\n[≈ìÃÉ.nu Ås]\n‚Äòa bear‚Äô\n\n\nc.\npetit ours\n[p…ô.ti.tu Ås]\n‚Äòlittle bear‚Äô\n\n\nd.\njoli ours\n[ í…î.li.u Ås]\n‚Äònice bear‚Äô\n\n\n\nIn this paradigm, the left edge of this noun in French is occupied by various liaison consonants (9a‚àíc), except when it is adjacent to a non-liaison word1, as in (9d) with [ í…îli]. This ordering results in an unstable position for the onset of the second word which has the potential to be rebracketed by the learners, as discussed in section 2.3. Understanding that these nouns do not have several C-initial variants and are in fact V-initial requires resisting the segmentation that aligns the onset of the nouns with the onset of the syllable. In phonological theories, aligning the onset of syllables and morphemes is part of the general alignment framework in the universal markedness constraints (McCarthy and Prince, 1993; Tesar and Smolensky, 2000). Consequently, particularly in early acquisition, it might not be surprising that this position of alternation contributes to how this issue is intuitively challenging to overcome. Furthermore, the role and saliency of onsets is also supported by various other psycholinguistic models of lexical segmentation such as the Syllable Onset Segmentation Hypothesis (Content et al., 2000), Shortlist B (Noris and McQueen, 2008).In the context of French liaison, the majority of word2s belong to an open class of lexical items (non-functional). Therefore, the need to resist alternations at the left edge of the word2s is crucial for the learning of the lexicon. As noted by Babineau et al.¬†(2023: 305), ‚ÄúFrench liaison violates this universal constraint, as liaison sacrifices to some degree the number of minimal pair words in the lexicon‚Äù. This also has direct implications for the task of morphological parsing and learning the underlying representations. For example, English learners can infer, due to this bias, that a set of minimally different words like foul, cowl, howl are separate words and not just variants of a vowel-initial word like owl. Conversely, for French learners, syllables like [zu Ås], [tu Ås], [nu Ås] are not separate words, but rather all share the same vowel-initial word [u Ås]. Babineau et al.¬†(2023) also note that this issue is not particularly severe, as the number of possible liaison consonants that can lead to ambiguous syllabification in French is limited to a few consonants.\nBy examining the issues surrounding underlying representations and segmentation, I have illustrated that learners must navigate the tension between language-specific properties and universal constraints in order to progress along the correct path. In the following subsection, I will propose a heuristic that could lead to a bias under which learning the underlying representations can be achieved for successful learning, and delve into the components of the initial state grammar along with the trajectory to the end state grammar.\n\n\n2.4.2 Initial state grammar to end state grammar\nIn the initial state of the grammar, markedness constraints fully outrank faithfulness constraints. This initial state is adopted as it is well-supported by the evidence in the literature of phonological acquisition in Optimality Theory. Smolensky (1996a; 1996b) and Gnanadesikan (1995) suggest this initial state can account for both children‚Äôs production and comprehension. It also makes better predictions for learning with no positive evidence of alternations per the Subset Problem (Smolensky, 1996a, Prince and Tesar, 2004). This assumption is also connected to the richness of the base principle in Optimality theory where any imaginable input may be considered since the outputs are constrained by the interaction of constraints. Davidson et al.¬†(2004) report results from experiments on the processing of infants and adults showing that this principle and its consequences for the initial ranking are supported by human linguistic performance. Apart from this traditional assumption, other possibilities within the literature include an unranked state or faithfulness constraints outranking markedness (Hale and Reiss, 1997).\nIn this analysis, I propose an initial state where all the learners of French start with the schematic Markedness &gt;&gt; Faithfulness constraint ranking with the constraints introduced below in (10).\n(10) The initial state grammar G0:\na. Markedness constraints: all speakers have structural constraints for the most unmarked syllable structure (CV.CV): NoV.V3, NoC.C4 as well as alignment constraints Align-L5.\nb. Faithfulness constraints: all speakers have constraints that control the input-output correspondence (i.e., penalizing deletion and insertion of segments): Max6, Dep7.\nc. These classes of constraints are ranked such that markedness constraints dominate the faithfulness constraint (M &gt;&gt; F):\nNoV.V, NoC.C, Align-L &gt;&gt; Max, Dep\nIn order to tackle the questions of learning morpho-phonological patterns, I focus on basic examples of prenominal configurations introduced above in section 2.2, such as the¬†common sequence of the plural determiner [le]/[lez] + nouns. Beginning from a universal initial state typical of learners of all languages, how does the grammar of learners of French develop as they are exposed to French-specific phonological patterns until they have fully learned morpheme-specific phonological alternations like liaison? The analysis provided here incorporates a mechanism that enables learners to recognize and address inconsistencies in their grammar adopting Pater (2010). This analysis is also informed by additional biases in two places, which will be elaborated upon later throughout this subsection. The learning algorithm implemented in the current analysis follows the biased error-driven constraint demotion algorithm (Prince and Tesar, 2004; Hayes, 2004). Implementing this algorithm, not only does the learner start with a markedness bias in its initial state, but it is also biased towards the markedness constraints during reranking stages or constraint demotion.\nIn the early stages of phonological development, as discussed in section 2.3, children learn language-specific phonotactics. For my analysis, I assume that prior to learning morphology, French learners acquire some phonotactic and distributional information about their language, which informs their language-specific knowledge. This suggests that during this early stage of learning, out of a tendency to maximize lexicon optimization, the initial bias of highly ranked universal constraints favouring unmarked structures would be gradually overridden by the set of constraints related to being faithful to the input form of words. In the error-driven process of learning, among the sets of phrases that contribute to learners‚Äô language-specific phonotactic knowledge is the non-alternating adjective + nouns in our examples. The non-alternating class of morphemes in general involves an open class of lexical items, making them shape most of the lexicon as they are more frequent type-wise. A learner who encounters phrases such as jeune b√©b√© and joli ami as well as many other similar verbal or prepositional phrases (containing V.V and C.C syllable patterns) simply stores each new word they learn in the input as is, based on the principles of lexicon optimization. The relevant errors learners make with regard to these phrases lead them to updating the initial state constraint ranking. During the update, the biased constraint demotion algorithm imposes a bias in favour of markedness constraints, therefore they are not to be demoted as long as possible\nTo illustrate how an ideal learning trajectory progresses at each step with an error-driven algorithm, first a table of error archives is presented, and then after resolving the errors, some example tableaux are provided with the new updated grammar at each stage. The table below shows the error archive and provides a pair of candidates consisting of the winning candidate (the observed output) and one losing candidate selected by the current grammar for each input. Each row in this error archive, which is known as the error, indicates whether the constraints prefer the optimal winner (W) or the¬†suboptimal competitor, the loser (L). The novelty in my analysis is the first column in the table called ‚ÄúError by Current Grammar‚Äù which notes whether the errors present in the error archive were made by the current state of the grammar. The current grammar of the learners, written on top of the table, has led them to the errors marked by ‚àö. Importantly, the concept of an error archive refers to a kind of permanent storage accessible to the algorithm where learners record their errors. If at any point during learning, the algorithm reads the archive and an error lacks any marks in the first column, it means that it has been archived in previous stages. At the same time, the inputs of input-output mappings that do not make it into the error archive are stored directly in the lexicon also accessible in phonological learning. At this point, the current grammar is the initial state grammar stated in (10).\n(11) Error archive with errors of the CG (NoV.V, NoC.C, Align-L &gt;&gt; Max, Dep)\n\n\n\n\n\n\n\n\n\n\n\n\n\nError by CG\nInput\nW ~ L\nNoC.C\nNoV.V\nAlign-L\nMax\nDep\n\n\n\n\n‚àö\na. / í…îli+ami/\n í…îli.ami ~  í…îli.zami\n\nL\nW\n\nW\n\n\n‚àö\nb. / í…îli+ami/\n í…îli.ami ~  í…î.lami\n\nL\nW\nW\n\n\n\n‚àö\nc. / í≈ìn+bebe/\n í≈ìn.bebe ~  í≈ì.bebe\nL\n\n\nW\n\n\n\n‚àö\nd. / í≈ìn+bebe/\n í≈ìn.bebe ~  í≈ìni.bebe\nL\n\n\n\nW\n\n\n‚àö\ne. / í≈ìn+ami/\n í≈ì.nami ~  í≈ì.ami\n\nW\nL\nW\n\n\n\n\nAs can be seen from the errors above, the very initial state grammar leads the learner to select these losing candidates which have unmarked syllable structures. Once new errors enter the error archive marked by ‚àö, the error-driven algorithm then requires an update and initiates a new constraint hierarchy according to the violation profiles of each candidate pair in terms of their constraint preference. With the biased constraint demotion algorithm, markedness constraints have priority to be placed at the top of the new stratum of the constraint hierarchy provided they prefer no losers. However, this is not the case at this stage, as there are not any markedness constraints which would prefer no losers. Whereas faithfulness constraints, Max and Dep, prefer the winning candidates of the pairs in each error. It is worthwhile to note that including errors that result in reranking Align-L in the first error archive, as seen in cases like error (11e) serves the purpose of providing a comprehensive analysis of all constraints. However, realistically, during the initial stages of phonotactic acquisition, learners may not be fully aware of morphological boundaries within phrases8. In this update, the resulting crucial constraint ranking in (12), achieved by the learner during the phonotactic learning stage, ensures that such errors are not being made anymore moving forward. Thus, all the errors in (11a-e) are then successfully archived.\n(12) Phonotactic Grammar: Dep, Max &gt;&gt; NoV.V, NoC.C, Align-L\n\n\n\n\n\n\n\n\n\n\n\n\n\n/ í≈ìn/+/bebe/\nDep\nMax\nNoV.V\nNoC.C\nAlign-L\n\n\n\n\n‚òû\n[ í≈ìn.bebe]\n\n\n\n*\n\n\n\n\n[ í≈ì.ni.bebe]\n*!\n\n\n\n\n\n\n\n[ í≈ì.bebe]\n\n*!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/ í≈ìn/+/ami/\nDep\nMax\nNoV.V\nNoC.C\nAlign-L\n\n\n\n\n‚òû\n[ í≈ì.na.mi]\n\n\n\n\n*\n\n\n\n[ í≈ì.a.mi]\n\n*!\n*\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/ í…îli/ +/ami/\nDep\nMax\nNoV.V\nNoC.C\nAlign-L\n\n\n\n\n‚òû\n[ í…î.li.a.mi]\n\n\n*\n\n\n\n\n\n[ í…î.li.za.mi]\n*!\n\n\n\n*\n\n\n\n[ í…î.la.mi]\n\n*!\n\n\n*\n\n\n\nThe important argument made here for the grammar in (12) is based on a reasonable assumption that when learners arrive at the inconsistency between liaison and non-liaison morphemes, they already possess a not-so-na√Øve grammar. The grammar with which they enter morphophonemic learning is not exactly the same as the initial state grammar and has undergone some modifications. This is illustrated in the tableaux above and evidenced by an error archive that accumulates as they encounter simple phrases during phonotactic learning.\nGradually, learning the underlying representations of the alternating morphemes becomes more feasible for the learner after gaining some knowledge about the phonotactics and the lexicon. When learners develop a certain level of awareness regarding morphological parsing through semantic or visual cues in the real world, they start forming hypotheses about the additional morpheme they have identified before the nouns in determiner + noun phrases. In other words, the learner eventually must realize that the phrases les amis and les b√©b√©s are not monomorphemic and contain more than one morpheme. At the same time, they could conclude that the first morphemes of these phrases are, in fact, semantically related and function as determiners. Incidentally, one type of evidence that can aid them in learning the underlying representation of the alternating liaison morphemes could come from the knowledge of nouns‚Äô underlying representations, especially when learners compare different contrastive paradigms in (9). For instance, as learners become more proficient in understanding the representations of the nouns, they may find it more likely that among the forms they had encountered for the noun ‚Äòfriend‚Äô (e.g., [tami], [zami] and [ami]), the underlying representation is /ami/ rather than the other C-initial forms. This realization could in fact lead them to segment the phrases correctly and discover that the first morphemes in phrases like /lez+ami/ or /le+bebe/ are not only semantically related, but are actually the same morpheme. By exploring different options for the underlying representations of the determiner, learners can formulate a range of hypotheses. First, I propose that learners prefer to maintain a single underlying representation rather than positing multiple ones. This approach narrows the hypotheses window and it simplifies the learning process by shifting the responsibility of learning onto the grammar, rather than requiring extensive storage in the lexicon. In this scenario, learners would be faced with two possibilities and they must select one as the underlying form: either the input is shaped as CVC or CV. The two tables below are provided just to illustrate the comparison between the larger input in CVC form (13a-d), and the smaller input in CV form (13e-h). If the learner takes the hypothesis of larger allomorph as the input such as /lez/ or /≈ìÃÉn/, the resulting errors would have the following preferences where Dep is indifferent with respect to winners or losers.\n(13)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInput\nW ~ L\nNoC.C\nNoV.V\nAlign-L\nMax\nDep\n\n\na.\n/lez/ + ‚Ä¶\nle.zami ~ le.ami\n\nW\nL\nW\n\n\n\nb.\n/lez/ + ‚Ä¶\nle.bebe ~ lez.bebe\nW\n\n\nL\n\n\n\nc.\n/≈ìÃÉn/ + ‚Ä¶\n≈ìÃÉ.nami ~ ≈ìÃÉ.ami\n\nW\nL\nW\n\n\n\nd.\n/≈ìÃÉn/ + ‚Ä¶\n≈ìÃÉ.bebe ~ ≈ìÃÉn.bebe\nW\n\n\nL\n\n\n\n\nIf the learner takes the hypothesis of the smaller allomorph as the input such as /le/ or /≈ìÃÉ/, the resulting errors would have different preferences where Max is indifferent with respect to winners or losers.\n(13)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInput\nW ~ L\nNoC.C\nNoV.V\nAlign-L\nMax\nDep\n\n\ne.\n/le/ + ‚Ä¶\nle.bebe ~ lez.bebe\nW\n\n\n\nW\n\n\nf.\n\nle.zami ~ le.ami\n\nW\nL\n\nL\n\n\ng.\n/≈ìÃÉ/ + ‚Ä¶\n≈ìÃÉ.bebe ~ ≈ìÃÉn.bebe\nW\n\n\n\nW\n\n\nh.\n\n≈ìÃÉ.nami ~ ≈ìÃÉ.ami\n\nW\nL\n\nL\n\n\n\nMy proposed heuristic could be in form of a bias in favour of storing the unpredictable information in the input by maximizing the structure encoded in the input. Under this heuristic, the larger allomorph /lez/ or /≈ìÃÉn/ (13a-d) would be the best underlying representation among the possible input forms. In other words, in instances of determiner + V-initial noun combinations when the learner has the opportunity to encounter the allomorph form with a maximal structure (here the CVC form), that should be the preferred choice9. With this input the grammar relies on satisfying the NoC.C constraint at the expense of Max, that is, not being faithful to the input by deleting the final consonant10. Other existing alternative analyses in the literature regarding French liaison in an OT-based grammar employ other mechanisms such as constraints for the UR (Smith, 2015) or multiple URs (Storme, 2024). However, applying this simple heuristic renders the input in conjunction with the ranking in the grammar learnable. As a result of choosing the larger allomorph for the underlying representation, learners are left with errors with the error proflies exemplified below in (14). Note that the other faithfulness constraint Dep is not playing a role among the crucial constraints anymore.\n(14)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInput\nW ~ L\nNoC.C\nNoV.V\nAlign-L\nMax\nDEP\n\n\n\n\na.\n/lez+ami/\nle.zami ~ le.ami\n\nW\nL\nW\n\n\n\nb.\n/lez+bebe/\nle.bebe ~ lez.bebe\nW\n\n\nL\n\n\n\n\nTo continue learning and to achieve a stable end-state of the grammar, the learner must reconcile the morpheme-specific constraints in two conflicting rankings within its grammar. In my current learnability analysis, I incorporate a mechanism borrowed from Pater‚Äôs (2010) constraint indexation model of Optimality Theory to analyze morpheme-specific phonology and exceptionality. He argues that this approach to morpheme-specific phonology offers advantages over previous proposals such as co-phonologies by Inkelas and Zoll (2007). Although similar mechanisms of indexation have been previously proposed by Fukuzawa (1999) and Ito and Mester (2001) for faithfulness constraints, Pater‚Äôs (2010) version allows both markedness constraints and faithfulness constraints to be lexically indexed if necessary. According to this approach, inconsistencies can be identified and resolved as the learner encounters them through facing errors during the learning process. The error archive below demonstrates a winner-loser pair for each input, representing the two classes of morphemes. The inconsistency is evident in error rows bolded (15c vs 15d) as there is not a consistent constraint that prefers only Ws. Here each constraint favours a morpheme belonging to a different class and the¬†reranking algorithm reaches a halt. The current grammar of the learners making the last two errors (15d-e) marked below with ‚àö is the grammar after phonotactic learning in (12). The other rows (15a-c) are the previous errors stored in the error archive11.\n(15) Error archive with errors of the CG (Max &gt;&gt; NoV.V, NoC.C, Align-L)\n\n\n\n\n\n\n\n\n\n\n\n\nError by CG\nInput\nW ~ L\nMax\nNoC.C\nNoV.V\nAlign-L\n\n\n\n\n\na. NL W1: / í…îli+ami/\n í…îliami ~  í…îlami\nW\n\nL\nW\n\n\n\nb. NL W1: / í≈ìn+ami/\n í≈ì.na.mi ~  í≈ì.ami\nW\n\nW\nL\n\n\n\nc.¬†NL W1: / í≈ìn+bebe/\n í≈ìn.bebe ~  í≈ì.bebe\nW\nL\n\n\n\n\n‚àö\nd.¬†L W1s: /lez+bebe/\nle.bebe ~ lez.bebe\nL\nW\n\n\n\n\n‚àö\ne. L W1s: /lez+ami/\nle.za.mi ~ le.ami\nW\n\nW\nL\n\n\n\nFollowing the instructions in the algorithm under Pater‚Äôs (2010: 20, ex 48) account for inconsistency resolution, the learner must perform the instruction given in (16).\n(16) Clone a constraint that prefers only Ws in all instances of some morpheme, and index it to every morpheme for which it prefers only Ws.\nAs Pater notes in his work, an aspect of constraint cloning remains to be fully elaborated. Particularly, how does the learner proceed when the choice of the constraint to be cloned is not straightforwardly down to one single constraint? The appendix in the original work by Pater (2010) reviews procedures that could be performed by the algorithm (borrowed from linear programming) during learning when the algorithm can not decide which constraint to clone to resolve the inconsistency. He also discusses one option to be the inclusion of an ‚Äúexceptional‚Äù bias targeting the smallest set of morphemes to help the learner choose one solution over the other depending on the lexicon, but he concludes that this procedure needs further research.\nMy analysis includes the proposal of a bias which could enable the learner to decide among equally available options for constraint cloning. This bias is shaped by considering the current grammar which is itself driven by earlier errors in the error archive. An explicit adjustment to the previous statement is stated in (17).\n(17) Clone the constraint that prefers Ws in all instances of the errors committed by the current grammar and index it to morphemes for which it prefers only Ws for those errors.\nThe minimal inconsistent pairs of errors for French learners /lez+bebe/ and / í≈ìn+bebe/ are shown again in a new table below in (18).\n(18) Minimal inconsistent pairs of errors by the CG (Max &gt;&gt; NoV.V, NoC.C, Align-L)\n\n\n\n\n\n\n\n\n\n\n\n\nError by CG\nInput\nW ~ L\nMax\nNoC.C\nNoV.V\nAlign-L\n\n\n\n\n\nNL W1: / í≈ìn+bebe/\n í≈ìn.bebe ~  í≈ì.bebe\nW\nL\n\n\n\n\n‚àö\nL W1s: /lez+bebe/\nle.bebe ~ lez.bebe\nL\nW\n\n\n\n\n\nAt this juncture, the choice of cloning the winning preferring constraint is not apparent. The two options below are both equally available to the learner to resolve the inconsistency:\n\nClone No C.C indexed to liaison W1s (e.g., /lez/)\nClone Max indexed to non-liaison W1s (e.g., / í≈ìn/)\n\nAs mentioned before, the important assumption taken for granted is that learners of French have a different (more mature) grammar from the initial state grammar at the point when they reach this inconsistency. As can be seen from the winner~loser pair above, currently, non-liaison word1s do not lead to an error by the learner anymore as the W preferring constraint for them is Max which is highly-ranked in the current grammar. A word1 in the liaison class (L W1) is the recent error made by the current grammar. Therefore, guided by the bias introduced in (17) they must clone the constraint preferring Ws for the liaison word1 morpheme (NoC.C), as it is the error committed by the current grammar and index it to that morpheme (NoC.C-L). The clone constraint is added to the constraint hierarchy as shown in (19).\n(19)\n\n\n\n\n\n\n\n\n\n\n\n\n\nError by CG\nInput\nW ~ L\nNo C.C-L\nMax\nNo C.C\nNo V.V\nAlign-L\n\n\n\n\n\na. NLW1: / í…îli+ami/\n í…îliami ~  í…îlami\n\nW\n\nL\nW\n\n\n\nb. NLW1: / í≈ìn+ami/\n í≈ì.na.mi ~  í≈ì.ami\n\nW\n\nW\nL\n\n\n\nc.¬†NLW1: / í≈ìn+bebe/\n í≈ìn.bebe ~  í≈ì.bebe\n\nW\nL\n\n\n\n\n‚àö\nd.¬†LW1s: /lez+bebe/\nle.bebe ~ lez.bebe\nW\nL\nW\n\n\n\n\n‚àö\ne. LW1s: /lez+ami/\nle.za.mi ~ le.ami\nW\nW\n\nW\nL\n\n\n\nNow with the new cloned constraint present, the reranking procedure in the update can be restarted. I will illustrate the biased constraint demotion algorithm here step-by-step. Among markedness constraints, the NoC.C-L constraint is placed on the top stratum, shown in (20), as it is the only markedness constraint that prefers only Ws in an instance of some morpheme. After this constraint is placed at the top of the hierarchy the relevant errors in rows (d-e) would technically be resolved and successfully archived.\n(20) No C.C-L &gt;&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\nError by CG\nInput\nW ~ L\nNo C.C-L\nMax\nNo C.C\nNo V.V\nAlign-L\n\n\n\n\n\na. NLW1: / í…îli+ami/\n í…îliami ~  í…îlami\n\nW\n\nL\nW\n\n\n\nb. NLW1: / í≈ìn+ami/\n í≈ì.na.mi ~  í≈ì.ami\n\nW\n\nW\nL\n\n\n\nc.¬†NLW1: / í≈ìn+bebe/\n í≈ìn.bebe ~  í≈ì.bebe\n\nW\nL\n\n\n\n\n‚àö\nd.¬†LW1s: /lez+bebe/\nle.bebe ~ lez.bebe\nW\nL\nW\n\n\n\n\n‚àö\ne. LW1s: /lez+ami/\nle.za.mi ~ le.ami\nW\nW\n\nW\nL\n\n\n\nAs the inconsistency is now resolved, the update can proceed with a similar procedure as before based on the winner~loser pairs in (20a-c). No markedness constraint prefers only Ws, therefore the faithfulness constraint is the next constraint to be placed in the new stratum. At the end, the learner has the hierarchy shown in (21) with a cloned constraint indexed to the set of morpheme(s) with the same index (here L chosen to stand for liaison morphemes).\n(21)\nNo C.C-L &gt;&gt; Max &gt;&gt; No C.C, No V.V, Align-L *L: {/lez/, ‚Ä¶}\nThe two tableaux below (22) show examples of phrases attached to an¬†indexed subset of the lexicon [lebebe] and [lezami]. Notice that the underlying representation of the word1 is the consonant-final form /lez/ which means NoC.C-L12 has to crucially outrank Max and Max has to in turn crucially outrank Align-L.\n(22)\n\n\n\n\n\n\n\n\n\n\n\n\n\n/lez/L+/bebe/\nNo C.C-L\nMax\nNo C.C\nNo V.V\nAlign-L\n\n\n\n\n‚òû\n[le.bebe]\n\n*\n\n\n\n\n\n\n[lez.bebe]\n*!\n\n*\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/lez/L+/ami/\nNo C.C-L\nMax\nNo C.C\nNo V.V\nAlign-L\n\n\n\n\n‚òû\n[le.za.mi]\n\n\n\n\n*\n\n\n\n[le.ami]\n\n*!\n\n*\n\n\n\n\nIt is also worth illustrating in (23) that this new cloned constraint does not affect words like [ í≈ìnbebe] or [ í…îliami] because it is only indexed to certain word1s (liaison word1s).\n(23)\n\n\n\n\n\n\n\n\n\n\n\n\n\n/ í≈ìn/+/bebe/\nNoC.C-L\nMax\nNoC.C\nNoV.V\nAlign-l\n\n\n\n\n‚òû\n[ í≈ìn.bebe]\n\n\n*\n\n\n\n\n\n[ í≈ì.bebe]\n\n*!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/ í…îli/+/ami/\nNoC.C-L\nMax\nNoC.C\nNoV.V\nAlign-l\n\n\n\n\n‚òû\n[ í…î.li.a.mi]\n\n\n\n*\n\n\n\n\n[ í…î.la.mi]\n\n*!\n\n\n\n\n\n\nA crucial concluding step remains in this analysis. The process outlined above addresses the procedure for instances of a single morpheme. Although multiple liaison morphemes in French need to be indexed, they constitute a finite set. Therefore, whenever learners encounter phrases containing new morphemes that fall within the same class as liaison words, such as the masculine singular indefinite article + noun in un ami, the same procedure applies to resolve any inconsistencies. In this case, the cloned constraints for each morpheme would essentially be duplicates of the NoC.C constraint and positioned within the same top stratum (24).\n(24)\nNo C.C-L2**, No C.C-L* &gt;&gt; Max &gt;&gt; No C.C, No V.V, Align-L *L: {/lez/} **L2: {/≈ìÃÉn/}\nTherefore, we expect that learners may have the option to either merge or delete the extra constraints, ultimately retaining a single constraint linked to multiple indexed liaison words instead. The complete final grammar in the current analysis where learners would converge on an end state, including all the relevant constraints, therefore would be as in (25).\n(25) Final state grammar:\nNo C.C -L* &gt;&gt; Max &gt;&gt; No C.C, No V.V, Align-L *L: {/lez/, /≈ìÃÉn/, ‚Ä¶}\nIt is beneficial at the end of this subsection to highlight that several key points emerge from this analysis based on certain underlying assumptions regarding an ideal learner. Firstly, it is assumed that the learners are equipped with a universal biased M&gt;&gt;F ranking in their initial state which leads to their first errors during phonotactic learning. Subsequently, another assumption posits that as a result of successful phonotactic learning, the ideal learner maintains a specific ranking of the grammar at the time of encountering inconsistencies and during the learning of allomorphy. This ideal trajectory shapes the bias that enables the learners to successfully overcome the challenge of learning morpheme-specific alternations in this analysis. As previously mentioned, other alternative biases for learning indexation in morpheme-specific patterns according to Pater (2010) include a bias for the smallest set of morphemes or constraints. In this regard, it remains ambiguous whether a definitive solution exists for assessing the relative frequency of liaison morphemes compared to non-liaison morphemes, as well as whether such criteria could be appropriately applied to these morphemes in a phonologically conditioned allomorphy domain. Therefore, an additional crucial criterion favouring the errors made by the current grammar was introduced and incorporated in this analysis.\nIn an earlier work on constraint indexation, Pater (2007:11) introduced the concept of constraint indexation in a context where the order of winner-loser pairs created by the learners is not explicitly addressed. He mentioned in a footnote that if a learner possesses an incomplete set of learning data, there is a risk of incorrectly cloning certain constraints. However, he then posits that this would likely have minimal impact, as these constraints would eventually be ranked lower in the hierarchy if they do not contribute to resolving inconsistencies. To further explore this idea with hypothetical learning processes, I have devised various experimental scenarios that control the sequence in which participants are exposed to different types of learning data. Additionally, I ensured that the experimental environment did not allow learners to exhibit exceptional biases based on familiarity with the lexicon or frequency. In the following section, I will review these conditions and discuss the predictions derived from them, enabling comparisons based on participants‚Äô accuracy and errors.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Background and Theoretical Framework</span>"
    ]
  },
  {
    "objectID": "chapters/2_backgroundandtheory.html#predictions-of-the-learning-theory-in-the-current-study",
    "href": "chapters/2_backgroundandtheory.html#predictions-of-the-learning-theory-in-the-current-study",
    "title": "2¬† Background and Theoretical Framework",
    "section": "2.5 Predictions of the learning theory in the current study",
    "text": "2.5 Predictions of the learning theory in the current study\nThe current study aims to compare the learning of morpheme-specific phonological patterns across various controlled learning trajectories. The order in which learning data was presented to the participants was carefully controlled in an experimental setting designed to induce an inconsistency detection in the learner‚Äôs learning algorithm. Before expanding on my predictions regarding the different learning trajectories, it is important to establish the assumptions guiding the hypotheses. The most crucial assumption is that, in all learners in the experiment, the initial state aligns with the state described in section 2.4.2. This means that, rather than possessing a set of unranked constraints, learners exhibit strong biases toward the most unmarked syllable structure (CV.CV) (for related points about the methodology of artificial language learning experiment and to the extent where L1 biases are transferred, see Tang and Baer-Henney, 2023) .\nTherefore, I assume that in an experimental setting, the initial state of all learners follows the G0 below in (26) with faithfulness constraints being outranked by markedness constraints.\n(26)\nG0: NoC.C, NoV.V, Align-L &gt;&gt; Max\nMoreover, as per my analysis discussed above following Pater (2010), I assume that learners would update their grammar based on the learning evidence presented to them at any given point. This grammar operates within a single hierarchy of constraints, requiring learners to rerank their constraints and make updates when faced with new evidence of errors or of inconsistencies. In the experiment, I investigated three different learning trajectories, assuming all are starting from the same initial state grammar (26) but exposed to merely positive learning data. These trajectories involved receiving either both classes of morphemes at once or in an isolated manner, one by one. The learning data consisted of phrases in an artificial mini-language that included similar patterns to the morpheme-specific phonological patterns in French. This mini-language consists of four word1s (two liaison, and two non-liaison) along with several word2s and its presentation to participants in the experiment is divided into two blocks (for more details, refer to Chapter 3). In the next subsections, I draw upon real French phrases to demonstrate the three learning trajectories and examine the outcome predicted by my hypotheses.\n\n2.5.1 Ordered learning trajectory - Liaison first\nIn this group of learners, the first block involves receiving two liaison morphemes while the second block includes the two non-liaison morphemes. In this manner, the initial evidence presented to learners based on what they encounter in the first block lacks any form of inconsistency.\nIn the first block, the ideal learner who has established the semantic connection between the allomorphs needs to identify the larger consonant-final allomorph /lez/ and /≈ìÃÉn/ as the underlying representation when presented with the following set of phrases: {le+bebe, lez+ami, ‚Ä¶, ≈ìÃÉ+bebe, ≈ìÃÉn+ami, ‚Ä¶}. By the end of the first block, assuming an initial state bias from G0, the learner does not make any errors with [lebebe] or [≈ìÃÉbebe]. However, the learner is required to rerank its constraint hierarchy due to errors introduced by Align-L which only occur with phrases such as [lezami] and [≈ìÃÉnami]13 shown in (27).\n(27)\n\n\n\n\n\n\n\n\n\n\n\na.\n/lez/+/bebe/\nNoC.C\nNoV.V\nAlign-L\nMax\n\n\n\n\n\n[lez.bebe]\n*!\n\n\n\n\n\n‚òû\n[le.bebe]\n\n\n\n*\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nb.\n/lez/+/ami/\nNoC.C\nNoV.V\nAlign-L\nMax\n\n\n\n\n‚òπ\n[le.zami]\n\n\n*!\n\n\n\n‚òû\n[le.ami]\n\n*!\n\n*\n\n\n\nAt this stage, the input forms /lez/ and /bebe/ directly enter the lexicon of the learner as there are no errors made on them. However, the phrase leading to the error in (27b) are shown in table (28) below.\n(28) Error archive with errors of the CG (NoC.C, NoV.V, Align-L &gt;&gt; Max)\n\n\n\n\n\n\n\n\n\n\n\n\nErrors by CG\nInput\nW ~ L\nNoC.C\nNoV.V\nAlign-L\nMax\n\n\n‚àö\na. L W1: /lez+ami/\nle.zami ~ le.ami\n\nW\nL\nW\n\n\n‚àö\nb. L W1: /≈ìÃÉn+ami/\n≈ìÃÉ.na.mi ~ ≈ìÃÉ.ami\n\nW\nL\nW\n\n\n\nAs the table in (28) shows, the two markedness constraints that prefer no losers are NoC.C and NoV.V. The biased constraint demotion would result in a crucial ranking among the markedness constraints, so they are no longer equally ranked in the top stratum of the hierarchy in (29). The two errors in (28a‚Äíb) are archived successfully, as the learner proceeds.\n(29)\nLiaison G1: No C.C, No V.V &gt;&gt; Align-L &gt;&gt; Max\nIn the second block of learning, learners are presented with the non-liaison morphemes only: { í≈ìn+bebe,  í≈ìn+ami, ‚Ä¶,  í…îli+ami,  í…îli+bebe, ‚Ä¶}, and they would make errors with these new word1s with their current grammar in G1 shown in tableaux below in (30).\n(30)\n\n\n\n\n\n\n\n\n\n\n\n\n/ í≈ìn/+/bebe/\nNoC.C\nNoV.V\nAlign-L\nMax\n\n\n\n\n‚òπ\n[ í≈ìn.bebe]\n*!\n\n\n\n\n\n‚òû\n[ í≈ì.bebe]\n\n\n\n*\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/ í…îli/+/ami/\nNoC.C\nNoV.V\nAlign-L\nMax\n\n\n\n\n‚òπ\n[ í…îli.ami]\n\n*!\n\n\n\n\n‚òû\n[ í…î.lami]\n\n\n*\n*\n\n\n\nDue to the errors recorded in their error archive recently as exemplified in the table below in (31), they are required to rerank and update their highest-ranked constraints from G1. Errors (31a‚Äíb) are previous errors from the first block stored in the error archive and errors (31c‚Äíd) are errors made by the current grammar marked by ‚àö.\n(31) Error archive with errors of the CG (No C.C, No V.V &gt;&gt; Align-L &gt;&gt; Max)\n\n\n\n\n\n\n\n\n\n\n\n\nErrors by CG\nInput\nW ~ L\nNoC.C\nNoV.V\nAlign-L\nMax\n\n\n\na. L W1: /lez+ami/\nle.zami ~ le.ami\n\nW\nL\nW\n\n\n\nb. L W1: /≈ìÃÉn+ami/\n≈ìÃÉ.na.mi ~ ≈ìÃÉ.ami\n\nW\nL\nW\n\n\n‚àö\nc. NL W1: / í≈ìn+bebe/\n í≈ìn.bebe ~  í≈ì.bebe\nL\n\n\nW\n\n\n‚àö\nd. NLW1: / í…îli+ami/\n í…îli.ami ~  í…î.lami\n\nL\nW\nW\n\n\n\nAs the table in (31) shows, no markedness constraint prefers only Ws, therefore the winner-preferring faithfulness constraint Max is ranked first at the top stratum in (32) and the other constraints are basically demoted.\n(32)\nLiaison G2: Max &gt;&gt; No C.C, No V.V, Align-L\nDuring the second block, the learners are only exposed to evidence of the non-liaison morphemes. However, the ideal learner not only has a complete archive of the errors, but they also have access to the lexicon they learned in this language up until this point. Even though they do not encounter them in this block, they must notice that the phrases in the second block differ significantly from the earlier phrases to eventually realize that the current G2 grammar will produce errors with /lez+bebe/ and other phrases with a similar structure (‚Ä¶VC.CV‚Ä¶) as demonstrated in a tableau in (33). Recall that these input to these phrases, previously shown in a tableau in (27a), had entered the lexicon directly with no issues in the first block.\n(33)\n\n\n\n\n\n\n\n\n\n\n\n\n/lez/+/bebe/\nMax\nNoC.C\nNoV.V\nAlign-L\n\n\n\n\n‚òû\n[lez.bebe]\n\n*\n\n\n\n\n‚òπ\n[le.bebe]\n*!\n\n\n\n\n\n\nNow powered by the G2, this recent error initiates an update to the grammar. As the learner compiles all evidence in the archive, they would realize the necessity of addressing the ranking paradox between the morphemes belonging to the two classes of word1s. The table below, which includes all the errors, illustrates the minimal inconsistent pair that highlights the inconsistency (34d vs 34e) in bold‚Äîwhere specific constraints (Max and NoC.C) exhibit the opposite preferences for the morphemes presented in the two blocks of the experiment. The learner is unable to continue the reranking process to install any constraints at this point.\n(34) Error archive with errors of the CG (Max &gt;&gt; No C.C, No V.V, Align-L)\n\n\n\n\n\n\n\n\n\n\n\n\nErrors by CG\nInput\nW ~ L\nMax\nNo C.C\nNo V.V\nAlign-L\n\n\n\na. LW1: /lez+ami/\nle.zami ~ le.ami\nW\n\nW\nL\n\n\n\nb. LW1: /≈ìÃÉn+ami/\n≈ìÃÉ.na.mi ~ ≈ìÃÉ.ami\nW\n\nW\nL\n\n\n\nc. NLW1: / í…îli+ami/\n í…îli.ami ~  í…î.lami\nW\n\nL\nW\n\n\n\nd.¬†NLW1: / í≈ìn+bebe/\n í≈ìn.bebe ~  í≈ì.bebe\nW\nL\n\n\n\n\n‚àö\ne. LW1: /lez+bebe/\nle.bebe ~ lez.bebe\nL\nW\n\n\n\n\n‚àö\nf. LW1: /≈ìÃÉn+bebe/\n≈ìÃÉ.bebe ~ ≈ìÃÉn.bebe\nL\nW\n\n\n\n\n\nFollowing the instruction in (17) to ‚Äúclone the constraint that prefers Ws in all instances of the errors committed by the current grammar and index it to morphemes for which it prefers only Ws for those errors‚Äù, the learners can activate their bias favouring the errors made by the current grammar marked by ‚àö when resolving the inconsistency. Therefore, they would clone the relevant constraint that prefers Ws for the errors made by the current grammar (NoC.C-L) indexed to those morphemes /lez/ and /≈ìÃÉn/ as shown in (34). With the addition of the new constraint, now there is a constraint that prefers only Ws with no inconsistencies (NoC.C-L), therefore that constraint(s) is ranked at the top stratum. After that, the errors (34e‚Äíf) are successfully resolved and the reranking can proceed as usual resulting in the final grammar below in (35).\n(35)\nLiaison G2.2: No C.C-L2**, No C.C-L1* &gt;&gt; Max &gt;&gt; NoC.C, Align-L, NoV.V\n*L1: {/lez/}, **L2: {/≈ìÃÉn/}\nEventually the learner then merges the two No C.C-L constraints under one constraint in the final grammar as shown in (36).\n(36)\nLiaison Final G: No C.C-L* &gt;&gt; Max &gt;&gt; No C.C, No V.V, Align-L14 *L: {/lez/, /≈ìÃÉn/}\n\n\n2.5.2 Ordered learning trajectory - Non-liaison first\nIn this group of learners, the first part involves receiving two non-liaison morphemes while the second part includes the two liaison morphemes. In this manner, the initial evidence presented to learners based on what they encounter in the first block lacks any form of inconsistency.\nIn the first block, learners encounter evidence of the following set of non-liaison phrases: { í≈ìn+bebe,  í≈ìn+ami, ‚Ä¶ &  í…îli+ami,  í…îli+bebe, ‚Ä¶}. Since the learning data does not contain alternations, the ideal learner is essentially only required to perform lexicon optimization and store underlying representations which are identical to the output. By the end of the first block, assuming an initial state bias from G0, the learner does make errors with examples such as [ í≈ìnbebe] or [ í…îliami]15 shown in (37).\n(37)\n\n\n\n\n\n\n\n\n\n\n\na.\n/ í≈ìn/+/bebe/\nNoC.C\nNoV.V\nAlign-L\nMax\n\n\n\n\n‚òπ\n[ í≈ìn.bebe]\n*!\n\n\n\n\n\n‚òû\n[ í≈ì.bebe]\n\n\n\n*\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nb.\n/ í…îli/+/ami/\nNoC.C\nNoV.V\nAlign-L\nMax\n\n\n\n\n‚òπ\n[ í…îli.ami]\n\n*!\n\n\n\n\n‚òû\n[ í…î.lami]\n\n\n*!\n*\n\n\n\nBased on the recent errors present in their error archive as exemplified in the table below in (38), they are required to rerank and update their highest-ranked constraints from the initial state in G0.\n(38) Error archive with errors of the CG (NoC.C, NoV.V, Align-L &gt;&gt; Max)\n\n\n\n\n\n\n\n\n\n\n\n\nErrors by CG\nInput\nW ~ L\nNoC.C\nNoV.V\nAlign-L\nMax\n\n\n\n\n‚àö\na. NLW1: / í…îli+ami/\n í…îli.ami ~  í…î.lami\n\nL\nW\nW\n\n\n‚àö\nb. NL W1: / í≈ìn+ami/\n í≈ì.na.mi ~  í≈ì.ami\n\nW\nL\nW\n\n\n‚àö\nc. NL W1: / í≈ìn+bebe/\n í≈ìn.bebe ~  í≈ì.bebe\nL\n\n\nW\n\n\n\nDue to the fact that both types of marked structures (V.V and C.C) as well as syllable misalignments are allowed in the output and the inputs remain the same in their outputs without any alternations, a reversal of the faithfulness and markedness constraints in the constraint hierarchy is bound to occur. Hence, the learner starting from G0, must update its grammar with a biased constraint demotion algorithm and in doing so all markedness constraints must be installed after Max, since none of them exclusively prefer Ws as shown in G1 (39).\n(39)\nNon-liaison G1: Max &gt;&gt; No C.C, No V.V, Align-L\nIn the second block of their learning, they are presented with the liaison morphemes only: {le+bebe, lez+ami, ‚Ä¶, ≈ìÃÉ+bebe, ≈ìÃÉn+ami, ‚Ä¶}. It is crucial that the ideal learner establishes the semantic connection between the allomorphs and identifies the larger consonant-final allomorph /lez/ and /≈ìÃÉn/ as the underlying representation employing the heuristic that we introduced in section 2.4.2. Such a learner relying on a uniform input of /lez/ would make errors on phrases such as [lebebe] or [≈ìÃÉbebe] following the previous grammar in G1, shown in (40).\n(40)\n\n\n\n\n\n\n\n\n\n\n\n\n/lez/+/bebe/\nMax\nNoC.C\nNoV.V\nAlign-L\n\n\n\n\n‚òû\n[lez.bebe]\n\n*\n\n\n\n\n‚òπ\n[le.bebe]\n*!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/≈ìÃÉn/+/bebe/\nMax\nNoC.C\nNoV.V\nAlign-L\n\n\n\n\n‚òû\n[≈ìÃÉn.bebe]\n\n*\n\n\n\n\n‚òπ\n[≈ìÃÉ.bebe]\n*!\n\n\n\n\n\n\nThese recent errors need to be present in the error archive. As the learners compile all evidence in the archive, they realize the necessity of addressing the ranking paradox between the morphemes belonging to the two classes of word1s. The table below, which includes all the errors, illustrates the minimal inconsistent pair that highlights the inconsistency (41c vs 41e) in bold‚Äîwhere the specific constraints (Max and NoC.C) exhibit the opposite preferences for the morphemes presented in the two blocks of the experiment. The learner is unable to continue the reranking process and to install any constraints at this juncture.\n(41) Error archive with errors of the CG (Max &gt;&gt; No C.C, No V.V, Align-L)\n\n\n\n\n\n\n\n\n\n\n\n\nErrors by CG\nInput\nW ~ L\nMax\nNoC.C\nNoV.V\nAlign-L\n\n\n\na. NLW1: / í…îli+ami/\n í…îli.ami ~  í…î.lami\nW\n\nL\nW\n\n\n\nb. NL W1: / í≈ìn+ami/\n í≈ì.na.mi ~  í≈ì.ami\nW\n\nW\nL\n\n\n\nc.¬†NL W1: / í≈ìn+bebe/\n í≈ìn.bebe ~  í≈ì.bebe\nW\nL\n\n\n\n\n‚àö\nd.¬†L W1: /lez+bebe/\nle.bebe ~ lez.bebe\nL\nW\n\n\n\n\n‚àö\ne. L W1: /≈ìÃÉn+bebe/\n≈ìÃÉ.bebe ~ ≈ìÃÉn.bebe\nL\nW\n\n\n\n\n\nSimilar to the previous scenario, following the instruction in (17) the learners can take advantage of their bias favouring the errors made by the current grammar marked by ‚àö when resolving the inconsistency. Therefore, they would clone the relevant constraint that prefers Ws for the errors made by the current grammar (NoC.C-L) indexed to those morphemes /lez/ and /≈ìÃÉn/. As shown in (42) NoC.C-L is the only markedness constraint that prefers only Ws, therefore, it is ranked at the top. After that, the errors (d-e) are resolved and the reranking can proceed as usual resulting in the final grammar below in G2.\n(42)\nNon-liaison G2: NoC.C-L2**, NoC.C-L1* &gt;&gt; Max &gt;&gt; NoC.C, Align-L, NoV.V\n*L1: {/lez/}, **L2: {/≈ìÃÉn/}\nEventually the learner then merges the two NoC.C-L constraints under one constraint in the final grammar as shown in (43).\n(43)\nNon-liaison Final G: NoC.C-L* &gt;&gt; Max &gt;&gt; NoC.C, NoV.V, Align-L *L: {/lez/, /≈ìÃÉn/}\n\n\n2.5.3 Mixed Learning trajectory\nFor this group of learners, the learning data of the 2 morpheme types is not presented in an isolated manner, rather they see both types of morphemes at the same time: one liaison morpheme and one non-liaison morpheme. In this manner, the first type of evidence presented to learners based on what they encounter in the first block, would in fact present an inconsistency to learners.\nIn the first block, learners encounter evidence of the following set of liaison and non-liaison phrases: {le+bebe, lez+ami, ‚Ä¶ &  í≈ìn+bebe,  í≈ìn+ami, ‚Ä¶}. The learner‚Äôs task is slightly different in nature here. An ideal learner needs to first recognize the crucial difference between these two morphemes, namely that one of them lacks a stable output form and exhibits an alternation. The representations of non-liaison morphemes can be learned through lexicon optimization. However, once the learner identifies that one of the morphemes is special in that it has two output forms with some established semantic connection, they need to employ the proposed heuristic to maximize the structure in the input. Thus, they determine the larger allomorph /lez/ to be the underlying representation. Phrases such as [lebebe] and [lezami] do not cause errors at first. However, by the end of the first block, assuming an initial state bias from G0, the learner does make errors with examples such as [ í≈ìnbebe] or [ í≈ìnami]16 show in tableaux in (44).\n(44)\n\n\n\n\n\n\n\n\n\n\n\na.\n/ í≈ìn/+/bebe/\nNoC.C\nNoV.V\nAlign-L\nMax\n\n\n\n\n‚òπ\n[ í≈ìn.bebe]\n*!\n\n\n\n\n\n‚òû\n[ í≈ì.bebe]\n\n\n\n*\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nb.\n/ í≈ìn/+/ami/\nNoC.C\nNoV.V\nAlign-L\nMax\n\n\n\n\n‚òπ\n[ í≈ìn.ami]\n\n\n*!\n\n\n\n‚òû\n[ í≈ì.ami]\n\n*!\n\n*\n\n\n\nAt this stage, the input forms such as /lez/ and /bebe/ and /ami/ directly enter the lexicon of the learner as there are no errors made on them. However, the phrase leading to the error in (44) has to be moved to the error archive as shown in table (45) below. Now, based on the recent errors present in their error archive marked by ‚àö they are required to rerank and update their highest-ranked constraints from the initial state in G0.\n(45) Error archive with errors of the CG (NoC.C, NoV.V, Align-L &gt;&gt; Max)\n\n\n\n\n\n\n\n\n\n\n\n\nErrors by CG\nInput\nW ~ L\nNo C.C\nNo V.V\nAlign-L\nMax\n\n\n\n\n‚àö\na. NLW1: / í≈ìn+bebe/\n í≈ìn.bebe ~  í≈ì.bebe\nL\n\n\nW\n\n\n‚àö\nb. NLW1: / í≈ìn+ami/\n í≈ì.nami ~  í≈ì.ami\n\nW\nL\nW\n\n\n\nDue to the fact that marked structures including C.C are allowed, the relevant markedness constraint is preferred by Ls. The biased constraint demotion would initiate the reranking of initial state markedness constraints outranking other constraints at the top. Consequently, among the possible markedness constraints, the learner must install No V.V as the only markedness constraint that prefers only Ws once the relevant row of error in (45b) is resolved shown in (46).\n(46) Error archive with errors of the CG (NoC.C, NoV.V, Align-L &gt;&gt; Max)\n\n\n\n\n\n\n\n\n\n\n\n\nErrors by CG\nInput\nW ~ L\nNo C.C\nNo V.V\nAlign-L\nMax\n\n\n\n\n‚àö\na. NLW1: / í≈ìn+bebe/\n í≈ìn.bebe ~  í≈ì.bebe\nL\n\n\nW\n\n\n‚àö\nb. NLW1: / í≈ìn+ami/\n í≈ì.nami ~  í≈ì.ami\n\nW\nL\nW\n\n\n\nAs the rest of the reranking algorithm continues based on (46a), the next markedness constraint that prefers no losers is Align-L, therefore it is placed in the second stratum which outranks Max. The final step installs NoC.C at the bottom as it prefers Ls as shown in G1 (47).\n(47)\nMix G1: No V.V &gt;&gt; Align-L &gt;&gt; Max &gt;&gt; No C.C\nOnce the learners form the hierarchy in G1, they would immediately make errors with the phrases including the other word1 in this block, which is a liaison word1, shown in a tableau in (48) such as [lebebe].\n(48)\n\n\n\n\n\n\n\n\n\n\n\n\n/lez/+/bebe/\nNOV.V\nAlign-L\nMax\nNOC.C\n\n\n\n\n‚òû\n[lez.bebe]\n\n\n\n*\n\n\n‚òπ\n[le.bebe]\n\n\n*!\n\n\n\n\nThis recent error needs to be recorded in the error archive shown in (49c). As the learners compile all evidence from errors they learned from in this block, they realize the necessity of addressing the ranking paradox between the morphemes belonging to the two word1s they encountered in this block. The table below in (49), illustrates the minimal inconsistent pair that highlights the inconsistency (49b vs 49c) in bold. The learner is unable to continue the reranking process and to install any constraints.\n(49) Error archive with errors of the CG (No V.V &gt;&gt; Align-L &gt;&gt; Max &gt;&gt; No C.C)\n\n\n\n\n\n\n\n\n\n\n\n\nErrors by CG\nInput\nW ~ L\nNo V.V\nAlign-L\nMax\nNo C.C\n\n\n\n\n\na. NL W1: / í≈ìn+ami/\n í≈ì.nami ~  í≈ì.ami\nW\nL\nW\n\n\n\n\nb. NL W1: / í≈ìn+bebe/\n í≈ìn.bebe ~  í≈ì.bebe\n\n\nW\nL\n\n\n‚àö\nc.¬†L W1: /lez+bebe/\nle.bebe ~ lez.bebe\n\n\nL\nW\n\n\n\nIn the event of detecting an inconsistency among constraints‚Äô Ws and Ls, following the instruction in (17) the learners can take advantage of their bias favouring the errors made by the current grammar to resolving the inconsistency. Therefore, they would clone the relevant constraint that prefers Ws for the errors made by the current grammar indexed to that morpheme /lez/ (No C.C-L). The new added constraint, NoC.C-L, is the only markedness constraint that prefers only Ws. Therefore, according to the biased constraint demotion algorithm it is ranked at the top stratum. After the error (49c) is resolved, the reranking can proceed as usual (similar to the steps 45‚Äí47) resulting in the grammar G1.2 below in (50).\n(50)\nMix G1.2: No C.C-L* &gt;&gt; No V.V &gt;&gt; Align-L &gt;&gt; Max &gt;&gt; No C.C *L: {/lez/}\nIn the second block of the experiment, participants encounter the other liaison and non-liaison morphemes: {≈ìÃÉ+bebe, ≈ìÃÉn+ami, ‚Ä¶ &  í…îli +ami,  í…îli +bebe, ‚Ä¶}. This block shares some similarities with the first block but also presents some new challenges. Similar to before, they must distinguish the inconsistency between the new liaison word1s from the new non-liaison word1s. For the liaison word1s, the ideal learner must connect the two allomorphs semantically and identify the larger allomorph as the underlying representation. The novelty in this block of the experiment goes beyond simply recognizing the inconsistencies within the grammar. It also involves the learners‚Äô ability to classify the new liaison word alongside /lez/ and the new non-liaison words with / í…îli/. However, the learners begin this block by making errors with / í…îli/17, as the NoV.V constraint is highly ranked in G1.2 shown in the tableau in (51).\n(51)\n\n\n\n\n\n\n\n\n\n\n\n\n\n/ í…îli/+/ami/\nNoC.C-L\nNoV.V\nAlign-L\nMax\nNoC.C\n\n\n\n\n‚òû\n[ í…îli.ami]\n\n*!\n\n\n\n\n\n‚òπ\n[ í…î.lami]\n\n\n*\n*\n\n\n\n\nThis recent error (51) needs to be present in the error archive, as shown in (52d). Once this new error is recorded, the learner is required to rerank and update their constraint hierarchy from the previous grammar.\n(52) Error archive with errors of the CG (No C.C-L* &gt;&gt; No V.V &gt;&gt; Align-L &gt;&gt; Max &gt;&gt; No C.C)\n\n\n\n\n\n\n\n\n\n\n\n\n\nErrors by CG\nInput\nW ~ L\nNo C.C-L\nNo C.C\nNo V.V\nAlign-L\nMax\n\n\n\n\n\na. NL W1: / í≈ìn+ami/\n í≈ì.nami ~  í≈ì.ami\n\n\nW\nL\nW\n\n\n\nb. NL W1: / í≈ìn+bebe/\n í≈ìn.bebe ~  í≈ì.bebe\n\nL\n\n\nW\n\n\n\nc. L W1: /lez+bebe/\nle.bebe ~ lez.bebe\nW\nW\n\n\nL\n\n\n‚àö\nd. NLW1: / í…îli+ami/\n í…îli.ami ~  í…î.lami\n\n\nL\nW\nW\n\n\n\nAccording to the biased constraint demotion algorithm, No C.C-L is still ranked at the top stratum since it is the only constraint that prefers only Ws. No other markedness constraints prefer only Ws, therefore Max outranks them as the grammar G2 below shows in (53).\n(53)\nMix G2: No C.C* &gt;&gt; Max &gt;&gt; No C.C, Align-L, No V.V *L: {/lez/}\nImportantly, for this particular learner, the highest ranked NoC.C-L constraint is only associated with /lez/. Consequently, leading the learner to initially make errors when confronted with the new liaison word /≈ìÃÉn/ as shown in a tableau in (54).\n(54)\n\n\n\n\n\n\n\n\n\n\n\n\n\n/≈ìÃÉn/+/bebe/\nNoC.C-L\nMax\nNoC.C\nNoV.V\nAlign-L\n\n\n\n\n‚òû\n[≈ìÃÉn.bebe]\n\n\n*\n\n\n\n\n‚òπ\n[≈ìÃÉ.bebe]\n\n*!\n\n\n\n\n\n\nThis recent error (54) needs to be recorded in the error archive shown in (55e). Similar to the other errors previously, once this new error is added to the error archive, they are required to update their constraint hierarchy.\n(55) Error archive with errors of the CG (No C.C* &gt;&gt; Max &gt;&gt; No C.C, Align-L, No V.V)\n\n\n\n\n\n\n\n\n\n\n\n\n\nErrors by CG\nInput\nW ~ L\nNo C.C-L\nMax\nNo C.C\nNo V.V\nAlign-L\n\n\n\n\nNLW1: / í≈ìn+ami/\n\n í≈ìna.mi ~  í≈ì.ami\n\nW\n\nW\nL\n\n\n\nb. NLW1: / í≈ìn+bebe/\n í≈ìn.bebe ~  í≈ì.bebe\n\nW\nL\n\n\n\n\n\nc. LW1: /lez+bebe/\nle.bebe ~ lez.bebe\nW\nL\nW\n\n\n\n\n\nd. NLW1: / í…îli+ami/\n í…îli.ami ~  í…î.lami\n\nW\n\nL\nW\n\n\n‚àö\ne. LW1: /≈ìÃÉn+bebe/\n≈ìÃÉ.bebe ~ ≈ìÃÉn.bebe\n\nL\nW\n\n\n\n\n\nThe learner is unable to continue the¬†reranking procedure as the errors in (54b vs 54e) are in conflict and there are no constraints that prefer only Ws. At this juncture, following the instruction in (17) their errors made by the current grammar can bias the learners when resolving the inconsistency. Therefore, they would clone the relevant constraint that prefers Ws for the errors made by the current grammar indexed to that morpheme /≈ìÃÉn/ (No C.C-L). This results in two cloned constraints as it is shown in the hierarchy below in (56).\n(56)\nMix G2.2: NoC.C-L2**, NoC.C-L1* &gt;&gt; Max &gt;&gt; NoC.C, Align-L, NoV.V\n *L1: {/lez/}, **L2: {/≈ìÃÉn/}\nHowever, it is expected that in such a situation these cloned constraints would merge; resulting in a single constraint being indexed to both morphemes instead as shown below in the final grammar in (57):\n(57)\nMix final G: NoC.C-L* &gt;&gt; Max &gt;&gt; NoC.C, Align-L, NoV.V *L: {/lez/, /≈ìÃÉn/}\n\n\n2.5.4 Predicted outcomes of trajectories\nIn this subsection, the predicted outcomes of the hypothesized trajectories are outlined. First, a summary of the trajectory of ideal learners who learned the artificial language through three different ordering of learning data is shown in the following Table 2-1. The three groups have the same initial state (G0) and end state (GE), but they differ in the order of learning data and thus also in their trajectories shaped in the two blocks. The constraint hierarchies built in the first block are shown as G1 and the constraint hierarchies build in the second block are shown as G2. If a block has more than one constraint hierarchy they are separated by a dashed line for clarity.\nThere are several aspects in which these ideal learners can be evaluated on, including: a. underlying representations, b. reranking of the constraint hierarchy, c.¬†lexicon indexation, and d.¬†constraint cloning. In the following subsection on predictions, I will review the theoretical pressures pertaining to an ideal learner for each aspect individually. It is important to note that the cognitive and memory constraints associated with each of these trajectories could impact learners‚Äô performance in various ways in experimental settings, a topic I will address under the pressures faced by a suboptimal learner in the last subsection.\nIn the experiment, after being presented with the learning data, participants engage in a series of forced choice tasks. The tasks are comparable to selecting between pairs of winning and losing candidates. By quantifying their accuracy with forced-choice tasks, making inferences about their learning is also possible through analyzing their errors. The main research question targets the difficulty of learning morpheme-specific alternations along certain theoretical dimensions which are included in a theory of learning within OT. Given that the only manipulation for these groups of participants is the distribution and order of learning input, I hypothesize that there will be differences in the difficulty of learning which would lead to differential learning outcomes in two respects simultaneously: quantitatively, and qualitatively, that is, in terms of overall differences across the groups, and the distribution of errors within each group, respectively.\nTable Error! No text of specified style in document.‚Äë1. The summary of learning trajectories of the three groups of learners\n\n\n\n\n\n\n\n\n\n\nLiaison First\nNon-liaison First\nMixed\n\n\nInitial state\n(G0 )\nNo C.C, No V.V, Align-L\n&gt;&gt; Max\nNo C.C, No V.V, Align-L\n&gt;&gt; Max\nNo C.C, No V.V, Align-L\n&gt;&gt; Max\n\n\nBlock1\n(G1 )\nNo C.C, No V.V\n&gt;&gt; Align-L\n&gt;&gt; Max\nMax\n&gt;&gt; No C.C, No V.V, Align-L\nNo V.V\n&gt;&gt; Align-L\n&gt;&gt; Max\n&gt;&gt; No C.C\n-\n‚Äî‚Äî‚Äî\n‚Äî‚Äî‚Äî-\nNo C.C-L*\n&gt;&gt; No V.V\n&gt;&gt; Align-L\n&gt;&gt; Max\n&gt;&gt; No C.C\n*L: {/lez/}\n\n\nBlock2\n(G2 )\nMax\n&gt;&gt; No C.C, No V.V, Align-L\n\nNo C.C-L**, No C.C-L*\n&gt;&gt; Max\n&gt;&gt; No C.C, No\nV.V, Align-L\n**L: {/≈ìÃÉn/} *L: {/lez/}\nNo C.C-L**, No C.C-L*\n&gt;&gt; Max\n&gt;&gt; No C.C, No\nV.V, Align-L\n**L: {/≈ìÃÉn/} *L: {/lez/}\nNo C.C-L*\n&gt;&gt; Max\n&gt;&gt; No C.C, No\nV.V, Align-L\n*L: {/lez/}\n\nNo C.C-L**, No C.C-L*\n&gt;&gt; Max\n&gt;&gt; No C.C, No\nV.V, Align-L\n**L: {/≈ìÃÉn/} *L: {/lez/}\n\n\nEnd state\n(GE )\nNo C.C-L*\n&gt;&gt; Max\n&gt;&gt; No C.C, No\nV.V, Align-L\n*L: {/lez/, /≈ìÃÉn/}\nNo C.C-L*\n&gt;&gt; Max\n&gt;&gt; No C.C, No\nV.V, Align-L\n*L: {/lez/, /≈ìÃÉn/}\nNo C.C-L*\n&gt;&gt; Max\n&gt;&gt; No C.C, No V.V,\nAlign-L *L: {/lez/, /≈ìÃÉn/}\n\n\n\n\nPredictions for an optimal learner\n\nUnderlying representations in alternations\nThe ideal learning of morpho-phonological patterns begins with the creation of underlying representations for new morphemes. In the experiment, all learners must understand that two of the four morphemes, described as liaison morphemes, share a single input corresponding to two output variants. Then they can posit the correct underlying representation using the heuristic. The rationale is that if the type of data and the order in which learning data is presented influences the¬†learning of the underlying representations of alternations, there would be differences among the groups‚Äô accuracy reflective of that. Specifically, learners who follow the liaison-first trajectory face less challenges to compute the underlying representations of the allomorphs. This is because the initial evidence of both liaison morphemes they receive in the first block will facilitate their understanding of the 2-1 mapping from output to input, rather than relying on secondary or inconsistent evidence. Conversely, learners in the non-liaison-first trajectory would find it more challenging, because they lost the clear opportunity in the very beginning to learn about the phonologically motivated alternations in the mini-language. This might hinder their ability to notice the alternations and thereby establish a unified input through the heuristic. The unordered group, on the other hand, has access to only one liaison morpheme learning data compared to the liaison-first group in the first block, placing them in an intermediate difficulty position.\nTherefore, this rationale for learning the underlying representation predicts the following order of learning difficulty among the groups, from easiest to hardest:\nliaison-first &gt; unordered &gt; non-liaison-first\n\n\nReranking of the constraint hierarchy\nDuring the morpho-phonological learning process driven by errors, once a learner has explored the possible underlying representations and selected the appropriate one, the next step involves reranking the constraints until reaching a final state grammar without errors. The rationale is that if the order in which learning data is presented influences the trajectory and steps of constraint reranking, the differences in performance across the groups would reflect that. In principle, the hypothesis is that having fewer stages of reranking is easier. That means, learners following a non-liaison-first trajectory are expected to find it easier to converge on the end state grammar. This is because they only need to update and rerank their constraint hierarchy twice to arrive at the correct end state grammar, whereas other groups require more reranking stages.\nTherefore, this rationale about reaching the end state grammar predicts the following order of learning among the groups from easiest to hardest:\nnon-liaison-first &gt; liaison-first &gt; unordered\n\n\nLexicon indexation and constraint cloning\nThe next crucial aspect of learning morpheme-specific alternations involves identifying inconsistencies in the grammar and acknowledging that certain morphemes within a subset of the lexicon may conflict with their current ranking. I predict that in scenarios requiring inconsistency detection, if the process of learning lexicon indexation varies depending on the order and type of learning data presented, the differences in performance among groups would be reflective of that. Specifically, the hypothesis is that it is easier for learners to recognize that a group of morphemes must be indexed to specific constraints when they are exposed to multiple morphemes of the same class in each block. Consequently, the two ordered groups would find it easier to choose indexations compared to the unordered group. The unordered group, in contrast, must learn that some morphemes require indexing twice, in the first and second blocks of the experiment.\nFurthermore, learners need to determine which constraints to clone and index to a subset of the lexicon. Echoing the previous argument on lexicon indexation, I predict that recognizing when a constraint needs to be cloned and indexed is more straightforward when learners encounter the set of morphemes in a class together. Moreover, this expectation also arises from the fact that the ordered groups can integrate their cloned versions of the constraints for each instance of the liaison morphemes as they generate them simultaneously, while the unordered group must grapple with this decision once more later as they have cloned their constraints in two separate blocks.\nTherefore, this rationale for constraint cloning and lexicon indexation predicts the following order of learning among the groups from easiest to hardest:\nnon-liaison-first, liaison-first &gt; unordered\n\n\nDistribution of errors within each group\nI also predict that the learning and performance of the three groups would differ qualitatively with each other, namely with respect to the type of errors that they frequently make. If the difficulties of learning are not of the same nature for the three groups, their error patterns will show differences in learning. Specifically, each group will make less errors with the type of phrases presented to them in their first block compared to the second block.\nTherefore, if the order of receiving the input affects their learning, it is easier to learn liaison phrases for the liaison-first group compared to the other groups and it is easier to learn non-liaison phrases for the non-liaison-first group compared to the other groups.\n\n\n\nPredictions for a suboptimal learner\nThere are various ways in which in an experimental setting the learner might face extra limitations due to memory or cognitive load of the tasks. The first issue could be influenced by restraints on memory and its connection to the storage of the¬†lexicon and the error archive. For instance, during block two of the experiment, the liaison-first group has to recall from memory that certain inputs that were learned in the first block without any errors, would lead to errors with their updated grammar at that stage. In this scenario, a suboptimal learner in an experimental setting might find it more difficult to hold the words and phrases in the language in memory for a longer time to consider their errors and non-errors as they progress in the experiment towards the end.\nThis rationale for possible memory constraints leads to the predictions that the difficulty among groups ranked from easiest to hardest are as follows:\nnon-liaison-first &gt; liaison-first, unordered\nAnother cognitively challenging aspect of learning concerns the cost of learning generalizations and lexical optimization sequentially. If a learner has to learn morpheme-specific alternations in an inconsistent grammar, they must learn two things: a) simple phonologically-motivated generalization supported by a¬†universal bias towards unmarked structures and b) more complicated lexical facts. Hsu and Chater (2010) propose a simplicity-based cognitive model for the logical problem of language acquisition where there is a trade-off in the costs of learning a simple generalization with less data and learning lexical and more complicated rules with more data. Inspired by the ideas in the simplicity framework, I hypothesize that cost of learning from an inconsistent set of learning data in the direction which the learner retreats from a generalization to lexicon optimization through receiving more evidence would be lower than learning in the opposite direction, from lexicon optimization to a generalization about alternations.\nConsequently, this rationale about the cognitive costs of the learning trajectory predicts the following order of difficulty among the groups from easiest to hardest:\nliaison-first &gt; non-liaison-first, unordered.\n\nTable 2‚Äë2. The summary of predictions of difficulty of learning by different aspects\n\n\n\n\n\n\n\n\n\neasiest\n\nhardest\n\n\n\n\nUR of alternations\nliaison-1st &gt;\nunordered &gt;\nnon-liaison-1st\n\n\nReranking of the constraint hierarchy\nnon-liaison-1st &gt;\nliaison-1st &gt;\nunordered\n\n\nLexicon indexation + constraint cloning\nliaison-1st, non-liaison-1st &gt;\n\nunordered\n\n\nLearning liaison patterns\nliaison-1st &gt;\nnon-liaison-1st, unordered\n\n\n\nLearning non-liaison patterns\nnon-liaison-1st &gt;\nliaison-1st, unordered\n\n\n\nmemory constraints\nnon-liaison-1st &gt;\nliaison-1st, unordered\n\n\n\ncognitive costs\nliaison-1st &gt;\nnon-liaison-1st, unordered\n\n\n\n\nTo summarize the predictions in this subsection, Table 2-2 shows the different groups ranked in the order of difficulty for each prediction. The predictions about the difficulty of learning with respects to various aspects could be evaluated by examining the performance differences among the three groups during testing.\nIn the next chapter, the experimental design, material, and training and testing procedures are described in more detail.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Background and Theoretical Framework</span>"
    ]
  },
  {
    "objectID": "chapters/2_backgroundandtheory.html#footnotes",
    "href": "chapters/2_backgroundandtheory.html#footnotes",
    "title": "2¬† Background and Theoretical Framework",
    "section": "",
    "text": "The facts about the acquisition of the past tense allomorphs in English are more complicated than presented here schematically with two of the forms (see Tomas et al., 2017; Oetting and Horohov, 1997).‚Ü©Ô∏é\nSee Kager (2004: 330-336) for an example of a learner with a morphological handicap which shows that the algorithm would still converge when learning the voicing alternation in plural morphemes in Dutch.‚Ü©Ô∏é\nNo V.V: Mark a violation if two hetero-syllabic vowels are adjacent (V.V).‚Ü©Ô∏é\nNo C.C: Mark a violation if two hetero-syllabic consonants are adjacent (C.C).‚Ü©Ô∏é\nAlign-L: Mark a violation if the left edge of a morphological word does not coincide with the left edge of a prosodic word (Kager, 2004: 111).‚Ü©Ô∏é\nMax: Mark a violation if every segment in the input does not have a corresponding segment in the output.‚Ü©Ô∏é\nDep: Mark a violation if every segment in the output does not have a corresponding segment in the input.‚Ü©Ô∏é\nThis constraint would be accessible in phrases where morphological knowledge is acquired. Learners have to know the prosodic word with a syllabification of /n/ for an output such as [ í≈ì.#na.mi] is not aligned with the morphological boundaries in the output [ í≈ì.n#a.mi].‚Ü©Ô∏é\nIt is worth mentioning that the literature is non-conclusive regarding this general criterion, so testing this heuristic in this specific domain needs further experimental investigation. The past literature in OT on what possible constraints can exist in Con has put forward a constraint family known as *Struc (Prince and Smolensky, 1993) which has a ‚Äúless is best‚Äù spirit. In this situation it would mean selecting the other possible input in CV form to be the underlying representation as it is structurally less complex. However, this view is not the established view in the literature as Gouskova (2003) has shown in her work.‚Ü©Ô∏é\nChoosing the smaller allomorph as the input, entails that the learner has to choose a grammar that ranks No V.V &gt;&gt; Dep. In this grammar, generating the other allomorph necessitates inserting the final consonant (LC) which makes the phonological learner reach a dead-end very soon, since it eventually becomes evident that each morpheme requires insertion of an arbitrary consonant. For instance, for the masculine singular indefinite article un a hypothetical learner could choose the nasal segment /n/ to be inserted because the vowel is nasalized. However, by inserting the same nasal segment in the plural definite morpheme les the learner would be making an error.‚Ü©Ô∏é\nThe idea is that all errors are permanently stored in the error archive; however, for space reasons, errors in (11a) and (11d) are not displayed since Dep is no longer included in the constraint set.‚Ü©Ô∏é\nI am assuming that the indexed constraints follow a locality principle and only apply locally to the morpheme indexed, here /lez/, and not to the full word1+word2 input.‚Ü©Ô∏é\nThe underlying assumption is that when the three markedness constraints are ranked equally at the top in the initial state, it essentially corresponds to maintaining six different permutations, such as A &gt; B &gt; C, A &gt; C &gt; B, and so forth. In the permutation that features Align-L &gt; NoV.V, learners would be making such errors shown in (27b).‚Ü©Ô∏é\nRecall that other faithfulness constraints such as Dep are not shown in this hierarchy due to reasons of space but it is at the same stratum as Max, outranking NoV.V.‚Ü©Ô∏é\nSame as the previous footnote, when the three markedness constraints are ranked equally at the top in the initial state, it essentially corresponds to maintaining six different permutations, such as A &gt; B &gt; C, A &gt; C &gt; B, and so forth. In the permutation that features NOV.V&gt;&gt;ALIGN-L, ultimately learners would be making such errors as shown in (36b).‚Ü©Ô∏é\nSame as the previous footnotes, when the three markedness constraints are ranked equally at the top in the initial state, in the permutation that features ALIGN-L &gt;&gt; NOV.V, ultimately learners would be making such errors as shown in (43b).‚Ü©Ô∏é\nThis order of errors is specific to this version. There are in fact two versions of the unordered group in the experiment to counterbalance the effect of specific errors. The other version of this trajectory would be to receive / í…îli/ in the first block and / í≈ìn/ in the second block. This difference does not affect how many stages of G there would be overall, but it is worth mentioning that the other version not reviewed here leads to a different error at this point with [ í≈ìnbebe].‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Background and Theoretical Framework</span>"
    ]
  },
  {
    "objectID": "chapters/3_methods.html",
    "href": "chapters/3_methods.html",
    "title": "3¬† Methods",
    "section": "",
    "text": "3.1 Participants\nA total of 241 participants were recruited from the UBC linguistics participants pool in the Alternating 1st, Non-alternating 1st, and Mixed experimental conditions with 82, 83, and 76 participants, respectively. This convenience sample consisted of participants aged 16 to 49 (Mean=20.6, SD = 4), with 167 as female (69%), 49 participants as male (21%) and 25 (10%) as nonbinary or unreported. Regarding the language background, 211 reported English as their dominant language, and 30 individuals listed English as their non-dominant language. Among the non-English dominant languages, Mandarin and/or Cantonese (13), Korean (5), Japanese (4), and Tagalog (3) were listed. Due to not meeting the language background eligibility criterion of the study, 5 French-speaking participants were excluded, specifically as they were raised with Francophone caretakers with an age of acquisition of French below four. More information about the data collection procedures and the inclusion and exclusion criteria is provided in Section 3.5.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "chapters/3_methods.html#overview-of-design",
    "href": "chapters/3_methods.html#overview-of-design",
    "title": "3¬† Methods",
    "section": "3.2 Overview of design",
    "text": "3.2 Overview of design\nThe experimental design employed a between-subjects approach, consisting of three distinct conditions. The conditions varied only in the distribution order and the content of the training blocks, but all participants completed the training phase with the same set of items and the same number of training trials by the time they reached the testing phase. Consequently, this independent variable was categorical, consisting of three levels (i.e., the training conditions). The dependent variable was binary, representing participants‚Äô correct responses in two forced-choice tasks during testing. Below is a diagram that provides a step-by-step overview of the experimental design in Figure 3-1.\n\n\n\nFigure ‚Äé3‚Äë1. Step-by-step overview of the experimental design and procedure\n\n\nThe experiment was conducted remotely (online) via a web browser. Participants had to blindly register in only one the conditions to receive a link. After the landing page, participants were required to complete a consent form and undergo a headphone check with six trials. Correct answers to four and more trials allowed them to progress to the experiment. Introductory instructions about the artificial language were provided to participants at the outset of the experiment and more specific instructions before the training and testing tasks.\nThe training phase included 144 trials, and the testing phase included 64 trials overall. At the end of the experiment, an optional debriefing form was made available, allowing participants to offer written feedback regarding their approach to the experiment. Further details about the procedure in each phase and the types of trials will be discussed in Section 3.4 after reviewing the materials used in the experiment in Section 3.3.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "chapters/3_methods.html#materials",
    "href": "chapters/3_methods.html#materials",
    "title": "3¬† Methods",
    "section": "3.3 Materials",
    "text": "3.3 Materials\nAll the nonce-word stimuli used throughout the¬†training phase of the experiment consisted of two-word phrases (word1 + word2) presented to participants as audio stimuli and simultaneously accompanied by visual stimuli. In the introduction to the mini-language that participants received, these phrases were framed as possessive phrases in an alien language, where the first word is the possessor and the second word is the possessed item (mirroring the most common order in English). These items were described as various everyday items that the aliens brought in their luggage to Earth to disguise themselves on their tour of the galaxy. The contextual scenario was introduced to participants as an opportunity to learn about possessive phrases in that alien language.\n\n3.3.1 Training and testing Stimuli\nIn total, there were four word1 stimuli, representing the aliens‚Äô names, categorized into alternating and non-alternating types. Each alternating word1 had two allomorph forms. The phonological conditioning pattern of the alternating word1s was analogous to that of the liaison allomorphs in French. The C-final allomorph ([me É] or [dit]) would occur before V-initial word2s and the V-final allomorph ([me] or [di]) would occur before C-initial word2s. The consonants in bold letters / É/ and /t/ are referred to as the liaison consonant (LC). Table 3-1 below displays all the word1s designed to be analogous to the morpheme-specific alternations in French, as presented in Section 2.2 in Examples (1‚àí5).\n\nTable ‚Äé3‚Äë1. Word1 stimuli representing alien names\n\n\n\n\n\n\n\n\nAlternating word1s\n\nNon-alternating word1s\n\n\n\n\n\nV(C)-final\n\nC-final\nV-final\n\n\n[me], [me É]\n[di], [dit]\n[k…ën]\n[lu]\n\n\n(as in petit)\n\n(as in jeune)\n(as in joli)\n\n\n\nThe rest of the stimuli were 32 word2s which were constructed using 18 consonants (/p b t d k g t É d í f v s z  É n m r l j/) and six vowels (/i e u o √¶ …ë/) organized into four distinct word shapes: V.CVC, V.CV, CV.CVC, and CV.CV. The segments adhered to the phonemic inventory and phonotactic rules of English so the vowels /e/ and /o/ were phonetically [e…™] and [o ä], except before /r/. Each vowel appeared between 4 and 6 times, while each consonant was included between 2 and 4 times in the overall list of word2s. Efforts were made to counterbalance the position of vowels and consonants in the words as much as possible. Table 3-2 below categorizes the word2s into V-initial and C-initial types. Sixteen of the word2 stimuli appeared both in training and in testing tasks 1 and 2, while the other 16 were only used in task 3 in testing (i.e., they did not appear in training).\n\nTable ‚Äé3‚Äë2. Word2 stimuli representing everyday object nouns\n\n\n\n\n\n\n\n\n\n\nV-initial word2s\n\nC-initial word2s\n\n\n\n\n\n\nV.CVC\nV.CV\nCV.CVC\nCV.CV\n\n\nTraining & Testing task 1 and task 2\n/ed√¶p/\n/√¶te/\n/p√¶z…ëm/\n/vezu/\n\n\n\n/ibud/\n/ogi/\n/budol/\n/m√¶ti/\n\n\n\n/…ët√¶l/\n/ufe/\n/g…ëbi É/\n/kis…ë/\n\n\n\n/ug√¶ É/\n/ido/\n/d íomet/\n/peku/\n\n\nTesting task 3\n(novel)\n/it É√¶n/\n/ez…ë/\n/f√¶rus/\n/fule/\n\n\n\n/…ëjor/\n/…ëvi/\n/t É√¶gef/\n/s√¶d í…ë/\n\n\n\n/ok…ën/\n/osu/\n/runip/\n/rej…ë/\n\n\n\n/e Éin/\n/im…ë/\n/zo Éek/\n/voli/\n\n\n\nThe word1 and training word2 stimuli paired together would result in two-word phrases that constitute the items in the training trials, as well as the choice stimuli in testing task 1 and the prompt stimuli in the testing task 2 trials. The word1 and novel word2 stimuli paired together would result in two-word phrases that are used only as the prompt stimuli in the testing task 3. Further details about the distribution of the stimuli in the training and testing phase are provided in Section 3.4 along with the experimental procedure.\n\n\n3.3.2 Audio stimuli\nThe audio stimuli were recorded by a phonetically-trained female speaker of Canadian English in a controlled recording environment, using a Shure WH20 dynamic headset microphone and PreSonus Studio One Pro software with a sampling rate of 44.1 kHz and a bit depth of 32-bit. All audio files were adjusted and normalized to an amplitude of 50 dB. The phrasal stimuli comprised a list of trisyllabic word1 + word2 phrases structured in six various shapes: CV.CV.CV(C) or CVC.CV.CV(C) or CV.VCV(C). The first vowel was pronounced with an unreduced quality to make sure all the stimuli adhere to the perceived stress pattern ÀåœÉ.ÀàœÉ.œÉ. The phrasal trisyllabic stimuli were recorded without a carrier phrase, in a list, but the single isolated words, used in testing tasks 2 and 3, were placed within the carrier phrase ‚ÄúI said [target word] again/twice‚Äù. The long recordings were transcribed by the researcher and subsequently annotated using the Montreal Forced Aligner 2 package in Python (McAuliffe et al., 2017). Following a thorough review of the annotation text grids produced by the MFA, smaller audio files containing only the target words (ranging from 1 to 3 seconds) were automatically extracted using a script in Praat 6.1.42 (Boersma and Weenink, 2021).\n\n\n3.3.3 Visual stimuli\nEach word in the experiment was associated with a distinct and colourful image consistently for all participants. The visual stimuli included 2D vector illustrations of various alien characters and everyday objects, covering themes related to school, the workplace, kitchen items, and more. These illustrations were sourced freely from a vector image and graphics website found on the internet.1 The images were displayed centrally on the screen, with dimensions between a maximum of 580 pixels and a minimum of 256 pixels. Figure 3-2 below provides an example of the illustrations used for the four alien characters and a set of the images of the objects which were the visual stimuli used across all trial types.\n\n\n\nFigure ‚Äé3‚Äë2. Visual stimuli used for word1 and a sample of visual stimuli used for word2",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "chapters/3_methods.html#experimental-procedure",
    "href": "chapters/3_methods.html#experimental-procedure",
    "title": "3¬† Methods",
    "section": "3.4 Experimental procedure",
    "text": "3.4 Experimental procedure\n\n3.4.1 Training\nThe initial phase of the experiment was dedicated to training participants on the two-word phrases. During this phase, participants listened to a phrase and simultaneously viewed two corresponding images in each trial, similar to the example shown in Figure 3-3. This was a self-paced exposure task, allowing participants to press a key when they felt prepared to advance to the next trial. The training phase included 144 trials and the total duration of this phase was on average 8 (SD=7) minutes for all participants.\n\n\n\nFigure ‚Äé3‚Äë3. A sample page of all experimental trials in the web browser\n\n\nParticipants in all three conditions were exposed to the same training stimuli over the course of the training phase; the only difference between the conditions was the distribution order of the training items. All the training trials were organized into 2 blocks. The three conditions are named based on the name of the first block, that is ‚ÄúAlternating 1st‚Äù (Alt1st), ‚ÄúNon-alternating 1st‚Äù (NonAlt1st) and ‚ÄúMix‚Äù (Mixed). Table 3-3 below illustrates the order of the blocks and trial types for each condition with some examples. For example, in the Alt1st condition: block1 contained phrases constructed with alternating word1s [me É]/[me] and [dit]/[di], each combined with 12 word2s, such as: [dit] + /ed√¶p/, [di] + /budol/, [me É] + /ed√¶p/, [me] + /kis…ë/ and block2 contained phrases constructed with non-alternating word1s /lu/ and /k…ën/, each combined with 12 word2s, such as: /lu/ + /ed√¶p/, /lu/ + /budol/, /k…ën/ + /ed√¶p/, /k…ën/ + /kis…ë/. Conversely, in the NonAlt1st condition, block 1 included phrases with two non-alternating word1s, and block 2 contained phrases with two alternating word1s. The third condition, Mix, did not consist of phrases exclusively constructed with non-alternating or alternating word1s; rather, each block contained an equal distribution, with half of the phrases created using one alternating word1 and the other half using one non-alternating word1. We recruited participants in two sub-conditions with both orders to counterbalance the effect of order (Mix1: block1 ‚Äì block2, Mix2: block2 ‚Äì block1) and later collapsed them into one condition for the analysis.\nIn each block, participants were presented with 24 unique training trials per block. The four word1s were divided equally between the blocks as shown with examples in Table 3-3, two of the word1s appeared in the first block and the other two appeared in the second block. For all conditions, out of the 16 unique word2s half were paired with both word1s while the remaining 8 were limited-context word2s and appeared with only one. In the subsequent tables (and in the next subsection about testing), this distinction among the phrases containing different word2 types is referred to as limited-context versus full-context. For instance, /ed√¶p/ is paired with both word1s inside a block so it is a full-context word2 and /budol/ or /kis…ë/ are paired with just one word1 inside a block so they are limited-context word2s. Therefore, in each block, the 24 training trials consisted of 2 word1s √ó 8 full-context word2s + 1 word1 √ó 4 limited-context word2s + 1 word1 √ó 4 limited-context word2s. This 24-trial list was repeated three times, each time with a different random order, resulting in a total of 72 training trials per block (24 trials √ó 3 repetitions). As there were two blocks, the training phase consisted of 144 trials overall. A pseudo-randomization order was employed to ensure that the word1 + full-context word2 phrases (that shared the same word2 but differed in their word1) were¬†immediately adjacent to¬†each other in the list of 24 trials. Consequently, the special arrangement of the 16 word2s in the entire training phase allowed participants to encounter half of the word2s in full contexts paired with 4 unique word1s while also being exposed to half of the word2s in a limited context, once with an alternating word1 and once with a non-alternating word1. For a comprehensive list of stimuli in each block, please refer to the Appendix A.\n\nTable ‚Äé3‚Äë3. Structure and the content of the blocks in the training phase of the experiment\n\n\n\n\n\n\n\n\n\n\n\nAlternating 1st\nCondition\nNon -alternating 1st Condition\nMixed\nCondition\n\n\n\n\nTraining block 1\nword2 type\n‚Äúalternating word1‚Äù\n‚Äúnon -alternating word1‚Äù\n‚Äúnon -alternating/ alternating word1‚Äù\n\n\nn total\n72= 24√ó3\nfull-context\nlimited-context\nfull-context\nlimited-context\n[dit] + /ed√¶p/\n[di] + /budol/\n‚Ä¶\n[me É] + /ed√¶p/\n[me] + /kis…ë/\n‚Ä¶\n/lu/ + /ed√¶p/\n/lu/ + /p√¶z…ëm/\n‚Ä¶\n/k…ën/ + /ed√¶p/\n/k…ën/ + /vezu/\n‚Ä¶\n/lu/ + /ed√¶p/\n/lu/ + /p√¶z…ëm/\n‚Ä¶\n[me É] + /ed√¶p/\n[me] + /kis…ë/\n‚Ä¶\n\n\nTraining block 2\n\n‚Äúnon -alternating word1‚Äù\n‚Äúalternating word1‚Äù\n‚Äúnon-alternating / alternating word1‚Äù\n\n\nn total\n72= 24√ó3\nfull-context\nlimited-context\nfull-context\nlimited-context\n/lu/ + /ed√¶p/\n/lu/ + /p√¶z…ëm/\n‚Ä¶\n/k…ën/ + /ed√¶p/\n/k…ën/ + /vezu/\n‚Ä¶\n[dit] + /ed√¶p/\n[di] + /budol/\n‚Ä¶\n[me É] + /ed√¶p/\n[me] + /kis…ë/\n‚Ä¶\n/k…ën/ + /ed√¶p/\n/k…ën/ + /vezu/\n‚Ä¶\n[dit] + /ed√¶p/\n[di] + /budol/\n‚Ä¶\n\n\n\nTable 3-4 below illustrates the structure inside each block for the specific pseudo-randomization order of training trials which applies to all conditions. For instance, when the full-context word2 /ed√¶p/ appears in the presentation list, we ensured that it also appears in the next phrase that follows. In an exclusively alternating word1 block, the sequence of phrases would be [dit] + /ed√¶p/ and [me É] + /ed√¶p/, while in an exclusively non-alternating block the phrases in sequence would be /lu/ + /ed√¶p/ and /k…ën/ + /ed√¶p/. Limited-context word2s only appear with one word1 type in the training trials; therefore, the other potential phrase is held-out from appearing in training. For instance, when /kis…ë/ appears in a phrase with an alternating word1 as in [me] + /kis…ë/, that would render [di] + /kis…ë/ a held-out phrase. The held-out phrases are incorporated in some of the testing trials.\n\nTable ‚Äé3‚Äë4. Pseudo-randomization structure inside each training block\n\n\n\n\n\n\n\n\nTraining blocks\nword2 type\n‚Äúalternating word1‚Äù\n‚Äù non-alternating word1‚Äù\n\n\n\n\nn total\n72=24√ó3\nfull-context\nfull-context\nlimited-context\nfull-context\nfull-context\nfull-context\nfull-context\nlimited-context\n[dit] + /ed√¶p/\n[me É] + /ed√¶p/\n[me] + /kis…ë/\n[dit] + /ufe/\n[me É] + /ufe/\n[di] + /peku/\n[me] + /peku/\n[di] + /budol/\n/lu/ + /ed√¶p/\n/k…ën/ + /ed√¶p/\n/k…ën/ + /kis…ë/\n/lu/ + /ufe/\n/k…ën/ + /ufe/\n/lu/ + /peku/\n/k…ën/ + /peku/\n/lu/ + /budol/\n\n\n\nUpon the completion of the training trials, participants were informed that the testing phase could be initiated by clicking on the Next button on the page. In the following section, we will describe the specifics of the testing trials.\n\n\n3.4.2 Testing\nIn this section, participants engaged in two different types of 2-alternative forced-choice (2AFC) tasks, using key presses on a keyboard. The number of times the first choice or second choice was the correct answer was completely counterbalanced throughout each task. Throughout all trials, prompts were presented to participants simultaneously through both auditory and visual means, just as in the training phase. Before each task, participants had the opportunity to practice with two practice stimuli. Participants were advised to select the choice that initially appears correct to them with two warnings: first, they would not have the option to alter their answers, and second, after a prolonged pause (6 seconds), the trials would time-out automatically proceeding to the next item. The testing phase included 64 trials overall and the total duration of this phase was on average 15 (SD=5) minutes for all participants.\nThe testing phase was organized into three blocks of testing trials. The first testing task was a phrase judgement task, and the second testing task was a segmentation task. In these first two tasks, both full-context and limited-context word2s were incorporated. Limited-context word2s here in the testing phrases refer to the word2s in the held-out word1 + word2 pairings not encountered by participants in training. The third testing task was a segmentation task incorporating only novel word2s. The following subsections provide further details regarding the stimuli and procedures for each task. For a comprehensive list of items in each task, please refer to the Appendix A. Table 3-5 below outlines the chronological order of the testing tasks along with the template for the test items in each trial.\nTable ‚Äé3‚Äë5. The structure of the tasks in the testing phase of the experiment\n\n\n\n\n\n\n\n\nTask Name\nStimuli Type and Number\nTesting Task Template\n\n\n\n\nPractice\nn=2\n\n\n\nTesting task 1\n‚ÄúPhrase Judgement‚Äù\nfull phrase choices\nn=24+8\n(familiar + held-out)\nüîä [Word1+Word2] vs üîä [Word1+Word2]\nüì∑\n\n\nPractice\nn=2\n\n\n\nTesting task 2\n‚ÄúFamiliar Segmentation‚Äù\nword2 choices\nn=12+4\n(familiar + held-out)\nüîä [Word1+Word2] +\nüîä [Word2] vs üîä [Word2]\nüì∑\n\n\nTesting task 3\n‚ÄúNovel Segmentation‚Äù\nword2 choices\nn=16\n(novel)\nüîä [Word1+Word2] +\nüîä [Word2] vs üîä [Word2]\nüì∑\n\n\n\n\nTesting Task 1: Phrase Judgement\nThe first forced choice task involved participants viewing an image while listening to two phrases featuring the combination of the same word1 and word2. The intended correct phrase either has appeared in the training or exactly matched the phonological patterns in the training items and the incorrect phrase was minimally different. Participants were instructed to choose the phrase that seems correct to them based on the alien language they have been exposed to in their training by pressing ‚ÄúQ‚Äù for the first and ‚ÄúP‚Äù for the second audio stimulus. All word1s and word2s presented were known by participants, as they had appeared during the training phase separately at least once. Although 8 out of 32 trials contained held-out phrases as they included limited-context word2s‚Äîonly paired with one non-alternating and one alternating word1 in training. The stimuli were automatically concatenated using a Praat script, with a 75ms inter-stimulus interval (ISI) between the choices. The trials were labelled and categorized into 3 types based on their word1 type as well as the error type: ‚Äúbasic‚Äù, ‚Äúalternating‚Äù, and ‚Äúnon-alternating‚Äù. Table 3-6 provides examples of choices coded as correct and incorrect for each trial type. The choices considered incorrect contained specific errors with minimal differences so, unlike the correct choices, they did not match the patterns presented in the training trials.\nThe 16 basic trials (B-A and B-NA) involved testing items constructed with both alternating and non-alternating word1s. The errors in the incorrect choices for the non-alternating word1s were specifically designed by either omitting the final consonant (/n/ of /k…ën/) before a vowel-initial word2 or by inserting an LC (after /lu/) before a consonant-initial word2. In contrast, the incorrect choices for the alternating word1s were created by swapping the LCs, that is changing / É/ to /t/ and vice versa, and then by inserting the wrong LC either before a consonant-initial word2 or a vowel-initial word2 where an LC was already present.\nThe 8 alternating trials (Alt) consisted of testing items made with the alternating word1s [me É/me] and [dit/di]. In this case, the errors in the incorrect choices were created by either omitting the LC before a vowel-initial word2 or inserting the LC before a consonant-initial word2.\nThe 8 non-alternating trials (NonAlt) comprised testing items constructed with non-alternating word1s like /lu/ and /k…ën/ The errors in these incorrect choices arose from either removing the final consonant (/n/) before a consonant-initial word2 or inserting an LC between the vowel-final word1 and a vowel-initial word2.\n\nTable ‚Äé3‚Äë6. Testing trial types in task 1 (phrase judgement) with examples\n\n\n\n\n\n\n\n\nTesting Trial Type\nCorrect\nIncorrect\nType of Error\n\n\n\n\nitemBasic (B-A) n=8\ndikis…ë\nme É√¶te\ndi Ékis…ë\nme Ét√¶te\ninserting another LC with C-initial word2s\ninserting an extra LC with V-initial word2s\n\n\nitemBasic (B-NA) n=8\nk…ën…ët√¶l\nlubudol\nk…ë_…ët√¶l\nlutbudol\ndeleting the final C of a non-alternating word1\ninserting an LC at the end of a non-alternating word1\n\n\nitemAltlternating (Alt) n=8\nditug√¶ É\nmebudol\ndi_ug√¶ É\nme Ébudol\ndeleting the LC\ninserting the LC\n\n\nitemNon-alternating (NonAlt) n=8\nk…ënpeku\nlu√¶te\nk…ë_peku\nlu É√¶te\ndeleting final C of word1\ninserting some LC after the word1\n\n\n\n\n\nTesting Task 2: Familiar Segmentation\nIn Task 2, the forced choice task was essentially a morpheme segmentation exercise. Participants viewed an image while listening to a phrase, which was followed by two choices representing potential realizations of the word2s from the phrase, as pronounced in isolation. Participants were instructed to choose the isolated word referring to the object‚Äôs name that seems correct to them by pressing keys on the keyboard. Essentially, in this task participants decide on the segmentation of the words using their own intuitions because the training trials did not contain any isolated words. All the word2 choices were known by participants from the training phase, although 4 of the 16 trials included held-out phrases. The stimuli were concatenated automatically using a Praat script, with an inter-stimulus interval (ISI) of 100ms between the full phrase and the first choice, and a 75ms ISI between the two choices. In contrast to the previous task, task 2 only included alternating word1s, so the trials are categorized into just two types: ‚Äúbasic‚Äù and ‚Äúalternating‚Äù. Including non-alternating word1s /lu/ and /k…ën/ was deemed unsuitable for this task, as our primary interest lay in how participants address the ambiguity in parsing liaison consonants / É/ and /t/. Table 3-7 illustrates examples of each item type.\nBasic trials (B-A) comprised testing items formed with word1s that did not have any liaison consonants (LC), meaning a consonant-initial word2 was present. In these trials, an incorrect choice resulted from misparsing the onset of word2 as an LC.\nConversely, alternating trials (Alt) featured items constructed with words that included an LC, indicating that a vowel-initial word2 was used. Here, the incorrect choice stemmed from misparsing the LC as the word2‚Äôs onset.\n\nTable ‚Äé3‚Äë7. Testing trial types in task 2 (familiar segmentation) with examples\n\n\n\n\n\n\n\n\n\nTesting Trial Type\nPrompt\nCorrect\nIncorrect\nType of Error\n\n\n\n\nitemBasic (B-A)\nn=8\nme kis…ë\ndim√¶ti\nkis…ë\nm√¶ti\nis…ë\n√¶ti\nmisparse the onset of word2 as LC\n\n\nitemAltlternating (Alt)\nn=8\nme Éido\ndited√¶p\nido\ned√¶p\n Éido\nted√¶p\nmisparse the LC as the word2‚Äôs onset\n\n\n\n\n\nTesting Task 3: Novel Segmentation\nIn the last task, testing task 3, the forced choice task was essentially the same morpheme segmentation task as above but with a different stimulus type. Specifically, the word2s here were all novel word2s that did not appear in the training phase at all. In this task similar to the first task the trials were divided into 3 types based on their word1 type as well as the error type: ‚Äúbasic‚Äù, ‚Äúalternating‚Äù, and ‚Äúnon-alternating‚Äù. Table 3-8 below shows examples of each trial type.\nThe basic trials involved testing items that were constructed using word1s without any LC or final consonants, specifically when a consonant-initial word2 was present. In these trials, the incorrect response was due to misparsing the word2 as vowel-initial.\nThe alternating trials, on the other hand, included items with the LC, occurring when there was a vowel-initial word2. In these cases, the incorrect choice stemmed from misinterpreting the LC as belonging to word2.\nThe non-alternating trials were solely comprised of /k…ën/, as it allowed for the inclusion of the final consonant to provide an incorrect option which could involve interpreting /n/ as belonging to the word2 (in contrast, /lu/ was used in the basic trials.\nTable ‚Äé3‚Äë8. Testing trial types in task 3 (novel segmentation) with examples\n\n\n\n\n\n\n\n\n\n\nTesting Trial Type\nPrompt\nCorrect\nIncorrect\nType of Error\n\n\n\n\nitemBasic (B-A)\nn=4\nmef√¶rus\ndit É√¶gef\nf√¶rus\nt É√¶gef\n√¶rus\n√¶gef\nmisparse the onset of the word2 as LC\n\n\nitemBasic (B-NA)\nn=4\nlurunip\nrunip\nunip\nmisparse the onset of the word2 as LC\n\n\nitemAltlternating\n(Alt) n=4\ndit…ëjor\nme Éez…ë\n…ëjor\nez…ë\nt…ëjor\n Éez…ë\nmisparse the LC as the word2‚Äôs onset\n\n\nitemNon-alternating (NonAlt) n=4\nk…ëne Éin\nk…ënosu\ne Éin\nosu\nne Éin\nnosu\nmisparse the final /n/ as the word2‚Äôs onset",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "chapters/3_methods.html#data-collection-procedure-and-data-analysis",
    "href": "chapters/3_methods.html#data-collection-procedure-and-data-analysis",
    "title": "3¬† Methods",
    "section": "3.5 Data collection procedure and data analysis",
    "text": "3.5 Data collection procedure and data analysis\nThe data collection procedures and methods for the experiment were according to the standards set by the UBC Behavioral Research Ethics Board and received their approval. Participants were recruited from the UBC linguistics participants pool in exchange for course credit for undergraduate-level linguistics courses. Potential participants were informed on the recruitment page that the study would involve learning sound patterns in an unfamiliar language and responding to some questions. All participants in this pool had to complete a language background questionnaire comprised of a comprehensive set of questions about all the languages they know or have experience with, as well as demographic information and relevant language disorders. To be eligible, participants must not have hearing loss. The language background eligibility criterion chosen was to only include participants that have not listed their L1(s) as French with an age of acquisition below four. This inclusion criterion was implemented to ensure participants shared a more homogeneous language background, thereby minimizing potential confounds arising from native-speaker intuitions regarding liaison-like patterns.\nThe experiment was executed using HTML and jsPsych, a JavaScript framework (de Leeuw, 2015), and was conducted remotely through a web browser. As a result, participants were able to take part in the experiment using their own desktop devices remotely. To ensure consistent audio quality and experimental conditions for all participants, a headphone check task (borrowed from Woods et al., 2017) was included. Participants were required to answer 4 of the 6 headphone check trials correctly to proceed to the experiment. After completing the experiment, the data was collected anonymously and securely stored on a UBC server in a comma-separated values (.csv) format.\nTo prepare files for analysis, the researcher conducted pre-processing and data-cleaning procedures on the .csv files using specific packages in R Studio (version 2024.04.0). During this process, apart from the eligibility criteria, to ensure that the conclusions drawn from the data are robust and informative participant data were also removed based on predetermined performance exclusion criteria allowing the researcher to focus on the effects of the experimental manipulations. At first, the data of 4 participants were removed as they had mentioned the word ‚Äúliaison‚Äù in their written feedback collected in the debriefing stage at the end of the experiment. Then the first of the predetermined exclusion criteria was applied to verify that participants were seriously engaged in the experiment. The participants who had over 10% of their total responses (n&gt;7) classified as null answers‚Äîspecifically, instances of not pressing a key during testing trials‚Äîwere excluded. After this initial filtering, among the remaining participants, the next criterion was to ascertain that they understood the task and its objectives. To assess this, participants whose mean accuracy on basic items in each task fell more than two standard deviations below the overall mean of all participants were also excluded from the experiment. In total, the data from 43 participants were removed from the study. The Table 3-9 shows the summary of the excluded participants by condition.\n\nTable ‚Äé3‚Äë9. Breakdown of participants exclusion by condition and criteria\n\n\n\n\n\n\n\n\n\n\nAlternating 1st\nNon-alternating 1st\nMixed\ntotal\n\n\n\n\nRecruited\n82\n83\n76\n241\n\n\nLanguage Background\n1\n3\n1\n5\n\n\nDebriefing\n1\n1\n2\n4\n\n\nNull answers\n-\n2\n2\n4\n\n\nBasic items mean\n9\n11\n10\n30\n\n\nAfter exclusion\n71\n66\n61\n198\n\n\n\nFinally, statistical analysis and modelling were performed using additional R packages, such as lme4 (Bates, et al., 2015). More details about the statistical models will be provided in the next chapter.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "chapters/3_methods.html#footnotes",
    "href": "chapters/3_methods.html#footnotes",
    "title": "3¬† Methods",
    "section": "",
    "text": "https://graphicsurf.com/‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "chapters/4_results.html",
    "href": "chapters/4_results.html",
    "title": "4¬† Results",
    "section": "",
    "text": "4.1 Task1 - Phrase Judgement\nThe first 2AFC task included 32 trials with the full phrases (word1+word2) as choices. To observe differences between the learning trajectories as hypothesized, it is necessary to compare the overall performance of participants in the 3 training conditions in this task. Figure 4-1 below plots overall accuracy by condition. The means (and SDs) of the participant means in the three conditions are: Alt1st 0.79 (0.12), NAlt1st 0.76 (0.10), and Mix 0.81 (0.08). The notches on the boxplots indicate the 95% confidence interval of the median and the small point on the box indicates the mean. The box represents the interquartile range, encompassing the two middle quartiles on either side of the median. By definition, this means that half of the participants fall within the box, while the other half lie outside of it. Notably, the overall mean accuracy as well as the 95% confidence interval ranges of the medians of the participants in all training conditions within this 32-trial task is above 0.69. This is a statistically above chance accuracy rate according to a right-tailed binomial test, with 0.05 alpha.\nThe trial types were categorized in 4 levels (total n 32 = 4 levels √ó 8 items): itemNonAlt, itemAltlt, itemB-A, itemB-NA. To better understand how the distribution of accuracy for participant responses in each training condition differ across the four trial types in this task and how trial types interact with conditions, it is important to compare the accuracy of participant responses to each trial type within the three conditions. As can be seen in the boxplots in Figure 4-2, for participants in all conditions, the itemB-A and itemB-NA (two boxes on the right) have similar distributions and their means are above 0.8 but their notch areas do not overlap with the itemAlt and itemNonAlt (two boxes on the left). The corresponding means (SDs) of each condition within the 4 trial types is also presented in Table 4-1 along with an example item (to see more examples, refer to Tables (3-6‚Äí3-8) in Chapter 3). In the table, the highest mean value among the three conditions in each row (trial type) is underlined.\nFigure 4-3 below shows the accuracy rates distributions of all participants in the three conditions to allow for a comparison of trials that contained familiar items (n=24) against held-out items (n=8). As can be seen in the boxplots, the distribution of the participant responses to familiar and held-out items overlap and their means are very close to each other for all conditions.\nA mixed-effects logistic regression model with the following formula shown in (1) including 4 fixed effects in addition to 2 random effects of intercept and slope for trial type by subject and intercept and slope for condition by item was fitted to the data.\n(1)\nresponses ~ condition + trial_type + condition:trial_type + item_familiarity +\n(1 + trial_type | subject) + (1 + condition | item)\nThe contrasts were treatment-coded where training condition (Alt1st=ref, NAlt1st, Mix), trial type (itemNonAlt=ref, itemB-A, itemB-NA, itemAltlt) and item familiarity (familiar=ref, held-out) and the interaction of condition and trial type were considered as fixed effects each with a reference level. The results showed two significant coefficient estimates: as for the training conditions, when the other variables are held constant at their respective reference levels, that is on itemNonA trials, and familiar items, participants in Alt1st were significantly more accurate than NAlt1st condition (Œ≤=-0.48, SE=0.21, p=0.02), and as for the item types, when other variables are held constant on Alt1st condition and familiar items, the responses to NonAlt trials was significantly less accurate than B-NA trials (Œ≤=1.2, SE=0.53, p=0.04).\nIn the model comparisons with a chi-square likelihood ratio test, the model above (1) once compared to a simpler model without the fixed effect of condition (p= 0.02) and once without the fixed effect of trial type (p=0.007) was significantly different, but not significantly improved compared to a model without the item familiarity as a fixed effect. It is worth noting that comparing the model in (1) with a simpler model without interaction effects did not indicate significant improvement either. However, as it is important for the research question to know the between-condition differences, using the emmeans package theoretically relevant interaction differences were calculated. Table (4.2) shows all the comparisons based on the emmeans package calculated for all the interactions (estimated in differences in log odds), averaged over the item familiarity variable with Bonferroni adjustment. As it is shown in the pairwise comparisons, the participants in Mix and the Alt1st conditions are more accurate than NAlt1st across all trial types; although the only statistically significant comparison is within their responses to the NonAlt items, where participants in NAlt1st were significantly less accurate than the Mix condition (Œ≤=-0.67, SE=0.21, p=0.00). The full model summaries can be found in the Appendix B1.\nThese comparisons can be viewed in Figure 4.4 which illustrates the predicted probability of accuracy from the mixed-effects logistic regression model. These probability values are averaged over the levels of item familiarity using marginal means. As can be seen from the lowest point (colored in light blue), the accuracy of participants in the NonAlt1st condition differ with other conditions on NonAlt trials more strongly with a bigger effect size.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "chapters/4_results.html#task1---phrase-judgement",
    "href": "chapters/4_results.html#task1---phrase-judgement",
    "title": "4¬† Results",
    "section": "",
    "text": "Figure ‚Äé4‚Äë1. Overall accuracy by condition in task 1 (phrase judgement)\n\n\n\n\nTable ‚Äé4‚Äë1. Mean accuracy by condition across trial type in task 1 (phrase judgement)\n\n\n\n\n\n\n\n\nTrial Type\nAlt1st\nNAlt1st\nMix\n\n\n\n\nitemNonAlt (n= 8)\n[lu√¶te] vs [lu É√¶te]\n0.71 (0.17)\n0.64 (0.19)\n0.75 (0.15)\n\n\nitemAlt (n= 8)\n[ditug√¶ É] vs [diug√¶ É]\n0.78 (0.16)\n0.76 (0.17)\n0.77 (0.15)\n\n\nitemB-A (n= 8)\n[me É√¶te] vs [me Ét√¶te]\n0.82 (0.16)\n0.83 (0.14)\n0.85 (0.15)\n\n\nitemB-NA (n= 8)\n[lubudol] vs [lutbudol]\n0.86 (0.16)\n0.84 (0.16)\n0.87 (013)\n\n\n\n\n\n\nFigure ‚Äé4‚Äë2. Accuracy by condition across trial type in task 1 (phrase judgement)\n\n\n\n\n\n\nFigure ‚Äé4‚Äë3. Accuracy by condition across item familiarity in task 1 (phrase judgement)\n\n\n\n\n\n\n\n\nTable ‚Äé4‚Äë2. Pairwise comparisons in the statistical model based on the emmeans package in task 1 (phrase judgement)\n\n\n\n\n\n\n\n\n\n\nContrast\nTrial type\nEstimate\nSE\nz.ratio\np.value\n\n\n\n\nAlt1st - NonAlt1st\nitemNonAlt\n0.48\n0.21\n2.31\n0.06\n\n\n\nitemAltlt\n0.27\n0.24\n1.13\n0.78\n\n\n\nitemB_A\n0.05\n0.25\n0.21\n1.00\n\n\n\nitemB_NA\n0.28\n0.28\n0.99\n0.96\n\n\nAlt1st - Mix\nitemNonAlt\n-0.18\n0.17\n-1.10\n0.82\n\n\n\nitemAltlt\n0.11\n0.20\n0.57\n1.00\n\n\n\nitemB_A\n-0.19\n0.22\n-0.85\n1.00\n\n\n\nitemB_NA\n-0.09\n0.26\n-0.34\n1.00\n\n\nNonAlt1st - Mix\nitemNonAlt\n-0.67\n0.21\n-3.20\n0.00\n\n\n\nitemAltlt\n-0.15\n0.24\n-0.66\n1.00\n\n\n\nitemB_A\n-0.24\n0.26\n-0.93\n1.00\n\n\n\nitemB_NA\n-0.37\n0.29\n-1.27\n0.61\n\n\n\n\n\n\n\nFigure ‚Äé4‚Äë4. Predicted probabilities of correct responses for each combination of condition and trial type in task 1 (phrase judgement)",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "chapters/4_results.html#task-2---familiar-segmentation",
    "href": "chapters/4_results.html#task-2---familiar-segmentation",
    "title": "4¬† Results",
    "section": "4.2 Task 2 - Familiar Segmentation",
    "text": "4.2 Task 2 - Familiar Segmentation\nThe second 2AFC task included 16 trials with the choices being between two potential word2s in the provided full phrase in the prompt. Similar to the previous task, to observe differences between the learning trajectories as hypothesized, it is necessary to compare the overall performance of participants in the 3 training conditions in this task. Figure 4.5 below plots overall accuracy by condition. The means (SD) of the participant means in the three conditions are almost at chance in this 16-trial task1: Alt1st 0.57 (0.15), NAlt1st 0.58 (0.16), and Mix 0.62 (0.17).\n\n\n\nFigure ‚Äé4‚Äë5. Overall accuracy by condition in task 2 (familiar segmentation)\n\n\nThe trial types were categorized in 2 levels (total n 16= 2 levels √ó 8 items): itemAltlt, itemB-A. Figure 4-6 shows the accuracy of participants in the three conditions within different trial types to visualize how the distribution of participant responses in each training condition differ across the two trial types in this task and how trial type interacts with condition. The corresponding means (s.d.) are also presented in Table 4-3 along with example items. In the table, the highest mean value among conditions in each row (trial type) is underlined. The distribution of the two trial types is completely non-overlapping, however the participants in all conditions seem to perform similarly across trial types where all participant means within itemAlts are below 0.5 and all participant means within itemB-A are above 0.85.\n\nTable ‚Äé4‚Äë3. Mean accuracy by condition across trial type in task 2 (familiar segmentation)\n\n\n\n\n\n\n\n\nTrial Type\nAlt1st\nNAlt1st\nMix\n\n\n\n\nitemAlt (n= 8)\n[dited√¶p] + [ed√¶p] vs [ted√¶p]\n0.38 (0.30)\n0.41 (0.30)\n[0.50 ( 0.31)]{.u nderline}\n\n\nitemB-A (n= 8)\n[mekis…ë] + [kis…ë] vs [is…ë]\n0.89 (0.15)\n0.89 (0.17)\n[0.90 ( 0.13)]{.u nderline}\n\n\n\n\n\n\nFigure ‚Äé4‚Äë6. Accuracy by condition across trial type in task 2 (familiar segmentation)\n\n\nFigure 4-7 below shows the accuracy rate distributions of all participants in the three conditions to compare responses to trials that contained familiar items (n=12) against held-out items (n=4). As can be seen in the boxplots, similar to the previous task, the distribution of the responses to familiar and held-out items overlap in all conditions and their means are very close to each other.\n\n\n\nFigure ‚Äé4‚Äë7. Accuracy by condition across item familiarity in task 2 (familiar segmentation)\n\n\nA mixed-effects logistic regression model with the following formula shown above in (1) including 4 fixed effects in addition to 2 random effects of intercept and slope for trial type by subject and intercept and slope for condition by item was fitted to the data.\nThe contrasts were treatment coded where training condition (Alt1st=ref, NAlt1st, Mix), trial type (itemAltlt=ref, itemB-A) and item familiarity (familiar=ref, held-out) and the interaction of condition and trial type was considered as fixed effects each with a reference level. The results showed only one significant coefficient estimate as the other variables are held constant on Alt1st and familiar items, the itemAltlt compared to the itemB-A were significantly worse (Œ≤=4.47, SE=0.41, p&lt;0.001). The full model summaries can be found in the Appendix B2.\nIn the model comparisons with a chi-square likelihood ratio test, the model above (1) compared to a simpler model without the fixed effect of trial type (p=0) was significantly different, but not significantly improved compare to a model without the item familiarity and condition as fixed effects separately. It is worth noting that comparing the following model with a simpler model without interaction effects with a chi-square likelihood ratio test did not indicate significant improvement either. Nevertheless, using the emmeans package theoretically relevant interaction differences were calculated. As it is shown in the pairwise comparisons in Table 4-4, across all trial types the participants in NAlt1st and the Alt1st conditions are less accurate than Mix while participants in Alt1st are less accurate than NAlt1st. Although, none of the pairwise comparisons are statistically significant.\n\nTable ‚Äé4‚Äë4. Pairwise comparisons in the statistical model based on the emmeans package in task 2 (familiar segmentation)\n\n\n\n\n\n\n\n\n\n\nContrast\nTrial type\nEstimate\nSE\nz.ratio\np.value\n\n\n\n\nAlt1st - NonAlt1st\nitemAltlt\n-0.21\n0.44\n-0.47\n1.00\n\n\n\nitemB_A\n-0.61\n0.50\n-1.22\n0.67\n\n\nNonAlt1st - Mix\nitem_A\n-0.40\n0.46\n-0.87\n1.00\n\n\n\nitemB_A\n-0.10\n0.34\n-0.31\n1.00\n\n\nAlt1st - Mix\nitemAltlt\n-0.44\n0.43\n-1.03\n0.90\n\n\n\nitemB_A\n-0.34\n0.37\n-0.89\n1.00\n\n\n\nThese comparisons can be viewed in Figure 4-8 which illustrates the predicted probability of a correct response from the mixed-effects logistic regression model. These probability values are averaged over the levels of item familiarity using marginal means. As can be seen from the lower point (colored in blue), the performance of participants on the itemAltlt trials is close to floor while their performance on itemB-A trials is close to the ceiling.\n\n\n\nFigure ‚Äé4‚Äë8. Predicted probabilities of correct responses for each combination of condition and trial type in task 2 (familiar segmentation)",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "chapters/4_results.html#task-3---novel-segmentation",
    "href": "chapters/4_results.html#task-3---novel-segmentation",
    "title": "4¬† Results",
    "section": "4.3 Task 3 - Novel Segmentation",
    "text": "4.3 Task 3 - Novel Segmentation\nThe third 2AFC task was similar to the previous task except all the word2s were novel; therefore, the item familiarity variable is irrelevant here. To observe differences between the learning trajectories as hypothesized, the training conditions‚Äô overall performances are compared in Figure 4-9 below. The means (SDs) of the participant means in the three conditions are almost at chance2: Alt1st 0.57 (0.14), NAlt1st 0.58 (0.16), and Mix 0.64 (0.17).\n\n\n\nFigure ‚Äé4‚Äë9. Overall accuracy by condition in task 3 (novel segmentation)\n\n\nThe trial types were categorized in 4 levels (total n 16 = 4 levels √ó 4 items): itemNonAlt, itemAltlt, itemB-A, itemB-NA. Figure 4-10 shows the accuracy of conditions within different trial types, to better understand how the distribution of participant responses in the three training conditions differs across different trial types in this task and how trial types interact with conditions. As can be seen in the boxplots, for all participants, the B items (two boxes on the right) have similar distributions and their means are above 0.8 whereas within the A and NonA items (two boxes on the left) there is more variability. The corresponding means (SD) are also presented in Table 4-5 along with an example item. In the table, the highest mean value among conditions in each row (trial type) is underlined.\n\nTable ‚Äé4‚Äë5. Mean accuracy by condition across trial type in task 3 (novel segmentation)\n\n\n\n\n\n\n\n\nTrial Type\nAlt1st\nNAlt1st\nMix\n\n\n\n\nitemNonAlt (n= 4)\n[k…ëne Éin] + [e Éin] vs [ne Éin]\n0.54 (0.29)\n0.63 (0.29)\n[0.68 (0.32)] {.underline}\n\n\nitemAlt (n= 4)\n[dit…ëjor] + […ëjor] vs [t…ëjor]\n0.55 (0.32)\n0.53 (0.28)\n[0.63 (0.33)] {.underline}\n\n\nitemB-A (n= 4)\n[mef√¶rus] + [f√¶rus] vs [√¶rus]\n0.86 (0.24)\n0.89 (0.19)\n[0.90 (0.19)] {.underline}\n\n\nitemB-NA (n= 4)\n[lurunip] + [runip] vs [unip]\n0.88 (0.18)\n0.79 (0.23)\n[0.89 (0.18)] {.underline}\n\n\n\n\n\n\nFigure ‚Äé4‚Äë10. Accuracy by condition across trial type in task 3 (novel segmentation)\n\n\nA mixed-effects logistic regression model with the following formula (2) including 3 fixed effects of condition, trial_type, and the interaction of condition and trial_type in addition to 2 random effects of intercept and slope by subject and intercept and slope by item was fitted to the data:\n(2)\nresponses ~ condition + trial_type + condition:rial_type + (1 + trial_type | subject)\n+ (1 + condition | item)\nThe contrasts were treatment coded where training condition (Alt1st=ref, NAlt1st, Mix), item type (itemNonAlt=ref, itemB-A, itemB-NA, itemAlt) and the interaction of condition and item type were considered as fixed effects each with a reference level. The results showed multiple significant coefficient estimates: when the other variables are held constant at their respective reference levels, that is on itemNonA trials, and familiar items, among the training conditions, participants in Alt1st were significantly less accurate than Mix (Œ≤=0.99, SE=0.49, p=0.05), and when the other variables are held constant at their respective reference levels, that is on Alt1st condition, and familiar items, among the item types, all differences were significant: participant responses to NonA were less than B-NA items (Œ≤=3.51, SE=0.44, p=0), responses to NonA was less than B-A items (Œ≤=4.04, SE=0.51, p=0), and responses to NonA was higher than A items (Œ≤=-1.72, SE=0.34, p=0). The full model summaries can be found in the Appendix B3.\nIn the model comparisons with a chi-square likelihood ratio test, the model above (2) once compared to a simpler model without the fixed effect of condition (p= 0.02) and once without the fixed effect of trial type (p=0) was significantly different. It is noteworthy that comparing the following model with a simpler model without interaction effects with a chi-square likelihood ratio test did not indicate significant improvement. Regardless, using the emmeans package theoretically relevant interaction differences were calculated. As it is shown in the pairwise comparisons in Table 4-6, across the itemNonAlt, itemAlt, itemB-A trial types the participants in the Alt1st conditions are less accurate than NonAlt1st and Mix. Although, none of the pairwise comparisons are statistically significant.\n\nTable ‚Äé4‚Äë6. Pairwise comparisons in the statistical model based on the emmeans package in task 3 (novel segmentation)\n\n\n\n\n\n\n\n\n\n\nContrast\nTrial type\nEstimate\nSE\nz.ratio\np.value\n\n\n\n\nAlt1st - NonAlt1st\nitemNonAlt\n-0.46\n0.50\n-0.92\n1.00\n\n\n\nitemAlt\n-0.42\n0.63\n-0.66\n1.00\n\n\n\nitemB_A\n-0.27\n0.50\n-0.53\n1.00\n\n\n\nitemB_NA\n0.73\n0.35\n2.11\n0.10\n\n\nAlt1st - Mix\nitemNonAlt\n-0.99\n0.49\n-1.99\n0.14\n\n\n\nitemAlt\n-1.46\n0.62\n-2.35\n0.06\n\n\n\nitemB_A\n-0.16\n0.49\n-0.31\n1.00\n\n\n\nitemB_NA\n0.06\n0.35\n0.16\n1.00\n\n\nNonAlt1st - Mix\nitemNonAlt\n-0.53\n0.50\n-1.05\n0.88\n\n\n\nitemAlt\n-1.05\n0.62\n-1.68\n0.28\n\n\n\nitemB_A\n0.11\n0.50\n0.22\n1.00\n\n\n\nitemB_NA\n-0.68\n0.33\n-2.05\n0.12\n\n\n\nThese comparisons can be viewed in Figure 4-11 which illustrates the predicted probability of accuracy on the task, based on population-level predictions from the mixed-effects logistic regression model.\n\n\n\nFigure ‚Äé4‚Äë11. Predicted probabilities of correct responses for each combination of condition and trial type in task 3 (novel segmentation)",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "chapters/4_results.html#task-2-and-3---both-segmentation-tasks",
    "href": "chapters/4_results.html#task-2-and-3---both-segmentation-tasks",
    "title": "4¬† Results",
    "section": "4.4 Task 2 and 3 - Both Segmentation Tasks",
    "text": "4.4 Task 2 and 3 - Both Segmentation Tasks\nIn sections 4.2 and 4.3 the two segmentation tasks were analyzed separately. To analyze both tasks together, the trial types are categorized based on the word2s. In order to test if different word2 types have a statistically significant effect on participants‚Äô performance, the trials are categorized in terms of whether the intended word2 in the phrase was C-initial or V-initial. This is equivalent to merging the itemAlt and itemNonAlt levels under the V-initial trials and merging itemB-A and itemB-NA levels together under the C-initial trials. Item familiarity has 3 levels here, familiar (phrases that occurred in training), held-out (phrases that did not occur training but the word1s and word2s appeared in training individually), and novel (phrases that did not occur in training because the word2s were absent in training), unlike the previous models.\nA mixed-effects logistic regression model with the following formula (3) including fixed effects of condition, trial type (based on word2), item familiarity, and the interaction of condition and trial type in addition to 2 random effects of intercept and slope by subject and intercept and slope by item was fitted to the data:\n(3)\nresponses ~ condition + trial_type + condition:trial_type + item_familiarity + (1 + trial_type | subject) + (1 + condition | item)\nThe contrasts were treatment-coded where training condition (Alt1st=ref, NAlt1st, Mix), trial types were divided into 2 levels based on their word2 shape (C-initial=ref, V-initial) and item familiarity in 3 levels (familiar=ref, held-out, novel) and the interaction of condition and trial type as fixed effects each with a reference level. The results showed only one significant coefficient estimatewhen the other variables are held constant on Alt1st condition, and familiar items, the C-initial word2s were more accurate than V-initial word2s (Œ≤=-4.24, SE=0.40, p=0). Figure 4-12 illustrates the predicted probability of accuracy on both segmentation tasks responses combined. These probability values are averaged over the levels of item familiarity using marginal means.\n\n\n\nFigure ‚Äé4‚Äë12. Predicted probabilities of correct responses for each combination of condition and trial type in task 2 and 3 (segmentations)\n\n\nIn the model comparisons with a chi-square likelihood ratio test, a model with 3 fixed effects once compared to a simpler model without the fixed effect of trial type (p=0) was significantly different and once without the fixed effect of condition was nearly significant (p=0.05), but not significantly different without the item familiarity. The model above (3) was not significantly improved compared to a model without interaction effect. The full model summaries can be found in the Appendix B4.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "chapters/4_results.html#footnotes",
    "href": "chapters/4_results.html#footnotes",
    "title": "4¬† Results",
    "section": "",
    "text": "The term ‚Äúat chance‚Äù here refers to a descriptive chance level of accuracy (8/16 trials). The binomial test was not used anymore to determine if this accuracy is significantly below or above chance level.‚Ü©Ô∏é\nThe term ‚Äúat chance‚Äù here refers to a descriptive chance level of accuracy (8/16 trials). The binomial test was not used anymore to determine if this accuracy is significantly below or above chance level.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "chapters/5_discussion.html",
    "href": "chapters/5_discussion.html",
    "title": "5¬† Discussion",
    "section": "",
    "text": "5.1 The effect of all the independent variables on learning\nBy designing an artificial mini-language exhibiting morpheme-specific phonological patterns, the aim in this implicit learning experiment was to induce inconsistencies in the phonological system underlying the input data. The motivation behind the primary research question was to investigate in what ways the structure of the input data, specifically the arrangement of blocks of training data affects learning. Based on a theory of learning with an error-driven learning algorithm, assuming the same initial state, and the same final state of learning theoretically for all learners in the three conditions of the experiment, the rationale was that structured input data would influence the learning trajectory of learners. The null hypotheses were that learning for all participants in the different conditions would be equally difficult; additionally, that the difficulty of learning would not depend on test trial types. In contrast to these null hypotheses, I would expect that except for item familiarity, the other independent variables in the experiment‚Äîtraining condition (i.e., the learning trajectory), test trial type and their interaction‚Äîwould be significant predictors of participants‚Äô performance in testing. According to our predictions provided in Section 2.5 in Chapter 2, if the learning trajectories in each condition affect the three aspects in learning and participants‚Äô responses, participants‚Äô performance in each condition would reflect the difficulty in the expected direction. Additionally, if the learning trajectories in each condition affect the extent to which learners learn various items, participants‚Äô performance in each condition would depend on the test trial type.\nThe results presented in the previous chapter, analyzed through statistical models and post-hoc chi-square likelihood ratio tests indicate the following conclusions regarding our null hypotheses. The interaction of training conditions and test trial types did not significantly improve the model fit, therefore failing to reject our null hypothesis that differences in performance between participants in the three conditions does not depend on the test trial type.\nWith respect to the other fixed effects in the statistical analysis, the inclusion of the three training conditions, each with a different learning input data structure, significantly improved the model fit. Therefore, this result rejects the null hypothesis that training condition has no effect on participant responses; albeit with the caveat that these results support the prediction only in 2 out of the 3 tasks: task 1 (the phrase judgement task), and task 3 (the novel segmentation task), and not task2 (the familiar segmentation task). The inclusion of test trial types, each with a different item and error type, improved the model fit in all tasks, therefore rejecting the null hypothesis that test trial type has no effect on participant responses. It is important to note that the fixed effect of item familiarity, the distinction between familiar and held-out phrases, was not a significant predictor in any of the tasks, which fails to reject the null hypothesis that item familiarity has no effect on participant responses. I predicted that item familiarity would be the only independent variable not involved in affecting participant response. Thus, in this case, failure to reject the null hypothesis supports the initial prediction about item familiarity. This result could be interpreted as an indication that the performance of participants did not rely on memorization of the items in the phrase judgement task and the segmentation task. Although from a different perspective, it is worth mentioning that having combined the two segmentation tasks in one model as presented in Section 4.4, item familiarity with three levels (familiar, held-out, novel) did not significantly improve the model fit. In other words, participants‚Äô overall performance in the segmentation tasks is not predicted by item familiarity.\nIn the next section, I will discuss the accuracy of participants in each condition on various types of testing trials in the same task as well as across tasks to assess the specific predictions about the three aspects of learning.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Discussion</span>"
    ]
  },
  {
    "objectID": "chapters/5_discussion.html#the-effect-of-the-interaction-of-input-data-structure-and-test-trial-types-on-learning",
    "href": "chapters/5_discussion.html#the-effect-of-the-interaction-of-input-data-structure-and-test-trial-types-on-learning",
    "title": "5¬† Discussion",
    "section": "5.2 The effect of the interaction of input data structure and test trial types on learning",
    "text": "5.2 The effect of the interaction of input data structure and test trial types on learning\nIn an error-driven learning theory, as learning progresses during the training phase, participants engage in strategies involved in the components of learning and employ mechanisms to construct the constraint hierarchy and the final state grammar. These components of learning which are generally thought to occur iteratively and in parallel, involve interpreting the structure of the output (phonological and morphological structure) and computing the underlying representation (Tesar and Smolensky, 2000). By the time the learners finish the training phase they must also employ a mechanism to resolve the inconsistency caused by the alternating and non-alternating morphemes. The experimental design for the testing phase operates under the set of assumptions that all participants by the end of the training have established a UR for each morpheme and a constraint hierarchy as their end-state grammar. In each forced-choice trial, their choice corresponds to the optimal candidate that emerges from the final-state grammar given the underlying representation they have posited for each particular morpheme. The responses coded as correct in the forced choice tasks are based on the optimal outputs from the end state grammar and the particular set of underlying representations predicted within the learnability framework outlined in Section 2.5. In order to effectively analyze the dimensions of learning, the testing trials were categorized by the types of errors and the specific word1 types. The participants in the three conditions experience distinct learning trajectories as receiving the input learning data in different orders is expected to influence the extent to which these dimensions are hard to learn. Based on this rationale, I hypothesized that there would be differential learning between participants in the three conditions, qualitatively and quantitatively. Specifically, I predicted that different learning trajectories could lead to differences in the accuracy rates among participants in the three conditions. In my hypothesis, the quantitative difference refers to the extent to which the participants have been affected in 1 (or more) dimension of their learning: underlying representation of the alternating morphemes reranking stages, and constraint cloning or lexicon indexation. The qualitative difference refers to differences in participants‚Äô performance on trial types within each condition.\nAs mentioned in the previous section, the prediction about the qualitative difference was not supported; that is, the null hypothesis regarding the interaction of condition and trial type was not rejected. As for the quantitative difference, the null hypothesis was rejected in the phrase judgement task and the novel segmentation task. Although, regardless of significant differences between conditions within some trial types in the pairwise comparisons, none of the predicted differences between conditions were observed. To discuss the current findings and their significance, all the relevant tasks and the items are discussed under each prediction separately in the following subsections.\n\n5.2.1 Computing the underlying representation (in alternations)\nThe first prediction was about the dimension of learning where differences in learning data and trajectory are expected to affect the learning of underlying representations in alternations. The interdependent and iterative nature of the strategies to compute underlying representations and to construct hierarchies given an output in different components of learning makes it challenging to isolate their effect in the learning outcome. Nevertheless, if the effect of the learning trajectory is strong enough, we could observe differences in performance. The prediction was that the learning trajectory of the participants in the Alt1st condition would be more advantageous for acquiring the underlying representations of the alternating morphemes (itemAlts and itemB-As in testing) because their input data is superior to that of other conditions. The initial data that the Alt1st condition participants receive in their first block makes them aware of the alternation sooner in their trajectory than others since they get the contrastive paradigm of different liaison consonants before the same word2 as shown in Section 3.4.1, copied below in Table 5-1. For instance, when a V-initial word2 such as /ibud/ ‚Äòcamera‚Äô appears in a phrase, the pseudo-randomization ensures that the same word2 appears in the next phrase. In this way, both alternating word1s appear consecutively in the same word2 context: /ditibud/, /me Éibud/, ‚Ä¶ which is expected to facilitate the learners‚Äô understanding of segmentation, and eventually the underlying representation. In terms of proportions, in each block, in the Alt1st and NonAlt1st conditions, 12 out of the 24 unique phrases contain V-initial word2s. With the specific arrangement of pseudo-randomization shown in Table 5-1, 8 phrases out of the 12 phrases with V-initial word2s always appear in pairs. This distribution was intended to reduce the probability of the word2s being coincidentally re-analyzed as / É/- or /t/-initial words. The participants in the NonAlt1st condition do not experience this paradigm in their first block of training, so they are expected to be at a disadvantage. The Mix condition only gets one alternating word1 in each of their blocks so according to this argument their accuracy rate would be between the Alt1st and the NonAlt1st condition.\n\nTable ‚Äé5‚Äë1. Pseudo-randomization order for Alt1st condition\n\n\n\n\n\n\n\nBlock 1\n‚Äúalternating word1s‚Äù\n‚Üí\nBlock 2\n‚Äúnon-alternating word1s‚Äù\n\n\n\n\n/dit/ + /ibud/\n/me É/ + /ibud/\n/me/ + /kis…ë/\n/dit/ + /ufe/\n/me É/ + /ufe/\n/di/ + /peku/\n/me/ + /peku/\n/di/ + /budol/\n‚Ä¶\n\n/lu/ + /ibud/\n/k…ën/ + /ibud/\n/k…ën/ + /kis…ë/\n/lu/ + /ufe/\n/k…ën/ + /ufe/\n/lu/ + /peku/\n/k…ën/ + /peku/\n/lu/ + /budol/\n‚Ä¶\n\n\n\nContrary to our predictions, the statistical analysis of the two segmentation tasks did not find any significant differences in the interaction of trial types and conditions. However, the results of the pairwise comparisons from the novel segmentation task show that within the itemAlt trials (alternating word1s) when the item familiarity is held constant the participants in the Mix are more accurate than Alt1st. This comparison was not technically statistically significant but it is being reported as the p-value was approaching significance (p= 0.06).\n\nTable ‚Äé5‚Äë2. Predictions and results of learning the underlying representation\n\n\n\n\n\n\n\n\nPrediction\n\nResults\nTask ‚Äì Item type\n\n\n\n\nAlt1st &gt; Mix &gt; NonAlt1st\n‚ùå\nMix &gt; Alt1s\n(nearly significant)\nNovel Segmentation\nitemAlt\n\n\n\nTo understand this result in the context of our predictions, it is important to examine how the three conditions performed throughout the segmentation trials. The goal behind these two tasks was to test participants‚Äô knowledge of the word boundaries which could in turn reveal to what extent they have learned the allomorphy, and if not what the reasons could be. Table 5-3 shows the possible combinations of trial types schematically in terms of response accuracy. The ‚Äúcorrect/incorrect‚Äù status indicates highly accurate/inaccurate responses and ‚Äúuncertain‚Äù indicates near-chance level accuracy of responses.\n\nTable ‚Äé5‚Äë3. Schema of possible interpretations based on trial types in segmentation tasks\n\n\n\n\n\n\n\n\n\nC-initial word2\n(itemB-A,\nitemB-NA)\nV-initial word2\n(itemAlt, itemNonAlt)\nInterpretation\n\n\n\n\na.\ncorrect\ncorrect\nlearned the allomorphy ‚Üí possibly also the maximal CVC form as the UR\n\n\nb.\ncorrect\nincorrect\nthe word1 is always CV ‚Üí did not learn the allomorphy, blocked by a segmentation bias\n\n\nc.\ncorrect\nuncertain\nthe word1 is CV more often than not ‚Üí learning of the allomorphy is hindered by a segmentation bias\n\n\nd.\n\nuncertain\ncorrect\nthe word1 is CVC more often than not ‚Üí did not fully learn the allomorphy\n\n\ne.\nincorrect\ncorrect\nlearned some kind of alternation but not the intended allomorphy (where the word1 is CVC with an alternating coda but not just /t/ and / É/)\n\n\n\nThe predicted probability values from the analysis of the segmentation tasks in Section 4.2 and 4.3 are summarized in Table 5-4 below. These accuracy probabilities indicate that both the participants in the Alt1st condition and the Mix have chosen the correct choice (&gt;0.89) on the C-initial word2s in the two segmentation tasks, however the V-initial word2 trials are where they slightly differ. In the familiar segmentation task, within V-initial word2s (the itemAlts) all the participants, regardless of condition, have low accuracies (&lt; 0.23), although the participants in Alt1st are slightly more inclined to choose the CV form than the rest. When participants‚Äô knowledge is probed with novel word2s in the novel segmentation task, within the V-initial word2s (here both the itemAlt and itemNonAlts) the probabilities indicate that the performance of the participants in the Mix condition are slightly higher (0.32) compared to others. It was in this task where within itemAlts, the comparison of performance of the participants in the two conditions Alt1st and Mix was approaching a statistically significant difference. It is also interesting that participants in the Alt1st condition have the same low probability of accuracy in the two segmentation tasks.\n\nTable ‚Äé5‚Äë4. Predicted probabilities (95% CI) in V-initial and C-initial trials in segmentation tasks\n\n\n\n\n\n\n\n\n\nAlt1st\nNonAlt1st\nMix\n\n\n\n\nC-initial ‚Äì familiar segmentation\n0.93 (¬±0.01)\n0.94 (¬±0.01)\n0.95 (¬±0.01)\n\n\nC-initial ‚Äì novel segmentation\n0.93 (¬±0.01)\n0.89 (¬±0.02)\n0.93 (¬±0.02)\n\n\nV-initial ‚Äì familiar segmentation\n0.14 (¬±0.03)\n0.17 (¬±0.04)\n0.23 (¬±0.07)\n\n\nV-initial ‚Äì novel segmentation\n0.14 (¬±0.04)\n0.2 (¬±0.05)\n0.32 (¬±0.08)\n\n\n\nThe predicted probabilities inboth tasks taken together across various item types underscores the challenges associated with parsing the liaison consonant in ambiguous scenarios. As it has been discussed in the context of child errors and developmental trajectory in Section 2.3, this challenge arises in early stages of learning when universal constraints in favour of left-edge alignment of phonological and morphological structure lead to certain biases during word segmentation favouring more C-initial parsing of word2s instead of V-initial. This matches the interpretation (c) and to some extent (b) in Table 5-3 where the word1 more often than not is in the CV form. In this context, a more nuanced interpretation of the results suggests that the Mix condition has not necessarily mastered the allomorphy more effectively than the Alt1st condition since their predicted probability of a correct response is below 0.5. Rather, the participants in the Mix condition appear to have been able to overcome the segmentation bias to some extent comparatively. This topic is discussed in more detail in this chapter in Section 5.5.\nTo discuss this issue from the perspective of the predictions of the learnability theory, it is useful to review the role of the universal markedness constraints in the forced-choice tasks. Align-L is the markedness constraint which directly influences segmentation through alignment requirements in parallel to other components of learning. A successful learner must learn the correct ranking of this constraint in relation to other constraints in the constraint hierarchy to learn the allomorphy pattern and infer the correct underlying representation. In the testing trials labelled itemAlt, the phrases were made from alternating word1 + V-initial word2. As can be seen from the tableau below in (1) with the example stimuli [ditogi] and choices /togi/ vs /ogi/, the intended correct choice in the segmentation task is the candidate that violates Align-L, given that the underlying representation is /dit/ + /ogi/ as assumed in the analysis in Section 2.4.2.\n(1)\n\n\n\n\n\n\n\n\n\n\nAlign-L\n\n\n\n\n‚òû\na. [di.to.gi] ü°® /di.t#o.gi/\n*\n\n\n\nb. [di.to.gi] ü°® /di.#to.gi/\n\n\n\n\nWhereas, in the trials labelled B-A the phrases were made from alternating word1 + C-initial word2. As can be seen from the tableau below in (2) with the example stimuli [mekis…ë] and choices /kis…ë/ vs /is…ë/, the intended correct choice in the segmentation task does not violate Align-L, given that the underlying representation is /me/ + /kis…ë/ as assumed in the analysis.\n(2)\n\n\n\n\n\n\n\n\n\n\nAlign-L\n\n\n\n\n‚òû\na. [me.ki.s…ë] ü°® /me.#ki.s…ë/\n\n\n\n\nb. [me.ki.s…ë] ü°® /me.k#is…ë/\n*\n\n\n\nIn the assumed biased initial state in the grammar, the Align-L constraint along with other markedness constraints outrank faithfulness constraints. Learning to demote Align-L must occur during block 1 for all learners due to errors with phrases containing /me É/ and /dit/ (e.g.¬†[me. É#o.gi]) as well as /k…ën/ (e.g.¬†[ka.n#o.gi]) or both depending on the condition. It is crucial that these errors would occur only if the underlying representations for the word1s posited by the learners were the CVC forms. Thus, by the time all the participants in the three conditions reach the end of the first block of training, Align-L is expected to have been demoted in their constraint hierarchies at that stage. These two strategies, that is constraint demotion and learning the correct segmentation and ultimately considering the CVC form to be the underlying form are part of the iterative loop in the learning algorithm that eventually enables participants to master the alternation pattern in their learning data. However, as the results of the segmentation tasks have shown, the majority of participants, regardless of condition, chose candidate (b) in tableau (1) while they picked candidate (a) in tableau (2); therefore, they preferred parsing the word2s as C-initial. The results of the phrase judgement task reported in Section 4.1 show that the three conditions are highly accurate in forced choice tasks of a different type with alternating items. In this task, in an itemAlt trial, to use the same phrases as an example, when choosing between /ditogi/ vs /diogi/ or /mekis…ë/ vs /me Ékis…ë/, majority of the participants choose the intended correct choice of /ditogi/ and /mekis…ë/. This choice could have also stemmed from a C-initial underlying representation for the word2 (/togi/ or /kis…ë/). Therefore, with the lack of significant differential learning outcomes, it is not simple to pinpoint which part of the aforementioned loop was difficult for the participants and where they deviated from it compared to the hypothesized algorithm for an ideal learner. However, as these two tableaux indicate, Align-L is the constraint which all participants prefer to satisfy.\nThe answer as to why the difference between the Mix and Alt1st, though only approaching significance, is in the opposite direction to our predictions requires further research. If the interpretation that the Mix condition is able to overcome (or is less impacted) by the segmentation bias is on the right track, one potential explanation could be that a more varied context of word1s preceding the same word2 might better accentuate the boundary between word1 and word2. That could lead to the use of the heuristic I proposed in Section 2.4 about choosing the maximal form as the underlying representation. For instance, as illustrated in below in Table 5-5, V-initial word2s like /ibud/ appear in both alternating and non-alternating word1 contexts across both blocks sequentially.\n\nTable ‚Äé5‚Äë5. Pseudo-randomization order in the Mixed condition\n\n\n\n\n\n\n\nBlock1\n‚Äú(non/)alternating word1‚Äù\n‚Üí\nBlock2\n‚Äú(non/)alternating word1s‚Äù\n\n\n\n\n/k…ën/ + /ibud/\n/me É/ + /ibud/\n/me/ + /kis…ë/\n/k…ën/ + /ufe/\n/me É/ + /ufe/\n/k…ën/ + /peku/\n/me/ + /peku/\n/k…ën/ + /budol/\n‚Ä¶\n\n/lu/ + /ibud/\n/dit/ + /ibud/\n/di/ + /kis…ë/\n/lu/ + /ufe/\n/dit/ + /ufe/\n/lu/ + /peku/\n/dit/ + /peku/\n/lu/ + /budol/\n‚Ä¶\n\n\n\nAnother possible explanation may relate to the context in which the constraint demotion algorithm operates in learning. Given the correct underlying representation posited for the ideal learners, the only error arising from the Alt1st condition training data in their first block stems from the wrong ranking of Align-L and Max. As shown in example (27b) in Section 2.5.1, initially the learner makes errors with considering an incorrect output such as [me.#i.bud] due to a ranking of NoV.V, NoC.C, Align-L &gt;&gt; Max in their initial-state constraint hierarchy. None of the other markedness constraints require demotion besides Align-L, as they are not the loser preferring constraints with the alternating morphemes. It is important to remember that this specific error relies on the underlying representations posited; if the underlying representation is not /dit/ or /me É/, the loser preferring constraints would be different. However assuming that the underlying representation is in place as we assumed, given that Align-L is considered a markedness constraint and the syllable structure in general in the first block is in line with the universal markedness bias in the initial-state, learning a language-specific (non-universal) pattern starting with errors that violate only one markedness constraint might have been more difficult for the participants in the Alt1st condition. In other words, breaking the feedback loop of the initial state and realizing that violating universal markedness constraints is in fact necessary in this language might have been more challenging for participants in the Alt1st condition. Whereas for learners of the other conditions, there is more than one markedness constraint that is loser preferring from the start in the first block.\n\n\n5.2.2 Reranking stages\nThe second prediction was about the error-driven reranking stages of the grammar. Based on the learnability theory of an ideal learner, the prediction was that for the NonAlt1st condition it would be easier to learn the final-state constraint hierarchy because they have the shortest trajectory to get to the final-state. However, contrary to our predictions, the participants in the NonAlt1st condition are lower in accuracy compared to Mix within the itemNonAlts when the other variables are held constant in the pairwise comparisons. Also, even though not statistically significant, the participants in the Alt1st condition are higher in accuracy compared to NonAlt1st within itemNonAlt (p=0.06).\n\nTable ‚Äé5‚Äë6. Predictions and results of reranking stages\n\n\n\n\n\n\n\n\nPrediction\n\nResults\nTask ‚Äì Item type\n\n\n\n\nNonAlt1st &gt; Alt1st &gt; Mix\n‚ùå\nMix &gt; NonAlt1st\nAlt1st &gt; NonAlt1st*\n(*nearly | significant)\nJudgement ‚Äì itemNonAlt\nJudgement ‚Äì itemNonAlt\n\n\n\nTo explore how this significant difference came about and what it could imply about the differences and similarities in the learning trajectories, I will compare the results for each item type.\nSignificant difference between NonAlt1st and Mix within itemNonAs\nThe first subtype of the itemNonAlts (n=4 familiar) is the choices made with the CVC word1 /k…ën/ and C-initial word2 as can be seen from the tableau below (3) with the example stimuli [k…ënd íomet] vs [k…ëd íomet]. The four items were familiar combinations exactly matching the training stimuli.\n(3)\n\n\n\n\n\n\n\n\n\n\n\n/k…ën # d íomet/\nNoC.C-L\nMax\nNoC.C\n\n\n\n\n‚òû\na. [k…ën.d íomet]\n\n\n*\n\n\n\nb. [k…ë.d íomet]\n\n*\n\n\n\n\nBy choosing the suboptimal candidate [k…ëd íomet] participants would be making an error, which could be due to three possibilities:\na. The learners know the correct underlying representation but they have not learned the correct ranking but instead have a ranking where Max is outranked by NoC.C;\nb. The learners have learned the correct ranking as above where NoC.C-L is at the top but failed to correctly limit the indexed lexicon, so they have expanded it and considered /k…ën/ to also be part of the -L indexed lexicon;\nc. They have not learned the correct underlying representation for /k…ën/ and think the underlying representation is actually /k…ë/.\nThe second subtype of the itemNonAs (n=2 familiar+2 heldout) is the choice made with the CV word1 /lu/ and the liaison consonant / É/ + V-initial word2 as can be seen from the tableau below with the example stimuli [lu√¶te] vs [lu É√¶te]. Two of these items were familiar combinations and two items were unfamiliar1.\n(4)\n\n\n\n\n\n\n\n\n\n\n\n\n/lu # √¶te/\nMax\nDep\nNoV.V\nAlign-l\n\n\n\n\n‚òû\na. [lu.√¶te]\n\n\n*\n\n\n\n\nb. [lu. É√¶te]\n\n*\n\n*\n\n\n\nThe errors in these subitems could be due to two possible reasons:\na. The learners have learned the correct underlying representation but have not learned the correct ranking but instead have a ranking where NoV.V outranks the faithfulness constraint Dep;\nb. The second reason could be due to a wrong underlying representation. However, it is far-fetched to assume that participants stored the underlying representation of the word1 as /lu É/ mistakenly, as that combination never occurs. It is more probable that the word2s start with / É/ since the word2s might appear after these liaison consonants in training.\nComparing the descriptive statistics for these two subtypes would allow a more detailed interpretation of how these two conditions might be different. Table 5-7 shows the mean accuracy of three conditions within itemNonAlts. These means show that NonAlt1st is less accurate than the Mix condition in both of the non-alternating word1s.\n\nTable ‚Äé5‚Äë7. Average accuracy in the two subtypes of itemNonAlt trials in task 1 (phrase judgement)\n\n\n\n\n\n\n\n\n\nAlt1st\nNonAlt1st\nMix\n\n\n\n\n/k…ën/ + C-initial\n0.84\n0.74\n0.87\n\n\n/lu/ + V-initial\n0.57\n0.54\n0.63\n\n\n\nWhy would the NonAlt1st condition be worse with the word1s that they learned in their first block? One possible explanation for the overall lower accuracy of NonAlt1st is a recency effect in memory. If the interpretation that the NonAlt1st condition does not have the correct ranking that allows them to pick the optimal candidate is correct; then for them, NoV.V, NoC.C &gt;&gt; Max, Dep seems to be the decisive ranking. This may be due to the fact that the recent syllable structure in the output that the participants in the NonAlt1st condition was exposed to just before this testing task consists of patterns of phonologically motivated alternations with unmarked structures. Interpreting the constraint hierarchy in the structure of the more recent alternating training trials could have influenced the choices of participants. On the other hand, if the recency effect was a strong bias in general in the experiment, we would expect to observe the Alt1st condition to be less accurate within itemAlts. However, that is not the case; no significant differences were found within itemAlts. Therefore, if there is in fact a memory effect, as suggested by the statistical analysis, it seems to be affecting the learning of the NonAlt1st condition more strongly.\nFurthermore, the means in Table 5-7 above show that in trials involving /lu/, all conditions are on average relatively uncertain in their responses (mean=0.5‚Äí0.65), and it is not just the participants in the NonAlt1st condition that have been affected by their recent experience. The participants in the other two conditions (Alt1st and Mix) are highly accurate in /k…ën/ items but their accuracy drops to near chance levels with /lu/ items. This consistent uncertainty can be attributed to a combination of two possible reasons:\na. The segmentation results indicate learning the correct representation of the V-initial word2s was challenging for all participants. Therefore, this result could have been caused by some of the participants being open to a segmentation of the phrases as /lu/ + / É/-initial word2s;\nb. The tendency to resist the violation of the NoV.V constraint could be shared by all conditions.\nNear significant difference between Alt1st and NonAlt1st within itemNonAlts\nSimilar to the Mix vs NonAlt1st differences reviewed before, the predicted probabilities of the correct responses in the three conditions across different item types, as reported in Section 4.1, indicate that the only item type wherein the Alt1st and the NonAlt1st condition seem to perform differently from each other is within the itemNonAlts. The better performance of Alt1st compared to the NonAlt1st condition within itemNonAlts in this task is also against the predictions with respect to the reranking stages. The results of the segmentation tasks did not confirm the prediction regarding the successful learning of the underlying representation by the Alt1st condition; therefore, this could not have acted as a leverage for them. In fact, the Alt1st and the Mix condition accuracy is similar and above chance level across different item types in this task.\n\n\n5.2.3 Constraint cloning and lexicon indexation\nOur predictions suggested that the non-uniform data of the Mix condition in both blocks of training should place them at a disadvantage compared to the ordered conditions. They have to cope with inconsistencies with their input data consisting of only one alternating word1 at a time. There were no predictions about the specific advantages between the two ordered conditions based on the learnability theory of an ideal learner reviewed in Section 2.5.1 and 2.5.2. Although, from the perspective of a suboptimal learner that might be affected by memory limitations, the trajectory for the participants in the Alt1st condition could have put them at a disadvantage because resolving the inconsistency theoretically relies on participants recalling the combination of word1s+word2s learned in the first block to be tested against their updated grammar in the second block without encountering them again. In other words, learners in the two ordered conditions are expected to resolve the inconsistency in the second block. For the NonAlt1st condition the errors stored in the archive in the second block would also lead to the inconsistency and then resolving the inconsistency in the same block. Whereas for the Alt1st condition, detecting the inconsistency relies on the harder-to-retrieve memory of the non-errors of word1+ C-initial word2s (such as [mepeku] with the underlying forms /me É/+/peku/) which have not entered in their error archive in the first block. Overall, these predictions from the point of view of the ideal and suboptimal learner were not borne out, since the results reviewed so far in both of the tasks does not match the order in the predicted outcomes.\n\nTable ‚Äé5‚Äë8. Predictions and results of lexical indexation and constraint cloning\n\n\n\n\n\n\n\n\nPrediction\n\nResults\nTask ‚Äì Item type\n\n\n\n\nAlt1st, NonAlt1st &gt; Mix\n+\nNonAlt1st &gt; Alt1st\n‚ùå\n‚ùå\nMix &gt; NonAlt1st\nAlt1st &gt; NonAlt1st*\nMix &gt; Alt1st*\n(*nearly significant)\nJudgement ‚Äì item NonA\nJudgement ‚Äì item NonA\nNovel Segmentation ‚Äì itemAlt\n\n\n\nSame as the previous subsections, examining the possible combinations of trial type accuracy rates is necessary to understand the similarities and differences among conditions better. To indicate that the constraint cloning has been successfully learned, while a high, above-chance accuracy for the alternating items (itemAlts) was essential, it was important that at the same time the accuracy for itemNonAlts was also above chance.\nTwo examples of the itemNonAlt subtypes (3-4) were reviewed in the previous section. The itemAlts consist of two subtypes of errors similar to the itemNonAlts. The first subtype of the alternating items (n=3 familiar+1 heldout) is the choice between the two allomorph forms + C-initial word2s as can be seen from the tableau (5) below with the example stimuli [mebudol] vs [me Ébudol].\n(5)\n\n\n\n\n\n\n\n\n\n\n\n/me É # budol/\nNoC.C-L\nMax\nNoC.C\n\n\n\n\n‚òû\na. [me.budol]\n\n*\n\n\n\n\nb. [me É.budol]\n*\n\n*\n\n\n\nBy choosing the suboptimal candidate [me Ébudol] participants would be making an error, which could be due to three possibilities:\na. The learners know the correct underlying representation but they have not learned the correct ranking, but instead have a ranking where Max outranks NoC.C-L;\nb. The learners have learned the correct ranking as above partially (Max &gt;&gt; NoC.C) but failed to correctly clone the indexed constraint, or they have not learned that the /me É/ is part of the -L indexed lexicon;\nc. They have not learned the correct UR for /me É/ and think the UR form is actually /me/.\nThe second subtype of the alternating items (n=3 familiar + 1 heldout) is the choice between the two allomorphs + V-initial word2s which can be seen in the tableau (6) below with the example stimuli [ditug√¶ É] vs [diug√¶ É] given the correct underlying representation /dit/ has been posited.\n(6)\n\n\n\n\n\n\n\n\n\n\n\n\n/dit # ug√¶ É/\nMax\nNoV.V\nAlign-l\nNoC.C\n\n\n\n\n‚òû\na. [di.tug√¶ É]\n\n\n*\n\n\n\n\nb. [di.ug√¶ É]\n*\n*\n\n\n\n\n\nThe errors in these subitems could be due to two possible reasons:\na. The learners have learned the correct underlying representation but have not learned the correct ranking, but instead have a ranking where Align-L outranks the faithfulness constraint Max;\nb. The second reason could be due to a wrong UR, that is /di/ instead of /dit/.\nIt is worth mentioning that all the itemB-A or itemB-NA trials in the judgement task included the choice between an optimal candidate and a harmonically bounded candidate as exemplified in the tableau (7) below for the [dikis…ë] vs [di Ékis…ë] trial.\n(7)\n\n\n\n\n\n\n\n\n\n\n\n\n/dit # kis…ë/\nNoC.C-L\nMax\nDep\nNoC.C\n\n\n\n\n‚òû\na. [di.kis…ë]\n\n*\n\n\n\n\n\nb. [di É.kis…ë]\n*\n*\n*\n*\n\n\n\nSince the suboptimal candidate in these trials is harmonically bounded, logically only not storing the correct representations would result in an error. For instance, in the example (7) the suboptimal candidate is violating the same markedness constraint as the one in example (5) but it even includes an unfaithful output besides the other violation. The majority of the participants in the three conditions responded to all the itemB-A and itemB-NA with high accuracy therefore, I do not include them in the discussion of the differences between conditions. In fact, as pointed out in Section 3.5 these items were a pre-set criterion to filter participants out from the whole analysis. This decision was made on the basis that incorrect choices on these items indicate the extreme outlier participants because such participants have not even tracked static co-occurrence restrictions such as *di É and *met. Choosing an incorrect underlying representation on this ground would be totally unmotivated and an error of a different nature. It is probably that this error could have arisen from not paying attention to the experimental stimuli and learning the phonotactic distributions rather than from a universal bias in favor of a particular segmentation in the face of ambiguities.\nAs can be seen from the itemAlt example tableaux (5‚Äí6), the reasons for making errors on these subtypes are somewhat similar to the itemNonAlts (3‚Äí4). Therefore, by dividing the testing trials itemAlt and itemNonAlt in the judgement task by their word2, Table 5-9 shows the possible combinations of the trial types accuracy schematically along with their interpretations. The ‚Äúcorrect/incorrect‚Äù status indicates highly accurate/inaccurate responses.\n\nTable ‚Äé5‚Äë9. Judgement task - schema of possible interpretations based on trial types\n\n\n\n\n\n\n\n\n\nC-initial Word2 +\n[me], [di] and /k…ën/\nV-initial Word2 +\n[dit], [me É] and /lu/\nInterpretation\n\n\n\n\na.\ncorrect\ncorrect\nlearned the end state grammar (the cloned constraints and the correct URs)\n\n\nb.\ncorrect\nincorrect\nlearned the NoC.C-L &gt;&gt; Max &gt;&gt; NoC.C constraint ranking,\nbut did not learn the crucial rankings of Dep &gt;&gt; NoV.V or Max &gt;&gt; Align-L or they did not learn the URs correctly\n\n\nc.\nincorrect\ncorrect\nlearned the Dep &gt;&gt; No V.V and Max &gt;&gt; Align-L,\nbut did not learn the NoC.C-L &gt;&gt; Max &gt;&gt; NoC.C or they did not learn the URs correctly\n\n\nd.\nincorrect\nincorrect\ncannot distinguish the alternating and non-alternating word1s or they did not learn the URs correctly\n\n\n\nTo evaluate the results based the possible interpretations above, the descriptive mean accuracy of participant means2 in the three conditions is given in Table 5-10. With regards to learning the constraint cloning to address the inconsistency, it seems that participants in none of the conditions belong to row (a) in Table 5-9; rather they are closer to the results in (b) where the responses are more accurate in C-initial word2 trials compared to V-initial ones. The original prediction that the participants in the ordered conditions would perform better than the participants in the unordered condition was based on the hypothesis that the mechanism to clone constraints in two different blocks would be more difficult. Instead, the participants that seem to have been the least successful in coping with the NoC.C-L &gt;&gt; Max &gt;&gt; NoC.C part of the constraint hierarchy are the NonAlt1st condition because they have the highest accuracy with alternating word1s + C-initial word2s and the lowest accuracy with non-alternating word1s + C-initial word2s. The 0.13 differences in means between the Mix and NonAlt1st within itemNonAlts of this subtype illustrates how they were significantly different in this task.\nThe accuracy rate of 0.72 and below for V-initial word2s could be due to several possibilities. On the one hand, all the participants, regardless of their condition, seem to find the /lu/ + V-initial word2 trials challenging. This could be related to an incorrect underlying representation of the word2. The results reviewed previously in the segmentation tasks also reflect a similar challenge where the word2s in phrases such as /me Éibud/ were segmented as / É/-initial more often than V-initial. However, on the other hand, if the wrong segmentation leading to an incorrect underlying representation was the only reason of their errors consistently, the results must have shown an asymmetry in accuracy compared with the alternating word1s + V-initial word2s such as the example in [ditug√¶ É]. If the incorrect underlying representations of the word2s were all C-initial such as /tug√¶ É/ then the accuracy in these items must have been as high as the C-initial word2 trials. Instead, the accuracy on these trials is somewhere between the trials such as [lu√¶te] and [mebudol] for all participants. This suggests that the difficulty of these trials was not equal for all participants, and might have been due to a mix of different reasons.\nWith respect to the differences between the error types reflecting different quality of their learning trajectory, participant in the Alt1st condition seem to have high accuracy with itemAlt trials, however the NonAlt1st participants have a lower accuracy rate with itemNonAlt trials which does not mirror the same pattern.\n\nTable ‚Äé5‚Äë10. Average accuracy in itemNonAlt and itemAlt trials in the Judgement task\n\n\n\n\n\n\n\n\n\n4 subtypes (each n=4)\nexample (W ~ L)\nAlt1st\nNonAlt1st\nMix\n\n\n\n\n[di], [me] + C-initial\n[mebudol] ~ [me Ébudol]\n0.82\n0.86\n0.85\n\n\n/kan/ + C-initial\n[k…ënd íomet] ~ [k…ëd íomet]\n0.84\n0.74\n0.87\n\n\n[dit], [me É] + V-initial\n[ditug√¶ É] ~ [diug√¶ É]\n0.72\n0.65\n0.69\n\n\n/lu/ + V-initial\n[lu√¶te] ~ [lu É√¶te]\n0.57\n0.54\n0.63\n\n\n\nTo speculate about the trajectories of each condition and their effects on learning the inconsistency some points can be taken from these results from a cognitive perspective. Firstly, in the predictions of the current study the Mix condition was not expected to be more successful than other conditions on any of the learning components in our predictions in Section 2.5.4. However, based on Table 5-10, the fact that these participants outperform the ordered conditions with a relatively big difference, exactly on those items that the participants of the two ordered conditions were trained on in their first block, seems to suggest that this could be related to the trajectory of this condition in ways that we have not considered in our rationale. The nature of the difference between the items the participants in the two ordered conditions received compared to the unordered condition could have led to a memory boost for the Mix condition when recognizing the items during the testing subconsciously and therefore performing better on them. It is also possible that as we have discussed in Section 5.2.1. the segmentation of word2 has been less difficult for them. Lastly, one other possibility is that the hypothesis about the difficulty of resolving inconsistency needs to be adjusted. We might rather consider that it is easier to clone a constraint and index the lexicon in two blocks than to clone a constraint in one block with batch evidence from errors.\nSecondly, the Alt1st outperforming the NonAlt1st condition in the judgement task could lend itself to an interpretation about the cognitive effect of the order of learning lexically-specific properties and generalizations in a system along the lines of the simplicity framework as developed in Hsu and Chater (2010) and experiments of Wonnacott et al.¬†(2017). In this framework, arguments about language learnability can be explicit and quantified. In order for a system to be learnable, there needs to be a trade-off between the cost of encoding the rules of a grammar and encoding all the individual observed items. Simpler grammars bear lower costs, however if the grammar requires too many exceptions and becomes complicated then it is worth investing in encoding more accurate individual lexical items. A conceptual evaluation of the two learning trajectories according to this framework seems to show that the learning trajectory of the Alt1st condition could be less costly. Learning a simple generalization motivated by the phonological grammar prior to the individual exceptions in the non-alternating items overloads the cost trade-off in their favour. However, the NonAlt1st learning trajectory starts with the lexically-specific aspect of the mini-language as learning the non-alternating word1s could be equivalent to encoding the language-specific lexical items where there is no single rule to be learned which would apply to both of them. This trajectory in comparison seems more costly.\nIn the next subsection the effect of segmentation bias on the performance of participants is discussed in more detail.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Discussion</span>"
    ]
  },
  {
    "objectID": "chapters/5_discussion.html#pilot-experiment-and-the-universal-biases-in-segmentation",
    "href": "chapters/5_discussion.html#pilot-experiment-and-the-universal-biases-in-segmentation",
    "title": "5¬† Discussion",
    "section": "5.3 Pilot experiment and the universal biases in segmentation",
    "text": "5.3 Pilot experiment and the universal biases in segmentation\nIn Section 2.3 example (7), the key components that an ideal learner must acquire along its path to master the allomorphy patterns for a morpheme are introduced. The first essential logical component identified for learning morpheme-specific alternations is understanding the semantic content of a phrase, which can inform the learner whether the phrase is mono-morphemic. We can argue based on the evidence of the results obtained that the next step in learning is to recognize word boundaries. However, before discussing the implications of the experimental results for an account of morpho-phonological learning, it is important to consider the nature of the learning data used in the experiment and its ecological validity. In real-world language acquisition, children‚Äôs learning input consists of adult output forms, which notably lack explicit segmentation or semantic information. The experimental setting mirrors real-world learning in that both contexts require the detection of morpheme boundaries. Discovering morpheme boundaries is closely linked to learning segmentation and semantic information. Participants in this artificial language learning experiment receive input data devoid of any segmentation information; however, they have access to additional contextual semantic cues. In this regard, their knowledge of semantics is controlled for, as compared to children, they are informed from the outset that they will be learning about phrases containing two words that correspond to the two images displayed on the screen. As we have discussed in the previous subsections, participants in the experiment did not perform as well in the word segmentation tasks as in the phrase judgement task.\nThe tension between universal constraints and language-specific patterns is one of the key challenges that is present for the acquisition of liaison developmentally speaking, as we have discussed in Section 2.4.1. Throughout the testing trials of the current artificial language learning experiment with adults, the effect of the universal constraints that favor the alignment of syllable structure and word boundaries can also be clearly observed among all conditions. The results show that the testing trials that involve any ambiguous segmentation due to resyllabification of the two morphemes consistently have lower accuracy rates than other types of trials. This decrease of accuracy appeared in the novel segmentation task with itemAlts such as [ditogi] - /ogi/ vs /togi/ and itemNonAlts such as [k…ëne Éin] - /e Éin/ vs /ne Éin/, as well as one type of judgment task with [lu√¶te] vs [lu É√¶te] where the V-initial word2 /√¶te/ could have been parsed as C-initial / É√¶te/. Using evidence of phrases that contain a word1 of the ‚Äúnon-alternating‚Äù type such as [k…ë.ne. Éin] and [k…ën.bu.dol] could lead to participants learning that Align-L is a violable constraint and that the underlying representation of the word1 is /k…ën/. Overcoming this universal bias by realizing that the constraint needs to be demoted has an important effect because it is a pre-requisite knowledge that could ideally unlock the path to learning the alternations.\nTo highlight the persistent strong effect of the universal markedness constraint, Align-L, I draw on the results from a pilot version of the experiment. In the pilot round of this experiment, unlike the current version, all the training items were fully randomized within each block and the number of training trials was lower. Figure 5-1 shows the accuracy in task 2, the familiar segmentation task and Figure 5-2 shows task 3, the novel segmentation task. In each side-by-side comparison, the box and whiskers on the right side are from the pilot experiment and the box and whiskers on the left are from the current experiment. The result of the pilot experiment plotted for the segmentation tasks illustrate the below chance or near chance accuracy for the itemAlts (top-leftmost facet of both Figures) and itemNonAlts (the top-rightmost facet of Figure 5-2) in the three conditions.\n\n\n\nFigure ‚Äé5‚Äë1. Pilot vs current experiment in the familiar segmentation task\n\n\nIn the familiar segmentation task (Figure 5-1), the differences between the pilot and the current experiment can be summarized in two ways. All participants seem to perform well on C-initial word2s (itemB-A) in both versions. However, participants in the Mix condition seem to perform differently on V-initial word2s (itemAs) than the other conditions in the current experiment. Focusing on the right set of box and whiskers in the left facet, the median accuracy of the Mix is close to 0.5 in the current experiment in contrast to being at the floor in the pilot. In the novel segmentation task, the differences in performance between the current experiment and the pilot seem to be among V-initial word2s (itemAlt and itemNonAlt) as well.\n\n\n\nFigure ‚Äé5‚Äë2. Pilot vs current experiment in the novel segmentation task\n\n\nIn the pilot version of the experiment, the number of participants was smaller and there were a few other changes. Therefore, a detailed statistical comparison would not be appropriate. However, it is interesting to compare the effect of the full randomization of the training stimuli with the pseudo-randomization. The pseudo-randomization order we have implemented in the current experiment is similar to the idea of a contrastive paradigm suggested by Tesar (2014: 247). Specifically, the idea is that the same V-initial noun in different contexts can facilitate the learning of alternation, such as encountering /ogi/ in the contexts of [ditogi] and [me Éogi] one after the other. As these two plots show, within the itemAlts in both tasks and itemNonAlts, the condition that seems to have been affected more consistently by the switch to the contrastive paradigm in the training trials is the Mix condition. As we have discussed before, the higher accuracy of the Mix condition in learning the allomorphy and the alternation was not part of the initial predictions of the study, therefore further research is needed to confirm whether the contrastive paradigm only benefits the Mix condition in terms of learning the language-specific demands for segmentation or whether it also extends to learning the morpheme-specific alternation as well.\nOverall, even though the contrastive paradigm in the training trials seem to have alleviated some of the confounding effects of the pressure under the universal constraints to ensure participants learn the language-specific patterns more efficiently, the results are not conclusive. There is still high variation among participants possibly due to intra-participant differences which prevents the effect of the different trajectories from being observed more clearly. The written feedback collected at the end of the experiment from participants also reflect this situation. It is generally known that not all participants perform equally well in artificial language learning experiments; some might be more focused on the task and others might struggle more with the cognitive load and memory issues (Moreton and Pertsova, 2023; Moreton et al., 2021; McMullin, 2016). The next section discusses the analysis of the data which focuses on the results of the subset of participants, who are referred to as ‚Äúsuccessful‚Äù learners in the artificial language learning literature, to gain some insight into the factors that contribute to successful learning of this mini language.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Discussion</span>"
    ]
  },
  {
    "objectID": "chapters/5_discussion.html#analysis-of-the-subset-of-successful-learners",
    "href": "chapters/5_discussion.html#analysis-of-the-subset-of-successful-learners",
    "title": "5¬† Discussion",
    "section": "5.4 Analysis of the subset of successful learners",
    "text": "5.4 Analysis of the subset of successful learners\nTo analyze the results further, I targeted the subset of participants whose overall accuracy was above a certain threshold showing above-chance level of accuracy. Following the literature of artificial language learning studies (McMullin, 2016; Martin and White, 2021) the threshold was defined using a binomial right-tailed test. In other words, to be included in the subset of successful learners, the participants must have an accuracy rate above 0.69 (23 or more correct trials out of 32 trials) which is above the minimum proportion that differs significantly from chance-performance according to a right-tailed binomial test at an alpha level of 0.05. The first judgement task was analyzed separately (n=32 trials) and the second task and the third task which were both segmentation tasks were collapsed together (n=32 trials). The different proportions of the subset of successful learners in different tasks are shown in Table 5-11.\nIn the judgement task, over 85% of the participants achieved success, while in the segmentation task, only 21% were successful. This stark contrast in the proportions of successful learners across the two tasks highlights that all participants were affected by segmentation biases stemming from outside the training phase. An interesting takeaway from this analysis could still lie in comparing the proportions of successful learners despite this bias. Among the three conditions, the subset of successful learners in the Mix condition was the largest in the segmentation task, with 33% of participants. The distribution of the 39 learners who had above-chance accuracy in all tasks are plotted first for the phrase judgement task in Figure 5-3.\n\nTable ‚Äé5‚Äë11. The proportion of the successful learners to all participants\n\n\n\n\n\n\n\n\n\n\nAlt1st\nNonAlt1st\nMix\nTotal\n\n\n\n\nAll participants\n71\n66\n61\n201\n\n\nSuccessful - Phrase judgement task\n83% (59)\n83% (55)\n93% (57)\n85% (171)\n\n\nSuccessful -Segmentation tasks\n13% (9)\n20% (13)\n33% (20)\n21% (42)\n\n\nSuccessful - all tasks\n13% (9)\n18% (12)\n29% (18)\n19% (39)\n\n\n\nFigure 5-3 indicates that the distributions across the conditions are similar both across and within trial types. On itemNonAlt trials, when considering all learners, a statistically significant difference exists among the Mix and Alt1st conditions. However, the accuracy in the NonAlt1st condition is higher in subset of successful learners compared to all participants, while the accuracy of the Mix is similar in both side-by-side box and whisker plots. A logistic regression model was applied to the subset of successful learners from both the judgment and segmentation tasks. The emmeans pairwise comparisons revealed that in Task1‚Äôs itemNonA, there was a significant difference between the Alt1st and NonAlt1st condition. The results of the comparisons can be found in the Appendix (Bx). Given the reduced number of participants in the segmentation task and the imbalance between the participants in the two groups of learners being compared, the regression model outputs are not comparable to previous models.\n\n\n\nFigure ‚Äé5‚Äë3. Subset of the successful learners vs.¬†all learners in task 1 (phrase judgement)\n\n\nThe distribution of the successful learners (in all tasks) is plotted by the three conditions across the three item types in Figures 5-4 and 5-5. The distributions of the conditions are similar both across and within itemB-A trial types. Within the itemAlt trials participants in the three conditions have below chance performance in all learners, but that is not the case anymore when considering the successful learners. Also, especially considering the comparison of the side-by-side box and whiskers in Figure 5-5, there seems to be a difference among the error distributions within each condition according to the prediction which stated that the participants in the Alt1st condition are more accurate with itemAlts and the participants in the NonAlt1st condition are more accurate with the itemNonAlt.\n\n\n\nFigure ‚Äé5‚Äë4. Subset of the successful learners vs.¬†all learners in task 2 (familiar segmentation)\n\n\nThe more intriguing aspect of this discussion lies in whether any of the conditions can more effectively generalize their learning to novel items, as illustrated in Figure 5.5. The pairwise emmeans comparisons did not show any significant differences in task 3. The full results can be found in Appendix B5.\nHowever, judging based on the descriptive statistics (such as variation and mean values) among the conditions, it appears that differences in the ability to generalize to novel items for participants in different conditions depends on the type of item. Notably, the Alt1st condition shows a ceiling effect in its ability to generalize what has been learned to novel itemAlts, while both the NonAlt1st and Mix conditions reach ceiling performance within novel itemNonAlts. These findings indicate that the trajectory of the Alt1st group differs qualitatively from the other two groups, at least within the limited sample size of successful learners.\n\n\n\nFigure ‚Äé5‚Äë5. Subset of the successful learners vs.¬†all learners in task 3 (novel segmentation)",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Discussion</span>"
    ]
  },
  {
    "objectID": "chapters/5_discussion.html#analysis-of-the-effect-of-segmentation-accuracy-on-the-phrase-judgement-task",
    "href": "chapters/5_discussion.html#analysis-of-the-effect-of-segmentation-accuracy-on-the-phrase-judgement-task",
    "title": "5¬† Discussion",
    "section": "5.5 Analysis of the effect of segmentation accuracy on the phrase judgement task",
    "text": "5.5 Analysis of the effect of segmentation accuracy on the phrase judgement task\nTo analyze if performance on the segmentation tasks can be a predictor of participants performance on the phrase judgement task, I have included a continuous variable called ‚Äúsegmentation score‚Äù for each participant. The values for this variable were calculated for each participant as their rate of overall accuracy in the two segmentation tasks (n/32; not including their null responses). A logistic regression model was fitted with the following formula where condition, trial type and segmentation score were included as fixed effects.\n(10)\nresponses ~ conditions + trials + segmentation score + (1 + trials|subject) + (1+ conditions|stimulus)\nThe results of the model summary show that a higher segmentation score significantly increases the probability of participant‚Äôs correct responses in the judgement task (Œ≤=2.1, SE=0.37, p=0). The likelihood ratio test comparing the model in (10) and a simpler model without the main effect of segmentation score was significant. Therefore, segmentation score is a predictor of accuracy in the judgment task.\nTwo separate models were also considered one with the addition of the interaction of segmentation score and trial type and another one with the addition of the interaction of segmentation score and condition in the fixed effects. The interactions were included to investigate whether the relationship between response accuracy and segmentation score depends on the trial type or condition. However, a likelihood ratio test comparing these models simpler models without the interaction terms was not significant. More details about these models can be found in the Appendix B6. Regardless to visualize the effect of the segmentation score, the following plots in Figure 5-6 and Figure 5-7 illustrate these effects across the four trial types and three conditions.\n\n\n\nFigure ‚Äé5‚Äë6. Predicted probabilities of a correct response in the judgement task by accuracy score in segmentation across trial types\n\n\nThe effect seems stronger in the itemNonAlts compared to itemAlts, as the degree of the positive slope in the probability of correct responses is slightly larger. This could be an indication that the variation in responses to itemNonAlt phrases such as choosing [lu É√¶te] over [lu√¶te] could have been due to the difficulties in segmentation. Similarly, among conditions, as shown in Figure 5-7 below, the variation in the responses seems to be correlated with the participant‚Äôs segmentation accuracy. This effect seems slightly stronger in Alt1st compared to the other ordered condition, NonAlt1st.\n\n\n\nFigure ‚Äé5‚Äë7. Predicted probabilities of a correct response in the judgement task by accuracy score in segmentation across conditions",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Discussion</span>"
    ]
  },
  {
    "objectID": "chapters/5_discussion.html#conclusion-and-future-work",
    "href": "chapters/5_discussion.html#conclusion-and-future-work",
    "title": "5¬† Discussion",
    "section": "5.6 Conclusion and future work",
    "text": "5.6 Conclusion and future work\nThe study of the learnability of interface phenomena has proven increasingly valuable, as it enhances our understanding of fundamental theoretical concepts such as representations and the modular knowledge of grammar, as well as implications for language acquisition and linguistic disorders. However, many experimental studies on morpho-phonological learning face challenges due to the complex interactions among different grammatical modules that need to be integrated. This research has focused on the effect of learning trajectories on learning French liaison, where syntactic, semantic, and morphological knowledge interact with phonological grammar. Preliminary results suggest that input data and learning trajectories influence participants‚Äô performance only when tested with full phrases in a forced-choice tasks. However, more research is certainly needed to confirm the predictions regarding the differential learning of different trajectories, as obtaining conclusive results to reject the null hypotheses has not been possible.\nThe findings indicate that in an experimental setting controlling for syntactic and semantic information, adult learners achieve above-chance accuracy rates in judgment tasks involving word1+word2 phrases. However, when presented with phrases including novel nouns and even phases with familiar nouns, their accuracy in word segmentation falls below chance levels. Moreover, the representational challenges observed in the segmentation errors of French-speaking children (Babineau et al., 2023; Buerkin-Pontrelli et al., 2017; Chevrot et al., 2009) similarly affect adult learners in our artificial language study. In Buerkin-Pontrelli et al.¬†(2017) experimental study, when asked to use novel V-initial word2s such as /ivak/ in a liaison-triggering context, children‚Äôs errors involved replacement or insertions (e.g., deux + /n-ivak/) whereas adults‚Äô errors involved omission of the liaison consonant more frequently. In the current study, adult participant errors were similar to those ‚Äú/n-ivak/‚Äù errors of children in that when provided with the opportunity to judge or segment phrases containing V-initial words they chose to fill the onset position to create C-initial or CV.CV(C) word2 forms. These results underscore the intricate interplay between the essential layer of morphological information and phonological learning, which may have obscured the effects of other challenging aspects of learning we originally aimed to investigate.\nDespite these complexities, learners of French successfully attain the end-state grammar and ultimately master morpheme-specific patterns. Recent studies indicate that as early as 24 months of age, toddlers are able to parse novel V-initial words within variable contexts, leveraging their differential knowledge of liaison consonants (Babineau, et al., 2023; Babineau et al., 2021). Although this developmental journey could be prolonged, elicitation tasks show that by age 3 children are on their way to adult-like representations (Buerkin-Pontrelli et al., 2017). Bearing in mind that children at a very young age are to some extent reaching adult level of performance in experimental contexts, in what ways could the current artificial language learning experiment shed light on the extent to which various learning dimensions emerging from the data are less challenging than expected, or in other words more ‚Äúlearnable‚Äù than others? One of the experimental conditions in the current study, the unordered condition (Mix), is analogous to children‚Äôs learning data because they both include a variety of different phrases including both alternating and non-alternating word1s from the start. As the results in Section 5.4 show, the proportion of successful learners in the current study who were trained with incremental inconsistencies present within two blocks (Mix condition) was greater than the proportion of successful learners trained on inconsistencies present between two separate blocks(29% vs 12% and 8%). Therefore, the implication of these results for a theory of learnability suggests that tackling inconsistencies could be less challenging than expected and that it seems to have been modulated by learning trajectory. On the other hand, challenges associated to overcoming the difficulties of segmenting V-initial word2s affect all participants equally and are of a different nature. Although, future research is essential to deepen our understanding of these two factors.\nIn the domain of artificial language learning experiments, significant inter-participant variability and discrepancies in learning outcomes are frequently observed (Moreton and Pertsova, 2023; McMullin, 2016). Consequently, an effective strategy for future research would be to replicate the study and to increase the participant population. It is also interesting to undertake a comparative analysis between various learner types to study the variation among learners. This approach would allow for the comparison of strategies used in learning between a subset of participants who consistently demonstrate an accurate versus those with inaccurate learning outcome throughout the testing phase.\nTo enhance the robustness of the findings, one suggestion is to replicate the experiments utilizing different variations of the experimental stimuli avoiding confounding effects. This could involve the creation of phrase structures beyond possessive phrase forms, such as the current ‚Äúproper noun + everyday object‚Äù configuration, as well as the incorporation of V-initial word1 forms into the experimental design. The latter is to introduce greater diversity in word shapes within the artificial language with V-initial phrases. On a separate note, in order to investigate the impact of the order of allomorphs in relation to nouns more effectively, a novel artificial language could be designed that positions allomorphs post-nominally, in contrast to the pre-nominal allomorphs in the current study. To more accurately discern the influence of recency in memory, an intermittent testing phase could be introduced between the two training blocks. However, in forced-choice tasks, the choice that is being compared with the intended optimal candidate could potentially be a form of negative learning data. It is important, therefore, to adopt an alternative testing methodology rather than forced-choice testing, in order to mitigate the effects arising from exposure to negative data amidst training. Furthermore, to explore age-related differences in learning strategies, it would be beneficial to conduct a similar version of the experiment with younger participants, who possess less meta-linguistic knowledge and linguistic experience, and to analyze their strategies and errors across different developmental stages.\nExperimental studies‚Äô advantages outweigh their disadvantages. While experimental settings often pose challenges in examining aspects of morpho-phonological learning, such as cognitive load and limited learning durations, the unique controlled nature of these experiments continues to offer valuable insights into the mechanisms of learning. The avenue of research on morpho-phonological learnability has the potential to advance our understanding of the learnability of interface phenomena, by considering necessary mechanisms and biases in the grammar, as well as the extent to which each module of the grammar must separately contribute to shaping our knowledge along the path.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Discussion</span>"
    ]
  },
  {
    "objectID": "chapters/5_discussion.html#footnotes",
    "href": "chapters/5_discussion.html#footnotes",
    "title": "5¬† Discussion",
    "section": "",
    "text": "Note that, due to the limitations in the design, both of the held-out phrases in the itemNonAs were with /lu/+word2 combinations.‚Ü©Ô∏é\nDescriptive statistics is used here since the grouping of trials based on word2 was not the levels incorporated in the variables in the statistical model.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Discussion</span>"
    ]
  },
  {
    "objectID": "chapters/references.html",
    "href": "chapters/references.html",
    "title": "Bibliography",
    "section": "",
    "text": "Babineau, M., Emond, E., & Shi, R. (2023). When language-general and language-specific processes are in conflict: The case of sub-syllabic word segmentation in toddlers. Infancy, 28(2), 301‚Äì321. https://doi.org/10.1111/infa.12510\nBabineau, M., Legrand, C., & Shi, R. (2021). Variable forms in French-learning toddlers‚Äô lexical representations. Developmental Psychology, 57(4), 457‚Äì470. https://doi.org/10.1037/dev0001157\nBabineau, M., & Shi, R. (2014). Distributional cues and the onset bias in early word segmentation. Developmental Psychology, 50(12), 2666‚Äì2674. https://doi.org/10.1037/a0038105\nBaer-Henney, D., K√ºgler, F., & Van De Vijver, R. (2015). The Interaction of Language-Specific and Universal Factors During the Acquisition of Morphophonemic Alternations with Exceptions. Cognitive Science, 39(7), 1537‚Äì1569. https://doi.org/10.1111/cogs.12209\nBates, D., M√§chler, M., Bolker, B., & Walker, S. (2015). Fitting Linear Mixed-Effects Models Using lme4. Journal of Statistical Software, 67(1). https://doi.org/10.18637/jss.v067.i01\nBerko, J. (1958). The Child‚Äôs Learning of English Morphology. Word, 14(2‚Äì3), 150‚Äì177. https://doi.org/10.1080/00437956.1958.11659661\nBoersma, P., & Weenik, D. (2024). Praat (Version 6.1.42) [Computer software]. http://www.praat.org/\nBuerkin-Pontrelli, A., Culbertson, J., Legendre, G., & Nazzi, T. (2017). Competing models of liaison acquisition: Evidence from corpus and experimental data. Language, 93(1), 189‚Äì219. https://doi.org/10.1353/lan.2017.0006\nChevrot, J.-P., Dugua, C., & Fayol, M. (2009). liaison acquisition, word segmentation and construction in French: A usage-based account. Journal of Child Language, 36(3), 557‚Äì596. https://doi.org/10.1017/S0305000908009124\nChevrot, J.-P., & Fayol, M. (2001). Acquisition of French liaison and Related Child Errors. In M. Almgren, A. Barre√±a, M. J. Ezeizabarrena, I. Idiazabal, & B. MacWhinney (Eds.), Research on Child Language Acquisition (Vol. 2, pp.¬†761‚Äì775). Cascadilla Press. https://hal.science/hal-00706711\nChong, A. J. (2021). The effect of phonotactics on alternation learning. Language, 97(2), 213‚Äì244.\nCoetzee, A. W. (2009). Learning lexical indexation. Phonology, 26(1), 109‚Äì145. https://doi.org/10.1017/S0952675709001730\nContent, A., Dumay, N., & Frauenfelder, U. (2000). The role of syllable structure in lexical segmentation: Helping listeners avoid mondegreens. In Spoken Word Access Processes. pp.¬†39-42. Max-Planck Institute for Psycholinguistics, Nijmegen.\nC«ít√©, M. (2011). French liaison. In M. Oostendorp, C. J. Ewen, E. Hume, & K. Rice (Eds.), The Blackwell Companion to Phonology: Vol. Volume V. Phonology across Languages (1st ed., pp.¬†1‚Äì26). John Wiley & Sons. https://doi.org/10.1002/9781444335262.wbctp0112\nCulbertson, J. (2024). Order shaped by cognition. Evidence for (and against) the effect of domain-general biases on word and morpheme order. Zeitschrift F√ºr Sprachwissenschaft, 43(2), 357‚Äì378. https://doi.org/10.1515/zfs-2024-2014\nDavidson, L., Jusczyk, P. W., & Smolensky, P. (2004). The initial and final states: Theoretical implications and experimental explorations of Richness of the Base. In R. Kager, J. Pater, & W. Zonneveld (Eds.), Constraints in Phonological Acquisition (pp.¬†321‚Äì365). Cambridge University Press.\nde Leeuw, J. R. (2015). jsPsych: A JavaScript library for creating behavioral experiments in a Web browser. Behavior Research Methods, 47(1), 1‚Äì12. https://doi.org/10.3758/s13428-014-0458-y\nDemuth, K., & Tremblay, A. (2008). Prosodically-conditioned variability in children‚Äôs production of French determiners. Journal of Child Language, 35(1), 99‚Äì127. https://doi.org/10.1017/S0305000907008276\nDugua, C. (2006). liaison, segmentation lexicale et sch√©mas syntaxiques entre 2 et 6 ans: Un mod√®le d√©veloppemental bas√© sur l‚Äôusage [Ph.D.¬†Dissertation]. Universit√© Stendhal, Grenoble.\nDurand, J., & Lyche, C. (2008). French liaison in the light of corpus data. Journal of French Language Studies, 18(1), 33‚Äì66. https://doi.org/10.1017/S0959269507003158\nEmbick, D. (2010). Localism versus globalism in morphology and phonology. MIT press.\nFinn, A.S., and Hudson Kam, C.L. (2015). Why segmentation matters: experience-driven segmentation errors impair ‚Äúmorpheme‚Äù learning. JEP: Learning, Memory, and Cognition, 41(5), 1560-1569\nFinley, S. (2021). Learning Exceptions in Phonological Alternations. Language and Speech, 64(4), 991‚Äì1017. https://doi.org/10.1177/0023830920978679\nFinley, S. (2023). Modeling harmony biases in learning exceptions to vowel harmony. Proceedings of the Linguistic Society of America, 8(1), 5530. https://doi.org/10.3765/plsa.v8i1.5530\nFriederici, A. D., & Wessels, J. M. I. (1993). Phonotactic knowledge of word boundaries and its use in infant speech perception. Perception & Psychophysics, 54(3), 287‚Äì295. https://doi.org/10.3758/BF03205263\nFukazawa, H. (1999). Theoretical implications of OCP effects on features in optimality theory [Ph.D.¬†Dissertation]. University of Maryland.\nGlewwe, E. (2022). Substantive bias and the positional extension of major place contrasts. Glossa: A Journal of General Linguistics, 7(1), Article 1. https://doi.org/10.16995/glossa.6537\nGnanadesikan, A. (2004). Markedness and faithfulness constraints in child phonology. In R. Kager, J. Pater, & W. Zonneveld (Eds.), Constraints in Phonological Acquisition (pp.¬†73‚Äì107). Cambridge University Press.\nGouskova, M. (2003). Deriving Economy: Syncope in Optimality Theory [Ph.D.¬†Dissertation]. University of Massachusetts Amherst.\nHale, M., & Reiss, C. (1998). Formal and Empirical Arguments concerning Phonological Acquisition. Linguistic Inquiry, 29(4), 656‚Äì683. https://doi.org/10.1162/002438998553914\nHannahs, S. J. (2011). Celtic Mutations. In M. Oostendorp, C. J. Ewen, E. Hume, & K. Rice (Eds.), The Blackwell Companion to Phonology (1st ed., pp.¬†1‚Äì24). Wiley. https://doi.org/10.1002/9781444335262.wbctp0117\nHayes, B., & White, J. (2013). Phonological naturalness and phonotactic learning. Linguistic Inquiry, 44(1), 45‚Äì75.\nHsu, A. S., & Chater, N. (2010). The Logical Problem of Language Acquisition: A Probabilistic Perspective. Cognitive Science, 34(6), 972‚Äì1016. https://doi.org/10.1111/j.1551-6709.2010.01117.x\nInkelas, S., & Zoll, C. (2007). Is grammar dependence real? A comparison between cophonological and indexed constraint approaches to morphologically conditioned phonology. Linguistics, 45(1). https://doi.org/10.1515/LING.2007.004\nIto, J., & Mester, A. (1995). The core-periphery structure of the lexicon and constraints on reranking. In J. Beckman, L. Dickey, & S. Urbanczyk (Eds.), University of Massachusetts Occasional Papers in Linguistics (Vol. 18, pp.¬†181‚Äì209). GLSA.\nIto, J., & Mester, A. (2001). Covert generalizations in Optimality Theory: The role of stratal faithfulness constraints. Studies in Phonetics, Phonology, and Morphology, 7, 273‚Äì299.\nJusczyk, P. W. (2000). The Discovery of Spoken Language. MIT Press.\nKager, R. (2004). Optimality Theory. Cambridge University Press.\nKerkhoff, A. (2007). The phonology‚Äìmorphology interface: Acquisition of alternations [Ph.D.¬†Dissertation]. Utrecht University.\nKim, Y. J., & Sundara, M. (2021). 6‚Äìmonth‚Äìolds are sensitive to English morphology. Developmental Science, 24(4), e13089. https://doi.org/10.1111/desc.13089\nLenth, R., Singmann, H., Love, J., Buerkner, P., & Herve, M. (2018). Emmeans: Estimated marginal means [Computer software]. https://cran.r-project.org/web/packages/emmeans/index.html\nLinzen, T., & Gallagher, G. (2017). Rapid generalization in phonotactic learning. Laboratory Phonology, 8(1), Article 1. https://doi.org/10.5334/labphon.44\nL√ºdecke, D. (2018). ggeffects: Tidy Data Frames of Marginal Effects from Regression Models. Journal of Open Source Software, 3(26), 772. https://doi.org/10.21105/joss.00772\n≈Åukaszewicz, B. (2006). Extrasyllabicity, transparency and prosodic constituency in the acquisition of Polish. Lingua, 116(1), 1‚Äì30. https://doi.org/10.1016/j.lingua.2005.03.002\nMacWhinney, B. (2014). The Childes Project (3rd ed., Vol. 1). Psychology Press. https://doi.org/10.4324/9781315805672\nMarquis, A., & Shi, R. (2012). Initial morphological learning in preverbal infants. Cognition, 122(1), 61‚Äì66. https://doi.org/10.1016/j.cognition.2011.07.004\nMartin, A., & White, J. (2021). Vowel harmony and disharmony are not equivalent in learning. Linguistic Inquiry, 52(1), 227‚Äì239. https://doi.org/10.1162/ling_a_00375\nMascar√≥, J. (2004). External allomorphy as emergence of the unmarked. In J. J. McCarthy (Ed.), Optimality Theory in Phonology (pp.¬†513‚Äì522). John Wiley & Sons. https://doi.org/10.1002/9780470756171.ch28\nMattys, S. L., & Jusczyk, P. W. (2001). Phonotactic cues for segmentation of fluent speech by infants. Cognition, 78(2), 91‚Äì121. https://doi.org/10.1016/S0010-0277(00)00109-8\nMcAuliffe, M., Socolof, M., Mihuc, S., Wagner, M., & Sonderegger, M. (2017). Montreal Forced Aligner: Trainable text-speech alignment using kaldi. Interspeech 2017, 498‚Äì502. https://doi.org/10.21437/Interspeech.2017-1386\nMcCarthy, J. J. (2005). Taking a free ride in morphophonemic learning. Catalan Journal of Linguistics, 4(1), 19. https://doi.org/10.5565/rev/catjl.112\nMcCarthy, J. J., & Prince, A. (1993). Generalized alignment. In G. Booij & J. Van Marle (Eds.), Yearbook of Morphology (pp.¬†79‚Äì153). Springer Netherlands. https://doi.org/10.1007/978-94-017-3712-8_4\nMcMullin, K. J. (2016). Tier-based locality in long-distance phonotactics: Learnability and typology [Ph.D.¬†Dissertation, University of British Columbia]. https://doi.org/10.14288/1.0228114\nMeinschaefer, J., Bonifer, S., & Frisch, C. (2015). Variable and invariable liaison in a corpus of spoken French. Journal of French Language Studies, 25(3), 367‚Äì396. https://doi.org/10.1017/S0959269515000186\nMintz, T. H. (2013). The segmentation of sub-lexical morphemes in English-learning 15-month-olds. Frontiers in Psychology, 4. https://doi.org/10.3389/fpsyg.2013.00024\nMorel, E. (1994). Le traitement de la liaison chez l‚Äôenfant: √âtudes exp√©rimentales. Travaux neuch√¢telois de linguistique, 21, Article 21. https://doi.org/10.26034/tranel.1994.2371\nMoreton, E., & Pater, J. (2012a). Structure and Substance in Artificial‚Äêphonology Learning, Part I: Structure. Language and Linguistics Compass, 6(11), 686‚Äì701. https://doi.org/10.1002/lnc3.363\nMoreton, E., & Pater, J. (2012b). Structure and Substance in Artificial‚ÄêPhonology Learning, Part II: Substance. Language and Linguistics Compass, 6(11), 702‚Äì718. https://doi.org/10.1002/lnc3.366\nMoreton, E., & Pertsova, K. (2023). Implicit and explicit processes in phonological concept learning. Phonology, 40(1‚Äì2), 101‚Äì153. https://doi.org/10.1017/S0952675724000034\nMoreton, E., Prickett, B., Pertsova, K., Fennell, J., Pater, J., & Sanders, L. (2021). Learning Repetition, but not Syllable Reversal. Proceedings of the Annual Meetings on Phonology, 9. https://doi.org/10.3765/amp.v9i0.4912\nMorin, Y. C. (2005). La liaison rel√®ve-t-elle d‚Äôune tendance √† √©viter les hiatus‚ÄØ? R√©flexions sur son √©volution historique. Langages, 158, 8‚Äì23.\nMorrison, A. (1986). A critical bibliography of studies of liaison in French speech since 1800 [Ph.D.¬†Dissertation]. Columbia University.\nNevins, A. (2011). Phonologically Conditioned Allomorph Selection: Phonologically Conditioned Allomorph Selection. In M. Van Oostendorp, C. J. Ewen, E. Hume, & K. Rice (Eds.), The Blackwell Companion to Phonology: Vol. Volume IV. Phonological Interfaces (pp.¬†1‚Äì26). John Wiley & Sons, Ltd.¬†https://doi.org/10.1002/9781444335262.wbctp0099\nNorris, D., & McQueen, J. M. (2008). Shortlist B: A Bayesian model of continuous speech recognition. Psychological Review, 115(2), 357‚Äì395. https://doi.org/10.1037/0033-295X.115.2.357\nOetting, J. B., & Horohov, J. E. (1997). Past-Tense Marking by Children With and Without Specific Language Impairment. Journal of Speech, Language, and Hearing Research, 40(1), 62‚Äì74. https://doi.org/10.1044/jslhr.4001.62\nPaster, M. (2006). Phonological Conditions on Affixation [Ph.D.¬†Dissertation]. University of California, Berkeley.\nPater, J. (2007). The Locus of Exceptionality: Morpheme-Specific Phonology as Constraint Indexation. In L. Bateman, M. O‚ÄôKeefe, E. Reilly, & A. Werle (Eds.), University of Massachusetts Occasional Papers in Linguistics (Vol. 32, pp.¬†259‚Äì296). GLSA. https://doi.org/doi:10.7282/T38C9TB6\nPater, J. (2010). Morpheme-specific phonology: Constraint indexation and inconsistency resolution. In Phonological Argumentation (pp.¬†123‚Äì154). University of Toronto Press. https://doi.org/10.3138/9781845532215.005\nPrince, A., & Smolensky, P. (1993). Optimality Theory: Constraint Interaction in Generative Grammar. Rutgers University Center for Cognitive Science, Technical Report no. RuCCS-TR-2.\nPrince, A., & Tesar, B. (2004). Learning phonotactic distributions. In R. Kager, J. Pater, & W. Zonneveld (Eds.), Constraints in Phonological Acquisition (1st ed., pp.¬†245‚Äì291). Cambridge University Press. https://doi.org/10.1017/CBO9780511486418.009\nRosen, E. (2003). Systematic Irregularity in Japanese Rendaku: How the grammar mediates patterned lexical exceptions. Canadian Journal of Linguistics/Revue Canadienne de Linguistique, 48(1‚Äì2), 1‚Äì37. https://doi.org/10.1017/S0008413100003261\nSelkirk, E. (1974). French liaison and the XÃÑ Notation. Linguistic Inquiry, 5(4), 573‚Äì590.\nSmith, B. W. (2015). Phonologically Conditioned Allomorphy and UR Constraints [Ph.D.¬†Dissertation, University of Massachusetts Amherst]. https://doi.org/10.7275/7540198.0\nSmolensky, P. (1996a). On the comprehension/production dilemma in child language. Linguistic Inquiry, 27(4), 720‚Äì731.\nSmolensky, P. (1996b). The Initial State and Richness of the Base in Optimality Theory. John Hopkins Cognitive Science Technical Report, 4.\nSmolensky, P., & Goldrick, M. A. (2016). Gradient Symbolic Representations in Grammar: The case of French liaison. Ms., Johns Hopkins University and Northwestern University. (ROA-1552)\nSoderstrom, M., White, K. S., Conwell, E., & Morgan, J. L. (2007). Receptive Grammatical Knowledge of Familiar Content Words and Inflection in 16‚ÄêMonth‚ÄêOlds. Infancy, 12(1), 1‚Äì29. https://doi.org/10.1111/j.1532-7078.2007.tb00231.x\nSouthworth, M.-J. (1970). French Words in H-. The French Review, 44(1), 63‚Äì71.\nStorme, B. (2024). Paradigm uniformity effects on French liaison. Natural Language & Linguistic Theory, 42(3), 1307‚Äì1352. https://doi.org/10.1007/s11049-023-09596-z\nSundara, M., Zhou, Z. L., Breiss, C., Katsuda, H., & Steffman, J. (2022). Infants‚Äô developing sensitivity to native language phonotactics: A meta-analysis. Cognition, 221, 104993. https://doi.org/10.1016/j.cognition.2021.104993\nTang, K., & Baer-Henney, D. (2023). Modelling L1 and the artificial language during artificial language learning. Laboratory Phonology, 14(1). https://doi.org/10.16995/labphon.6460\nTesar, B. (2014). Output-driven Phonology. Cambridge University Press.\nTesar, B., & Smolensky, P. (2000). Learnability in Optimality Theory. MIT Press.\nTessier, A.-M. (2012). Testing for OO-Faithfulness in the Acquisition of Consonant Clusters. Language Acquisition, 19(2), 144‚Äì173. https://doi.org/10.1080/10489223.2012.660552\nTessier, A.-M. (2016). Morpho-phonological Acquisition. In J. L. Lidz, W. Snyder, & J. Pater (Eds.), The Oxford Handbook of Developmental Linguistics (Vol. 1, pp.¬†111‚Äì132). Oxford University Press. https://doi.org/10.1093/oxfordhb/9780199601264.013.7\nTessier, A.-M., Jesney, K., Vesik, K., Lo, R., & Bouchard, M.-E. (2023). The Productive Status of Laurentian French liaison: Variation across Words and Grammar. Proceedings of the Annual Meetings on Phonology, 10. https://doi.org/10.3765/amp.v10i0.5447\nTomas, E., Demuth, K., & Petocz, P. (2017). The Role of Frequency in Learning Morphophonological Alternations: Implications for Children With Specific Language Impairment. Journal of Speech, Language, and Hearing Research, 60(5), 1316‚Äì1329. https://doi.org/10.1044/2016_JSLHR-L-16-0138\nTomasello, M. (2003). Constructing a language: A usage-based theory of language acquisition. Harvard University Press.\nTranel, B. (1995a). Current issues in French phonology: liaison and position theories. In J. A. Goldsmith (Ed.), The Handbook of Phonological Theory (pp.¬†798‚Äì816). Blackwell.\nTranel, B. (1995b). French final consonants and nonlinear phonology. Lingua, 95(1), 131‚Äì167. https://doi.org/10.1016/0024-3841(95)90104-3\nTranel, B. (2000). Aspects De La Phonologie Du Fran√ßais Et La Th√©orie De L‚Äôoptimalit√©. Langue Fran√ßaise, 126, 39‚Äì72.\nWauquier-Gravelines, S., & Braud, V. (2005). Proto-d√©terminant et acquisition de la liaison obligatoire en fran√ßais. Langages, 158(2), 53‚Äì65. https://doi.org/10.3917/lang.158.0053\nWhite, J., & Sundara, M. (2014). Biased generalization of newly learned phonological alternations by 12-month-old infants. Cognition, 133(1), 85‚Äì90. https://doi.org/10.1016/j.cognition.2014.05.020\nWhite, K. S., Peperkamp, S., Kirk, C., & Morgan, J. L. (2008). Rapid acquisition of phonological alternations by infants. Cognition, 107(1), 238‚Äì265. https://doi.org/10.1016/j.cognition.2007.11.012\nWolf, M. A. (2008). Optimal Interleaving: Serial Phonology-Morphology Interaction in a Constraint-Based Model [Ph.D.¬†Dissertation, University of Massachusetts Amherst].\nWonnacott, E., Brown, H., & Nation, K. (2017). Skewing the evidence: The effect of input structure on child and adult learning of lexically based patterns in an artificial language. Journal of Memory and Language, 95, 36‚Äì48. https://doi.org/10.1016/j.jml.2017.01.005\nWoods, K. J. P., Siegel, M. H., Traer, J., & McDermott, J. H. (2017). Headphone screening to facilitate web-based auditory experiments. Attention, Perception, & Psychophysics, 79(7), 2064‚Äì2072. https://doi.org/10.3758/s13414-017-1361-2\nZamuner, T. S., Kerkhoff, A., & Fikkert, P. (2012). Phonotactics and morphophonology in early child language: Evidence from Dutch. Applied Psycholinguistics, 33(3), 481‚Äì499. https://doi.org/10.1017/S0142716411000440",
    "crumbs": [
      "Appendices",
      "Bibliography"
    ]
  },
  {
    "objectID": "chapters/appendix1.html",
    "href": "chapters/appendix1.html",
    "title": "Appendix A ‚Äî Stimuli List",
    "section": "",
    "text": "List of Training Trials\n\n\n\n\n\n\n\n\nWord1\nWord2\nWord1 Type Phrase\n\n\n\n\n\n\n\ndit\n√¶te\nAlternating Word1\n\n\n\n\n\n\n\ndit\n…ët√¶l\nAlternating Word1\n\n\n\n\n\n\n\nme É\n…ët√¶l\nAlternating Word1\n\n\n\n\n\n\n\nme\nbudol\nAlternating Word1\n\n\n\n\n\n\n\ndi\nd íomet\nAlternating Word1\n\n\n\n\n\n\n\nme\nd íomet\nAlternating Word1\n\n\n\n\n\n\n\nme É\ned√¶p\nAlternating Word1\n\n\n\n\n\n\n\ndi\ng…ëbi É\nAlternating Word1\n\n\n\n\n\n\n\nme\ng…ëbi É\nAlternating Word1\n\n\n\n\n\n\n\ndit\nibud\nAlternating Word1\n\n\n\n\n\n\n\ndi\nkis…ë\nAlternating Word1\n\n\n\n\n\n\n\nme\nkis…ë\nAlternating Word1\n\n\n\n\n\n\n\nme\nm√¶ti\nAlternating Word1\n\n\n\n\n\n\n\nme É\nogi\nAlternating Word1\n\n\n\n\n\n\n\ndi\np√¶z…ëm\nAlternating Word1\n\n\n\n\n\n\n\nme\npeku\nAlternating Word1\n\n\n\n\n\n\n\ndit\nufe\nAlternating Word1\n\n\n\n\n\n\n\nme É\nufe\nAlternating Word1\n\n\n\n\n\n\n\ndit\nug√¶ É\nAlternating Word1\n\n\n\n\n\n\n\nme É\nug√¶ É\nAlternating Word1\n\n\n\n\n\n\n\ndi\nvezu\nAlternating Word1\n\n\n\n\n\n\n\nlu\n√¶te\nNonAlternating Word1\n\n\n\n\n\n\n\nk…ën\n…ët√¶l\nNonAlternating Word1\n\n\n\n\n\n\n\nlu\n…ët√¶l\nNonAlternating Word1\n\n\n\n\n\n\n\nk…ën\nbudol\nNonAlternating Word1\n\n\n\n\n\n\n\nk…ën\nd íomet\nNonAlternating Word1\n\n\n\n\n\n\n\nlu\nd íomet\nNonAlternating Word1\n\n\n\n\n\n\n\nk…ën\ned√¶p\nNonAlternating Word1\n\n\n\n\n\n\n\nk…ën\ng…ëbi É\nNonAlternating Word1\n\n\n\n\n\n\n\nlu\ng…ëbi É\nNonAlternating Word1\n\n\n\n\n\n\n\nlu\nibud\nNonAlternating Word1\n\n\n\n\n\n\n\nk…ën\nido\nNonAlternating Word1\n\n\n\n\n\n\n\nlu\nido\nNonAlternating Word1\n\n\n\n\n\n\n\nk…ën\nkis…ë\nNonAlternating Word1\n\n\n\n\n\n\n\nlu\nkis…ë\nNonAlternating Word1\n\n\n\n\n\n\n\nk…ën\nm√¶ti\nNonAlternating Word1\n\n\n\n\n\n\n\nk…ën\nogi\nNonAlternating Word1\n\n\n\n\n\n\n\nlu\np√¶z…ëm\nNonAlternating Word1\n\n\n\n\n\n\n\nk…ën\npeku\nNonAlternating Word1\n\n\n\n\n\n\n\nlu\npeku\nNonAlternating Word1\n\n\n\n\n\n\n\nk…ën\nufe\nNonAlternating Word1\n\n\n\n\n\n\n\nlu\nufe\nNonAlternating Word1\n\n\n\n\n\n\n\nk…ën\nug√¶ É\nNonAlternating Word1\n\n\n\n\n\n\n\nlu\nug√¶ É\nNonAlternating Word1\n\n\n\n\n\n\n\nlu\nvezu\nNonAlternating Word1\n\n\n\n\n\n\n\n\nList of Training Trials (held-out phrases that do not appear in training)\n\n\n\n\n\n\n\n\nWord1\nWord2\nWord1 Type Phrase\n\n\n\n\n\n\n\nme É\n√¶te\nAlternating Word1\n\n\n\n\n\n\n\ndi\nbudol\nAlternating Word1\n\n\n\n\n\n\n\ndit\ned√¶p\nAlternating Word1\n\n\n\n\n\n\n\nme É\nibud\nAlternating Word1\n\n\n\n\n\n\n\ndit\nogi\nAlternating Word1\n\n\n\n\n\n\n\nme\np√¶z…ëm\nAlternating Word1\n\n\n\n\n\n\n\nme\nvezu\nAlternating Word1\n\n\n\n\n\n\n\nk…ën\n√¶te\nNonAlternating Word1\n\n\n\n\n\n\n\nlu\nbudol\nNonAlternating Word1\n\n\n\n\n\n\n\nlu\ned√¶p\nNonAlternating Word1\n\n\n\n\n\n\n\nk…ën\nibud\nNonAlternating Word1\n\n\n\n\n\n\n\ndi\nm√¶ti\nNonAlternating Word1\n\n\n\n\n\n\n\nlu\nm√¶ti\nNonAlternating Word1\n\n\n\n\n\n\n\nlu\nogi\nNonAlternating Word1\n\n\n\n\n\n\n\nk…ën\np√¶z…ëm\nNonAlternating Word1\n\n\n\n\n\n\n\nk…ën\nvezu\nNonAlternating Word1\n\n\n\n\n\n\n\n\nTesting trials ‚Äì Task 1 Phrase Judgement\n\n\n\n\n\n\n\n\n\nCorrect Choice\nIncorrect Choice\nTrial Type\nItem Familiarity\n\n\n\n\n\n\n\n\nluibud\nlu Éibud\nitemNonAlt\nfamiliar\n\n\n\n\n\n\n\n\nlued√¶p\nlu Éed√¶p\nitemNonAlt\nheld-out\n\n\n\n\n\n\n\n\nlu√¶te\nlu É√¶te\nitemNonAlt\nfamiliar\n\n\n\n\n\n\n\n\nluogi\nlu Éogi\nitemNonAlt\nheld-out\n\n\n\n\n\n\n\n\nk…ëng…ëbi É\nk…ëg…ëbi É\nitemNonAlt\nfamiliar\n\n\n\n\n\n\n\n\nk…ënd íomet\nk…ëd íomet\nitemNonAlt\nfamiliar\n\n\n\n\n\n\n\n\nk…ënkis…ë\nk…ëkis…ë\nitemNonAlt\nfamiliar\n\n\n\n\n\n\n\n\nk…ënpeku\nk…ëpeku\nitemNonAlt\nfamiliar\n\n\n\n\n\n\n\n\nme Éed√¶p\nme Éted√¶p\nitemB-A\nfamiliar\n\n\n\n\n\n\n\n\nditido\ndit Éido\nitemB-A\nfamiliar\n\n\n\n\n\n\n\n\ndikis…ë\ndi Ékis…ë\nitemB-A\nfamiliar\n\n\n\n\n\n\n\n\nmem√¶ti\nmetm√¶ti\nitemB-A\nfamiliar\n\n\n\n\n\n\n\n\ndipeku\ndi Épeku\nitemB-A\nfamiliar\n\n\n\n\n\n\n\n\nditufe\ndit Éufe\nitemB-A\nheld-out\n\n\n\n\n\n\n\n\nme É√¶te\nme Ét√¶te\nitemB-A\nfamiliar\n\n\n\n\n\n\n\n\nmevezu\nmetvezu\nitemB-A\nheld-out\n\n\n\n\n\n\n\n\nlup√¶z…ëm\nlutp√¶z…ëm\nitemB-NA\nfamiliar\n\n\n\n\n\n\n\n\nlubudol\nlutbudol\nitemB-NA\nfamiliar\n\n\n\n\n\n\n\n\nluvezu\nlutvezu\nitemB-NA\nfamiliar\n\n\n\n\n\n\n\n\nlum√¶ti\nlutm√¶ti\nitemB-NA\nfamiliar\n\n\n\n\n\n\n\n\nk…ën…ët√¶l\nk…ë…ët√¶l\nitemB-NA\nfamiliar\n\n\n\n\n\n\n\n\nk…ënug√¶ É\nk…ëug√¶ É\nitemB-NA\nfamiliar\n\n\n\n\n\n\n\n\nk…ënufe\nk…ëufe\nitemB-NA\nheld-out\n\n\n\n\n\n\n\n\nk…ënido\nk…ëido\nitemB-NA\nheld-out\n\n\n\n\n\n\n\n\ndit…ët√¶l\ndi…ët√¶l\nitemAlt\nfamiliar\n\n\n\n\n\n\n\n\nmebudol\nme Ébudol\nitemAlt\nfamiliar\n\n\n\n\n\n\n\n\ndid íomet\nditd íomet\nitemAlt\nfamiliar\n\n\n\n\n\n\n\n\ndig…ëbi É\nditg…ëbi É\nitemAlt\nfamiliar\n\n\n\n\n\n\n\n\nme Éogi\nmeogi\nitemAlt\nfamiliar\n\n\n\n\n\n\n\n\nditug√¶ É\ndiug√¶ É\nitemAlt\nfamiliar\n\n\n\n\n\n\n\n\nme Éibud\nmeibud\nitemAlt\nheld-out\n\n\n\n\n\n\n\n\nmep√¶z…ëm\nme Ép√¶z…ëm\nitemAlt\nheld-out\n\n\n\n\n\n\n\n\ngodoeves\ngodoteves\npractice\n\n\n\n\n\n\n\n\n\ngodotnoros\ngodonoros\npractice\n\n\n\n\n\n\n\n\n\n\nTesting trials ‚Äì Task 2 Familiar Segmentation\n\n\n\n\n\n\n\n\n\n\nPhrase Prompt\nCorrect Choice\nIncorrect Choice\nTrial Type\nItem Familiarity\n\n\n\n\n\n\n\n\n\ndit√¶te\n√¶te\nt√¶te\nitemAlt\nseen\n\n\n\n\n\n\n\n\n\nme É…ët√¶l\n…ët√¶l\n É…ët√¶l\nitemAlt\nseen\n\n\n\n\n\n\n\n\n\nditibud\nibud\ntibud\nitemAlt\nseen\n\n\n\n\n\n\n\n\n\nme Éido\nido\n Éido\nitemAlt\nseen\n\n\n\n\n\n\n\n\n\nme Éufe\nufe\n Éufe\nitemAlt\nseen\n\n\n\n\n\n\n\n\n\nme Éug√¶ É\nug√¶ É\n Éug√¶ É\nitemAlt\nseen\n\n\n\n\n\n\n\n\n\ndited√¶p\ned√¶p\nted√¶p\nitemAlt\nunseen\n\n\n\n\n\n\n\n\n\nditogi\nogi\ntogi\nitemAlt\nunseen\n\n\n\n\n\n\n\n\n\ndit√¶te\n√¶te\nt√¶te\nitemAlt\nseen\n\n\n\n\n\n\n\n\n\nme É…ët√¶l\n…ët√¶l\n É…ët√¶l\nitemAlt\nseen\n\n\n\n\n\n\n\n\n\nditibud\nibud\ntibud\nitemAlt\nseen\n\n\n\n\n\n\n\n\n\nme Éido\nido\n Éido\nitemAlt\nseen\n\n\n\n\n\n\n\n\n\nmed íomet\nd íomet\nomet\nitemB-A\nseen\n\n\n\n\n\n\n\n\n\nmeg…ëbi É\ng…ëbi É\n…ëbi É\nitemB-A\nseen\n\n\n\n\n\n\n\n\n\nmekis…ë\nkis…ë\nis…ë\nitemB-A\nseen\n\n\n\n\n\n\n\n\n\ndip√¶z…ëm\np√¶z…ëm\n√¶z…ëm\nitemB-A\nseen\n\n\n\n\n\n\n\n\n\nmepeku\npeku\neku\nitemB-A\nseen\n\n\n\n\n\n\n\n\n\ndivezu\nvezu\nezu\nitemB-A\nseen\n\n\n\n\n\n\n\n\n\ndibudol\nbudol\nudol\nitemB-A\nunseen\n\n\n\n\n\n\n\n\n\ndim√¶ti\nm√¶ti\n√¶ti\nitemB-A\nunseen\n\n\n\n\n\n\n\n\n\nmed íomet\nd íomet\nomet\nitemB-A\nseen\n\n\n\n\n\n\n\n\n\nmeg…ëbi É\ng…ëbi É\n…ëbi É\nitemB-A\nseen\n\n\n\n\n\n\n\n\n\nbodominid\nminid\ninid\npractice\n\n\n\n\n\n\n\n\n\n\nbodoperem\nerem\nperem\npractice\n\n\n\n\n\n\n\n\n\n\n\nTesting trials ‚Äì Task 3 Novel Segmentation\n\n\n\n\n\n\n\n\n\n\nPhrase Prompt\nCorrect Choice\nIncorrect Choice\nTrial Type\nItem Familiarity\n\n\n\n\n\n\n\n\n\nme Éit É√¶n\nit É√¶n\n Éit É√¶n\nitemAlt\nnovel\n\n\n\n\n\n\n\n\n\nme Éez…ë\nez…ë\n Éez…ë\nitemAlt\nnovel\n\n\n\n\n\n\n\n\n\ndit…ëjor\n…ëjor\nt…ëjor\nitemAlt\nnovel\n\n\n\n\n\n\n\n\n\ndit…ëvi\n…ëvi\nt…ëvi\nitemAlt\nnovel\n\n\n\n\n\n\n\n\n\nme Éit É√¶n\nit É√¶n\n Éit É√¶n\nitemAlt\nnovel\n\n\n\n\n\n\n\n\n\nmef√¶rus\nf√¶rus\n√¶rus\nitemB-A\nnovel\n\n\n\n\n\n\n\n\n\nmefule\nfule\nule\nitemB-A\nnovel\n\n\n\n\n\n\n\n\n\ndit É√¶gef\nt É√¶gef\n√¶gef\nitemB-A\nnovel\n\n\n\n\n\n\n\n\n\ndis√¶d í…ë\ns√¶d í…ë\n√¶d í…ë\nitemB-A\nnovel\n\n\n\n\n\n\n\n\n\nlurunip\nrunip\nunip\nitemB-NA\nnovel\n\n\n\n\n\n\n\n\n\nluzo Éek\nzo Éek\no Éek\nitemB-NA\nnovel\n\n\n\n\n\n\n\n\n\nlurej…ë\nrej…ë\nej…ë\nitemB-NA\nnovel\n\n\n\n\n\n\n\n\n\nluvoli\nvoli\noli\nitemB-NA\nnovel\n\n\n\n\n\n\n\n\n\nk…ënok…ën\nok…ën\nnok…ën\nitemNonAlt\nnovel\n\n\n\n\n\n\n\n\n\nk…ëne Éin\ne Éin\nne Éin\nitemNonAlt\nnovel\n\n\n\n\n\n\n\n\n\nk…ënosu\nosu\nnosu\nitemNonAlt\nnovel\n\n\n\n\n\n\n\n\n\nk…ënim…ë\nim…ë\nnim…ë\nitemNonAlt\nnovel",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Stimuli List</span>"
    ]
  },
  {
    "objectID": "chapters/appendix2.html",
    "href": "chapters/appendix2.html",
    "title": "Appendix B ‚Äî Statistical Summaries",
    "section": "",
    "text": "B.0.1 1. Task1 - Phrase Judgement\nLogistic regression model:\nformula6_b1 &lt;- responses ~ conditions + trials + familiarity + conditions * trials + conditions * familiarity + (1 + trials|subject) + (1+ conditions |stimulus)\nmodel6_b1 &lt;- glmer(formula6_b1, data = data_s1, control=glmerControl(optimizer = \"optimx\", calc.derivs = FALSE, optCtrl = list(method = \"nlminb\", starttests = FALSE, kkt = FALSE)), family = binomial(link = \"logit\"))\nMODEL INFO:\nObservations: 6297\nDependent Variable: responses\nType: Mixed effects generalized linear regression\nError Distribution: binomial\nLink function: logit\nMODEL FIT:\nAIC = 5808.26, BIC = 6003.95\nPseudo-R¬≤ (fixed effects) = 0.05\nPseudo-R¬≤ (total) = 0.31\nFIXED EFFECTS:\n------------------------------------------------------------------\nEst. S.E. z val. p\n----------------------------------- ------- ------ -------- ------\n(Intercept) 1.11 0.38 2.94 0.00\nconditionsNA1 -0.48 0.21 -2.31 0.02\nconditionsMix 0.18 0.17 1.10 0.27\ntrialsitemA 0.47 0.53 0.90 0.37\ntrialsitemBL_A 0.74 0.53 1.40 0.16\ntrialsitemBL_NA 1.12 0.53 2.10 0.04\nfamiliarityheld-out 0.26 0.31 0.85 0.40\nconditionsNA1:trialsitemA 0.21 0.30 0.70 0.48\nconditionsMix:trialsitemA -0.30 0.25 -1.20 0.23\nconditionsNA1:trialsitemBL_A 0.43 0.31 1.36 0.17\nconditionsMix:trialsitemBL_A 0.00 0.26 0.01 0.99\nconditionsNA1:trialsitemBL_NA 0.20 0.33 0.61 0.54\nconditionsMix:trialsitemBL_NA -0.10 0.28 -0.35 0.73\n------------------------------------------------------------------\nEmmeans\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\ntrial_type\n\nestimate\nSE\nz.ratio\np.value\n\n\n\n\nA1 - NA1\nitemNonA\n\n0.48\n0.21\n2.31\n0.06\n\n\n\nitemA\n\n0.27\n0.24\n1.13\n0.78\n\n\n\nitemBL_A\n\n0.05\n0.25\n0.21\n1.00\n\n\n\nitemBL_NA\n\n0.28\n0.28\n0.99\n0.96\n\n\nA1 - Mix\nitemNonA\n\n-0.18\n0.17\n-1.10\n0.82\n\n\n\nitemA\n\n0.11\n0.20\n0.57\n1.00\n\n\n\nitemBL_A\n\n-0.19\n0.22\n-0.85\n1.00\n\n\n\nitemBL_NA\n\n-0.09\n0.26\n-0.34\n1.00\n\n\nNA1 - Mix\nitemNonA\n\n-0.67\n0.21\n-3.20\n0.00\n\n\n\nitemA\n\n-0.15\n0.24\n-0.66\n1.00\n\n\n\nitemBL_A\n\n-0.24\n0.26\n-0.93\n1.00\n\n\n\nitemBL_NA\n\n-0.37\n0.29\n-1.27\n0.61\n\n\n\nChi-Square Anova tests\n2 main effects (without condition) vs 3 main effects\nModels:\nmodel3_b1: responses ~ trial_type + familiarity + (1 + trial_type | subject) + (1 + conditions| stimulus)\nmodel4_b1: responses ~ conditions+ trial_type + familiarity + (1 + trial_type | subject) + (1 + conditions| stimulus)\nnpar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq)\nmodel3_b1 21 5804.1 5945.8 -2881.1 5762.1\nmodel4_b1 23 5800.7 5955.9 -2877.4 5754.7 7.3755 2 0.02503 *\n2 main effects (without trials) vs 3 main effects\nModels:\nmodel2_b1: responses ~ conditions+ familiarity + (1 + trial_type | subject) + (1 + conditions| stimulus)\nmodel4_b1: responses ~ conditions+ trial_type + familiarity + (1 + trial_type | subject) + (1 + conditions| stimulus)\nnpar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq)\nmodel2_b1 20 5806.8 5941.8 -2883.4 5766.8\nmodel4_b1 23 5800.7 5955.9 -2877.4 5754.7 12.092 3 0.007074 **\n2 main effects (without familiarity) vs 3 main effects\nModels:\nmodel1_b1: responses ~ conditions+ trial_type + (1 + trial_type | subject) + (1 + conditions| stimulus)\nmodel4_b1: responses ~ conditions+ trial_type + familiarity + (1 + trial_type | subject) + (1 + conditions| stimulus)\nnpar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq)\nmodel1_b1 22 5799.4 5947.9 -2877.7 5755.4\nmodel4_b1 23 5800.7 5955.9 -2877.4 5754.7 0.6758 1 0.411\n3 main effects (without interaction) vs with interaction\nModels:\nmodel4_b1: responses ~ conditions+ trial_type + familiarity + (1 + trial_type | subject) + (1 + conditions| stimulus)\nmodel5_b1: responses ~ conditions + trials + familiarity + conditions * trials + (1 + trials | subject) + (1 + conditions | stimulus)\nnpar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq)\nmodel4_b1 23 5800.7 5955.9 -2877.4 5754.7\nmodel5_b1 29 5808.3 6003.9 -2875.1 5750.3 4.4784 6 0.6122\n\n\nB.0.2 2. Task2 - Familiar segmentation\nLogistic regression model:\nformula5_b2 &lt;- responses ~ conditions + trials + familiarity + conditions * trials + (1 + trials|subject) + (1+ conditions |stimulus)\nmodel5_b2 &lt;- glmer(formula5_b2, data = data_s2, control=glmerControl(optimizer = \"optimx\", calc.derivs = FALSE, optCtrl = list(method = \"nlminb\", starttests = FALSE, kkt = FALSE)), family = binomial(link = \"logit\"))\nMODEL INFO:\nObservations:3141\nDependent Variable:responses\nType:Mixed effects generalized linear regression\nError Distribution:binomial\nLink function:logit\nMODEL FIT:\nAIC= 2499.17, BIC = 2596.00\nPseudo-R¬≤ (fixed effects)= 0.42\nPseudo-R¬≤ (total)= 0.72\nFIXED EFFECTS:\n-----------------------------------------------------------------\nEst. S.E. z val. p\n---------------------------------- ------- ------ -------- ------\n(Intercept) -1.86 0.30 -6.16 0.00\nconditionsNA1 0.21 0.44 0.47 0.64\nconditionsMix 0.61 0.50 1.22 0.22\ntrialsitemBL_A 4.47 0.41 10.84 0.00\nfamiliarityheld-out 0.17 0.17 1.00 0.32\nconditionsNA1:trialsitemBL_A -0.10 0.61 -0.17 0.86\nconditionsMix:trialsitemBL_A -0.17 0.71 -0.24 0.81\n-----------------------------------------------------------------\nEmmeans:\n\n\n\ncontrast\ntrial_type\n\nestimate\nSE\nz.ratio\np.value\n\n\n\n\nA1 - NA1\nitemA\n\n-0.21\n0.44\n-0.47\n1.00\n\n\n\nitemBL_A\n\n-0.61\n0.50\n-1.22\n0.67\n\n\nNA1 - Mix\nitem_A\n\n-0.40\n0.46\n-0.87\n1.00\n\n\n\nitemBL_A\n\n-0.10\n0.34\n-0.31\n1.00\n\n\nA1 - Mix\nitemA\n\n-0.44\n0.43\n-1.03\n0.90\n\n\n\nitemBL_A\n\n-0.34\n0.37\n-0.89\n1.00\n\n\n\nChiSquare Anova test:\n2 main effects (without condition) vs 3 main effects\nmodel3_b2: responses ~ trials+ familiarity+ (1 + trials| subject) + (1 + conditions| stimulus)\nmodel4_b2: responses ~ conditions+ trials+ familiarity+ (1 + trials| subject) + (1 + conditions| stimulus)\nnpar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq)\nmodel3_b2 12 2494.2 2566.9 -1235.1 2470.2\nmodel4_b2 14 2495.2 2580.0 -1233.6 2467.2 3.0025 2 0.2229\n2 main effects (without trial) vs 3 main effects\nModels:\nmodel2_b2: responses ~ conditions+ familiarity+ (1 + trials| subject) + (1 + conditions| stimulus)\nmodel4_b2: responses ~ conditions+ trials+ familiarity+ (1 + trials| subject) + (1 + conditions| stimulus)\nnpar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq)\nmodel2_b2 13 2548.2 2626.9 -1261.1 2522.2\nmodel4_b2 14 2495.2 2580.0 -1233.6 2467.2 54.995 1 1.209e-13 ***\n2 main effects (without familiarity) vs 3 main effects\nModels:\nmodel1_b2: responses ~ conditions+ trials+ (1 + trials| subject) + (1 + conditions| stimulus)\nmodel4_b2: responses ~ conditions+ trials+ familiarity+ (1 + trials| subject) + (1 + conditions| stimulus)\nnpar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq)\nmodel1_b2 13 2494.1 2572.8 -1234.1 2468.1\nmodel4_b2 14 2495.2 2580.0 -1233.6 2467.2 0.9062 1 0.3411\n3 main effects (without interaction) vs with interaction\nModels:\nmodel4_b2: responses ~ conditions+ trials+ familiarity+ (1 + trials| subject) + (1 + conditions| stimulus)\nmodel5_b2: responses ~ conditions + trials + familiarity + conditions * trials + (1 + trials | subject) + (1 + conditions | stimulus)\nnpar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq)\nmodel4_b2 14 2495.2 2580 -1233.6 2467.2\nmodel5_b2 16 2499.2 2596 -1233.6 2467.2 0.0597 2 0.9706\n\n\nB.0.3 3. Task 3 - Novel segmentation\nLogistic regression model:\nMODEL INFO:\nObservations:3146\nDependent Variable:responses\nType:Mixed effects generalized linear regression\nError Distribution:binomial\nLink function:logit\nMODEL FIT:\nAIC= 2665.30, BIC = 2834.81\nPseudo-R¬≤ (fixed effects)= 0.37\nPseudo-R¬≤ (total)= 0.74\nFIXED EFFECTS:\n------------------------------------------------------------------\nEst. S.E. z val. p\n----------------------------------- ------- ------ -------- ------\n(Intercept) -1.15 0.33 -3.47 0.00\nconditionsNA1 0.46 0.50 0.92 0.36\nconditionsMix 0.99 0.49 1.99 0.05\ntrialsitemA -1.72 0.34 -5.11 0.00\ntrialsitemBL_A 4.04 0.51 7.90 0.00\ntrialsitemBL_NA 3.51 0.44 7.93 0.00\nconditionsNA1:trialsitemA -0.04 0.52 -0.08 0.93\nconditionsMix:trialsitemA 0.48 0.50 0.96 0.34\nconditionsNA1:trialsitemBL_A -0.19 0.77 -0.25 0.80\nconditionsMix:trialsitemBL_A -0.83 0.77 -1.08 0.28\nconditionsNA1:trialsitemBL_NA -1.19 0.67 -1.79 0.07\nconditionsMix:trialsitemBL_NA -1.04 0.66 -1.57 0.12\n------------------------------------------------------------------\nEmmeans:\n\n\n\ncontrast\ntrial_type\n\nestimate\nSE\nz.ratio\np.value\n\n\n\n\nA1 - NA1\nitemNonA\n\n-0.46\n0.50\n-0.92\n1.00\n\n\n\nitemA\n\n-0.42\n0.63\n-0.66\n1.00\n\n\n\nitemBL_A\n\n-0.27\n0.50\n-0.53\n1.00\n\n\n\nitemBL_NA\n\n0.73\n0.35\n2.11\n0.10\n\n\nA1 - Mix\nitemNonA\n\n-0.99\n0.49\n-1.99\n0.14\n\n\n\nitemA\n\n-1.46\n0.62\n-2.35\n0.06\n\n\n\nitemBL_A\n\n-0.16\n0.49\n-0.31\n1.00\n\n\n\nitemBL_NA\n\n0.06\n0.35\n0.16\n1.00\n\n\nNA1 - Mix\nitemNonA\n\n-0.53\n0.50\n-1.05\n0.88\n\n\n\nitemA\n\n-1.05\n0.62\n-1.68\n0.28\n\n\n\nitemBL_A\n\n0.11\n0.50\n0.22\n1.00\n\n\n\nitemBL_NA\n\n-0.68\n0.33\n-2.05\n0.12\n\n\n\nChiSquare Anova test:\n1 main effect (without condition) vs 2 main effects\nModels:\nmodel3_b3: responses ~ trials + (1 + trials | subject) + (1 + conditions | stimulus)\nmodel4_b3: responses ~ conditions + trials + (1 + trials | subject) + (1 + conditions | stimulus)\nnpar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq)\nmodel3_b3 20 2664.8 2785.8 -1312.4 2624.8\nmodel4_b3 22 2661.7 2794.8 -1308.8 2617.7 7.121 2 0.02843 *\n1 main effects (without item) vs 2 main effects\nModels:\nmodel2_b3: responses ~ conditions + (1 + trials | subject) + (1 + conditions | stimulus)\nmodel4_b3: responses ~ conditions + trials + (1 + trials | subject) + (1 + conditions | stimulus)\nnpar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq)\nmodel2_b3 19 2723.3 2838.3 -1342.7 2685.3\nmodel4_b3 22 2661.7 2794.8 -1308.8 2617.7 67.675 3 1.343e-14 ***\n2 main effects (without interaction) vs with interaction\nModels:\nmodel4_b3: responses ~ conditions + trials + (1 + trials | subject) + (1 + conditions | stimulus)\nmodel5_b3: responses ~ conditions + trials + conditions * trials + (1 + trials | subject) + (1 + conditions | stimulus)\nnpar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq)\nmodel4_b3 22 2661.7 2794.8 -1308.8 2617.7\nmodel5_b3 28 2665.3 2834.8 -1304.7 2609.3 8.3503 6 0.2135\n\n\nB.0.4 4. Task 2 and 3 - Both segmentation tasks\nLogistic regression model:\nformula5_b4 &lt;- responses ~ conditions + trials_sw + conditions * trials_sw + familiarity + (1 + trials|subject) + (1+ conditions|stimulus)\nmodel5_b4 &lt;- glmer(formula5_b4, data = data_s4, control=glmerControl(optimizer = \"optimx\", calc.derivs = FALSE, optCtrl = list(method = \"nlminb\", starttests = FALSE, kkt = FALSE)), family = binomial(link = \"logit\"))\nMODEL INFO:\nObservations:6287\nDependent Variable:responses\nType:Mixed effects generalized linear regression\nError Distribution:binomial\nLink function:logit\nMODEL FIT:\nAIC= 4970.83, BIC = 5132.73\nFIXED EFFECTS:\n-------------------------------------------------------------\nEst. S.E. z val. p\n------------------------------ ------- ------ -------- ------\n(Intercept) 2.58 0.22 11.73 0.00\nconditionsNA1 -0.13 0.27 -0.48 0.63\nconditionsMix 0.22 0.28 0.77 0.44\ntrials_swV -4.23 0.40 -10.47 0.00\nfamiliarityheld-out 0.12 0.25 0.46 0.65\nfamiliaritynovel -0.14 0.17 -0.84 0.40\nconditionsNA1:trials_swV 0.35 0.57 0.60 0.55\nconditionsMix:trials_swV 0.53 0.59 0.90 0.37\n-------------------------------------------------------------\nChiSquare Anova test:\n2 main effects (without condition) vs 3 main effects\nModels:\nmodel3_b4: responses ~ trials_sw + familiarity + (1 + trials | subject) + (1 + conditions | stimulus)\nmodel4_b4: responses ~ conditions + trials_sw + familiarity + (1 + trials | subject) + (1 + conditions | stimulus)\nnpar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq)\nmodel3_b4 20 4969.4 5104.3 -2464.7 4929.4\nmodel4_b4 22 4967.6 5116.1 -2461.8 4923.6 5.7478 2 0.05648 .\n2 main effects (without item) vs 3 main effects\nModels:\nmodel2_b4: responses ~ conditions + familiarity + (1 + trials | subject) + (1 + conditions | stimulus)\nmodel4_b4: responses ~ conditions + trials_sw + familiarity + (1 + trials | subject) + (1 + conditions | stimulus)\nnpar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq)\nmodel2_b4 21 5058.0 5199.7 -2508.0 5016.0\nmodel4_b4 22 4967.6 5116.1 -2461.8 4923.6 92.368 1 &lt; 2.2e-16 ***\n2 main effects (without familiarity) vs 3 main effects\nModels:\nmodel1_b4: responses ~ conditions + trials_sw + (1 + trials | subject) + (1 + conditions | stimulus)\nmodel4_b4: responses ~ conditions + trials_sw + familiarity + (1 + trials | subject) + (1 + conditions | stimulus)\nnpar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq)\nmodel1_b4 20 4964.9 5099.8 -2462.4 4924.9\nmodel4_b4 22 4967.6 5116.1 -2461.8 4923.6 1.2368 2 0.5388\n3 main effects (without interaction) vs with interaction\nModels:\nmodel4_b4: responses ~ conditions + trials_sw + familiarity + (1 + trials | subject) + (1 + conditions | stimulus)\nmodel5_b4: responses ~ conditions + trials_sw + conditions * trials_sw + familiarity + (1 + trials | subject) + (1 + conditions | stimulus)\nnpar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq)\nmodel4_b4 22 4967.6 5116.1 -2461.8 4923.6\nmodel5_b4 24 4970.8 5132.7 -2461.4 4922.8 0.8177 2 0.6644\n\n\nB.0.5 5. Analysis of the subset of successful learners\nTask 1 Emmeans:\n&gt; pairs(emm6_b1, adjust = \"bonferroni\")\ntrial = itemNonA:\ncontrast estimate SE df z.ratio p.value\nA1 - NA1 0.49606 0.203 Inf 2.447 0.0432\nA1 - Mix -0.08251 0.179 Inf -0.461 1.0000\nNA1 - Mix -0.57857 0.205 Inf -2.817 0.0145\ntrial = itemA:\ncontrast estimate SE df z.ratio p.value\nA1 - NA1 0.31351 0.237 Inf 1.322 0.5588\nA1 - Mix 0.33733 0.209 Inf 1.616 0.3181\nNA1 - Mix 0.02382 0.233 Inf 0.102 1.0000\ntrial = itemBL_A:\ncontrast estimate SE df z.ratio p.value\nA1 - NA1 0.20357 0.247 Inf 0.823 1.0000\nA1 - Mix -0.03067 0.226 Inf -0.136 1.0000\nNA1 - Mix -0.23424 0.251 Inf -0.933 1.0000\ntrial = itemBL_NA:\ncontrast estimate SE df z.ratio p.value\nA1 - NA1 0.35611 0.285 Inf 1.249 0.6346\nA1 - Mix 0.34814 0.261 Inf 1.332 0.5481\nNA1 - Mix -0.00797 0.277 Inf -0.029 1.0000\nResults are averaged over the levels of: familiarity\nResults are given on the log odds ratio (not the response) scale.\nP value adjustment: bonferroni method for 3 tests\nTask2 Emmeans\n&gt; pairs(emm6_b2, adjust = \"bonferroni\")\ntrial = itemA:\ncontrast estimate SE df z.ratio p.value\nA1 - NA1 1.170 0.936 Inf 1.249 0.6345\nA1 - Mix 1.572 0.824 Inf 1.908 0.1691\nNA1 - Mix 0.402 0.728 Inf 0.552 1.0000\ntrial = itemBL_A:\ncontrast estimate SE df z.ratio p.value\nA1 - NA1 -0.674 0.916 Inf -0.736 1.0000\nA1 - Mix -0.246 0.763 Inf -0.322 1.0000\nNA1 - Mix 0.428 0.811 Inf 0.529 1.0000\nResults are averaged over the levels of: familiarity\nResults are given on the log odds ratio (not the response) scale.\nP value adjustment: bonferroni method for 3 tests\nTask3 Emmeans\n&gt; pairs(emm4_b3, adjust = \"bonferroni\")\ntrial = itemNonA:\ncontrast estimate SE df z.ratio p.value\nA1 - NA1 -1.977 1.496 Inf -1.322 0.5583\nA1 - Mix -1.547 1.300 Inf -1.191 0.7014\nNA1 - Mix 0.430 1.341 Inf 0.321 1.0000\ntrial = itemA:\ncontrast estimate SE df z.ratio p.value\nA1 - NA1 1.664 1.448 Inf 1.149 0.7511\nA1 - Mix 1.361 1.367 Inf 0.995 0.9591\nNA1 - Mix -0.304 0.976 Inf -0.311 1.0000\ntrial = itemBL_A:\ncontrast estimate SE df z.ratio p.value\nA1 - NA1 -0.537 0.961 Inf -0.559 1.0000\nA1 - Mix -1.207 0.933 Inf -1.294 0.5874\nNA1 - Mix -0.670 0.820 Inf -0.817 1.0000\ntrial = itemBL_NA:\ncontrast estimate SE df z.ratio p.value\nA1 - NA1 0.414 0.849 Inf 0.487 1.0000\nA1 - Mix 0.149 0.785 Inf 0.189 1.0000\nNA1 - Mix -0.265 0.593 Inf -0.448 1.0000\nResults are given on the log odds ratio (not the response) scale.\nP value adjustment: bonferroni method for 3 tests\n\n\nB.0.6 6. Analysis of the segmentation accuracy\nLogistic regression model:\nGeneralized linear mixed model fit by maximum likelihood (Laplace Approximation) [glmerMod]\nFamily: binomial ( logit )\nFormula: responses ~ conditions + trials + acc + (1 + trials | subject) +\n(1 + conditions | stimulus)\nData: data_s1\nControl:\nglmerControl(optimizer = \"optimx\", calc.derivs = FALSE, optCtrl = list(method = \"nlminb\",\nstarttests = FALSE, kkt = FALSE))\nAIC BIC logLik deviance df.resid\n5775.2 5930.4 -2864.6 5729.2 6274\nScaled residuals:\nMin 1Q Median 3Q Max\n-6.9966 0.1751 0.3267 0.5181 1.9099\nRandom effects:\nGroups Name Variance Std.Dev. Corr\nsubject (Intercept) 0.15000 0.3873\ntrialsitemA 0.34288 0.5856 -0.45\ntrialsitemBL_A 0.40546 0.6368 -0.26 0.89\ntrialsitemBL_NA 0.55866 0.7474 0.05 0.86 0.78\nstimulus (Intercept) 0.96203 0.9808\nconditionsNA1 0.09705 0.3115 -1.00\nconditionsMix 0.02924 0.1710 -0.42 0.42\nNumber of obs: 6297, groups: subject, 198; stimulus, 32\nFixed effects:\nEstimate Std. Error z value Pr(&gt;|z|)\n(Intercept) -0.26268 0.36796 -0.714 0.475293\nconditionsNA1 -0.33948 0.12798 -2.653 0.007985 **\nconditionsMix -0.01529 0.12426 -0.123 0.902038\ntrialsitemA 0.71396 0.36022 1.982 0.047475 *\ntrialsitemBL_A 1.23056 0.36911 3.334 0.000856 ***\ntrialsitemBL_NA 1.32407 0.37787 3.504 0.000458 ***\nacc 2.10837 0.37946 5.556 2.76e-08 ***\n---\nLogistic regression model 2\nGeneralized linear mixed model fit by maximum likelihood (Laplace Approximation) [glmerMod]\nFamily: binomial ( logit )\nFormula: responses ~ conditions + trials + acc + conditions * acc + (1 +\ntrials | subject) + (1 + conditions | stimulus)\nData: data_s1\nControl:\nglmerControl(optimizer = \"optimx\", calc.derivs = FALSE, optCtrl = list(method = \"nlminb\",\nstarttests = FALSE, kkt = FALSE))\nAIC BIC logLik deviance df.resid\n5777.6 5946.3 -2863.8 5727.6 6272\nScaled residuals:\nMin 1Q Median 3Q Max\n-6.9312 0.1750 0.3264 0.5177 1.9665\nRandom effects:\nGroups Name Variance Std.Dev. Corr\nsubject (Intercept) 0.14147 0.3761\ntrialsitemA 0.34087 0.5838 -0.43\ntrialsitemBL_A 0.40245 0.6344 -0.26 0.89\ntrialsitemBL_NA 0.56340 0.7506 0.07 0.87 0.78\nstimulus (Intercept) 0.96835 0.9840\nconditionsNA1 0.09998 0.3162 -1.00\nconditionsMix 0.02967 0.1723 -0.44 0.44\nNumber of obs: 6297, groups: subject, 198; stimulus, 32\nFixed effects:\nEstimate Std. Error z value Pr(&gt;|z|)\n(Intercept) -0.6725 0.4964 -1.355 0.175486\nconditionsNA1 0.3385 0.5525 0.613 0.540177\nconditionsMix 0.4919 0.5733 0.858 0.390861\ntrialsitemA 0.7132 0.3588 1.988 0.046819 *\ntrialsitemBL_A 1.2297 0.3677 3.344 0.000825 ***\ntrialsitemBL_NA 1.3280 0.3769 3.524 0.000426 ***\nacc 2.8380 0.7062 4.019 5.84e-05 ***\nconditionsNA1:acc -1.1952 0.9453 -1.264 0.206123\nconditionsMix:acc -0.8875 0.9523 -0.932 0.351346\nLogistic regression model 3\nGeneralized linear mixed model fit by maximum likelihood (Laplace Approximation) [glmerMod]\nFamily: binomial ( logit )\nFormula: responses ~ conditions + trials + acc + trials * acc + (1 + trials |\nsubject) + (1 + conditions | stimulus)\nData: data_s1\nControl:\nglmerControl(optimizer = \"optimx\", calc.derivs = FALSE, optCtrl = list(method = \"nlminb\",\nstarttests = FALSE, kkt = FALSE))\nAIC BIC logLik deviance df.resid\n5777.0 5952.4 -2862.5 5725.0 6271\nScaled residuals:\nMin 1Q Median 3Q Max\n-7.3655 0.1712 0.3245 0.5219 1.9387\nRandom effects:\nGroups Name Variance Std.Dev. Corr\nsubject (Intercept) 0.13318 0.3649\ntrialsitemA 0.30791 0.5549 -0.38\ntrialsitemBL_A 0.38594 0.6212 -0.18 0.87\ntrialsitemBL_NA 0.53793 0.7334 0.13 0.86 0.76\nstimulus (Intercept) 0.95614 0.9778\nconditionsNA1 0.09517 0.3085 -1.00\nconditionsMix 0.03293 0.1815 -0.35 0.35\nNumber of obs: 6297, groups: subject, 198; stimulus, 32\nFixed effects:\nEstimate Std. Error z value Pr(&gt;|z|)\n(Intercept) 0.15567 0.41776 0.373 0.70942\nconditionsNA1 -0.33908 0.12773 -2.655 0.00794 **\nconditionsMix -0.01339 0.12495 -0.107 0.91464\ntrialsitemA -0.05066 0.57922 -0.087 0.93030\ntrialsitemBL_A 0.37156 0.61750 0.602 0.54737\ntrialsitemBL_NA 0.52164 0.65476 0.797 0.42563\nacc 1.37824 0.50783 2.714 0.00665 **\ntrialsitemA:acc 1.33260 0.78463 1.698 0.08944 .\ntrialsitemBL_A:acc 1.49199 0.85787 1.739 0.08200 .\ntrialsitemBL_NA:acc 1.39314 0.92746 1.502 0.13307\n\n2 main effects (without acc score) vs 3 main effects\nModels:\nmodel1_b1: responses ~ conditions + trials + (1 + trials | subject) + (1 + conditions | stimulus)\nmodel9_b1: responses ~ conditions + trials + acc + (1 + trials | subject) + (1 + conditions | stimulus)\nnpar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq)\nmodel1_b1 22 5799.4 5947.9 -2877.7 5755.4\nmodel9_b1 23 5775.2 5930.4 -2864.6 5729.2 26.178 1 3.113e-07 ***\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n3 main effects (without interaction) vs with interaction of accuracy score and trials\nModels:\nmodel9_b1: responses ~ conditions + trials + acc + (1 + trials | subject) + (1 + conditions | stimulus)\nmodel8_b1: responses ~ conditions + trials + acc + trials * acc + (1 + trials | subject) + (1 + conditions | stimulus)\nnpar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq)\nmodel9_b1 23 5775.2 5930.4 -2864.6 5729.2\nmodel8_b1 26 5777.0 5952.4 -2862.5 5725.0 4.2351 3 0.2372\n3 main effects (without interaction) vs with interaction of accuracy score and condition\nModels:\nmodel9_b1: responses ~ conditions + trials + acc + (1 + trials | subject) + (1 + conditions | stimulus)\nmodel10_b1: responses ~ conditions + trials + acc + conditions * acc + (1 + trials | subject) + (1 + conditions | stimulus)\nnpar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq)\nmodel9_b1 23 5775.2 5930.4 -2864.6 5729.2\nmodel10_b1 25 5777.6 5946.3 -2863.8 5727.6 1.6376 2 0.441",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Statistical Summaries</span>"
    ]
  }
]